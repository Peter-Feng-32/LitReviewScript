Authors,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,DOI,Link,Abstract,Author Keywords,Index Keywords,Document Type,Publication Stage,Source,EID,Reviewer,Reviewer Comments,Reviewer Decision
"Münstermann J., Hübner J., Büntzel J.",57895364900;16233809400;55873256300;,Can Cancer Education Programs Improve Health Literacy Among Deaf and Hard of Hearing Patients: a Systematic Review,2023,Journal of Cancer Education,38,1,,3,15,,,10.1007/s13187-022-02222-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138248637&doi=10.1007%2fs13187-022-02222-3&partnerID=40&md5=35dd86a9a1d5d0c526abca6ef2e3b5b1,"Patients affected from hearing loss face many problems when visiting oncologists. We conducted a systematic review to survey if cancer education programs can promote health literacy among deaf and hard of hearing (DHH) patients. The authors searched two databases for RCTs, and cohort studies with interventions promoting cancer health literacy for adult DHH patients. Risk of bias was assessed with SIGN Methodology Checklist for RCTs, and cohort studies. Significance of mean changes over time, and mean differences between comparison groups were used to present outcomes of each study. Surveyed interventions addressed three domains: cancer knowledge, coping skills, and cancer screening. Key information was gathered and synthesized providing a juxtaposition of the content and presenting important effects in detail. Nine RCTs and seven cohorts with 1865 participants were included. In total, 13 studies showed that cancer health literacy interventions improved mean scores significantly from pre- to post-test measures. There are hints that captioning and written texts may be sufficient for milder forms of hearing loss. Three studies showed that resiliency skill training promotes various domains of well-being. Three studies indicated that educational interventions encourage cancer screening practices. Educational programs are an effective way to promote cancer health literacy among DHH patients to facilitate communication with oncologists. As extent of hearing loss was not assessed, the authors cannot say the degree to which results are applicable to all degrees of hearing loss. To obtain hard data, further studies with more diverse populations, various cancer entities, different methods, and exact hearing loss assessments are required. © 2022, The Author(s).",Cancer; Communication; Deaf; Education; Hard of hearing; Health literacy,health literacy; health promotion; hearing impaired person; hearing impairment; human; neoplasm; Deafness; Health Literacy; Health Promotion; Humans; Neoplasms; Persons With Hearing Impairments,Review,Final,Scopus,2-s2.0-85138248637,Peter,,
"Li B., Qi P., Liu B., Di S., Liu J., Pei J., Yi J., Zhou B.",57215558055;57613607500;55574234335;57532404600;14519272800;57221322968;36095116600;57222344238;,Trustworthy AI: From Principles to Practices,2023,ACM Computing Surveys,55,9,3555803,,,,3.0,10.1145/3555803,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146608362&doi=10.1145%2f3555803&partnerID=40&md5=be423d2ba11b3d2ff832d0f88ce8d927,"The rapid development of Artificial Intelligence (AI) technology has enabled the deployment of various systems based on it. However, many current AI systems are found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection. These shortcomings degrade user experience and erode people's trust in all AI systems. In this review, we provide AI practitioners with a comprehensive guide for building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, and accountability. To unify currently available but fragmented approaches toward trustworthy AI, we organize them in a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to system development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items for practitioners and societal stakeholders (e.g., researchers, engineers, and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges for the future development of trustworthy AI systems, where we identify the need for a paradigm shift toward comprehensively trustworthy AI systems. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.",accountability; explainability; fairness; generalization; privacy protection; reproducibility; robustness; transparency; Trustworthy AI,Data acquisition; Data privacy; Life cycle; Robustness (control systems); User interfaces; Accountability; Artificial intelligence systems; Artificial intelligence technologies; Explainability; Fairness; Generalisation; Privacy protection; Reproducibilities; Robustness; Trustworthy artificial intelligence; Transparency,Review,Final,Scopus,2-s2.0-85146608362,Peter,,
Teng M.F.,57946200000;,Effectiveness of captioned videos for incidental vocabulary learning and retention: the role of working memory,2023,Computer Assisted Language Learning,,,,,,,,10.1080/09588221.2023.2173613,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147528969&doi=10.1080%2f09588221.2023.2173613&partnerID=40&md5=9d8ed69954bd1541771c76fbce573090,"Working memory (WM) may be an essential component of incidental vocabulary learning and retention from captioned videos. However, how WM affects young learners’ incidental vocabulary learning under different types of captions remains unclear. The present study employs a between-subject research design. The main purpose is to examine how two types of WM— phonological short-term memory and complex WM—impact vocabulary learning outcomes incidentally learned and retained from three types of captioning: (1) glossed captions (GCs), (2) full captions (FCs), and (3) keyword captions (KCs). A total of 125 young learners (M age = 12.17, SD = 1.06) watched four videos and completed two vocabulary tests administered as pretest, posttest, and delayed tests. After treatment, participants completed two WM tasks: (1) an operation span test for measuring complex WM, and (2) a nonword repetition test for measuring phonological short-term memory. The findings reveal that (1) captioning types, particularly GCs, led to the best outcome in incidental vocabulary learning and retention, and (2) phonological WM provided a more predictive effect on incidental vocabulary learning and retention than complex WM. Phonological and complex WM may have different predictive effects on incidental vocabulary learning and retention under different types of captioning. Relevant implications were discussed based on these results. © 2023 Informa UK Limited, trading as Taylor & Francis Group.",captioned videos; glossed captions; Incidental vocabulary learning; retention; working memory,,Article,Article in Press,Scopus,2-s2.0-85147528969,Peter,,
"Creed C., Al-Kalbani M., Theil A., Sarcar S., Williams I.",6602750527;57192164584;57218577044;36139109600;57200566697;,Inclusive AR/VR: accessibility barriers for immersive technologies,2023,Universal Access in the Information Society,,,,,,,,10.1007/s10209-023-00969-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147372094&doi=10.1007%2fs10209-023-00969-0&partnerID=40&md5=11753bc4f5fb096577de107336395b85,"Augmented and virtual reality (AR/VR) hold significant potential to transform how we communicate, collaborate, and interact with others. However, there has been a lack of work to date investigating accessibility barriers in relation to immersive technologies for people with disabilities. To address current gaps in knowledge, we led two multidisciplinary sandpits with key stakeholders (including academic researchers, AR/VR industry specialists, people with lived experience of disability, assistive technologists, and representatives from national charities and special needs colleges) to collaboratively explore and identify existing challenges with AR and VR experiences. We present key themes that emerged from sandpit activities and map out the interaction barriers identified across a spectrum of impairments (including physical, cognitive, visual, and auditory disabilities). We conclude with recommendations for future work addressing the challenges highlighted to support the development of more inclusive AR and VR experiences. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Accessibility; Augmented reality; Inclusive design; Virtual reality,Assistive technology; Virtual reality; Accessibility; Accessibility barriers; Assistive; Augmented and virtual realities; Current gap; Immersive technologies; Inclusive design; People with disabilities; Special needs; Spectra's; Augmented reality,Article,Article in Press,Scopus,2-s2.0-85147372094,Peter,,
"Ahlin T., Hiddinga A.",56204463600;7801320619;,Technological socialities: The impact of information and communication technologies on belonging among deaf and hard-of-hearing people,2023,Sociology Compass,,,,,,,,10.1111/soc4.13068,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147334517&doi=10.1111%2fsoc4.13068&partnerID=40&md5=d0761f61767b9574f433986621efee58,"This review article examines how different types of communication technologies, from the specialized medical to generic social devices, influence belonging and sociality among deaf and hard-of-hearing (DHH) people. The emphasis is on DHH adolescents and young adults who may be impacted differently across countries, given state-specific policies regarding the status of sign language and deaf education, and based on different availability, affordability, and accessibility of communication technologies. We introduce different perspectives on deafness, ranging from pathological to cultural, a heuristic on which we build to explore DHH socialities as complex and evolving. We then analytically review ethnographic research on how cochlear implants impact DHH people's belonging to the “deaf world” and/or the “hearing world,” and how they navigate between these worlds. Then we move on to technologies such as text messages and social media, which enable DHH people to extend their socialities beyond local communities. Belonging is a fluid phenomenon, and technologies which are in a constant process of innovation and development may influence it in complex ways. We argue that to explore questions of belonging, identity, and sociality among DHH people, and how they are shaped by technologies, (visual) ethnographic methods are particularly productive. © 2023 The Authors. Sociology Compass published by John Wiley & Sons Ltd.",cochlear implants; communication technologies; deaf and heard-of-hearing people; deafness; social media; video applications; visual ethnography,,Review,Article in Press,Scopus,2-s2.0-85147334517,Peter,,
"Elsherif M.M., Preece E., Catling J.C.",57193547763;55586012600;8261138100;,Age-of-Acquisition Effects: A Literature Review,2023,Journal of Experimental Psychology: Learning Memory and Cognition,,,,,,,,10.1037/xlm0001215,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147022868&doi=10.1037%2fxlm0001215&partnerID=40&md5=1c34ecd52092d687723ec9c80721ddb9,"Age of acquisition (AoA) refers to the age at which people learn a particular item and the AoA effect refers to the phenomenon that early-acquired items are processed more quickly and accurately than those acquired later. Over several decades, the AoA effect has been investigated using neuroscientific, behavioral, corpus and computational techniques. We review the current evidence for the AoA effect stemming from a range of methodologies and paradigms and apply these findings to current explanations of how and where the AoA effect occurs. We conclude that the AoA effect can be found both in the connections between levels of representations and within these representations themselves, and that the effect itself occurs through the process of the distinct coding of early and late items, together with the nature of the connections between levels of representation. This approach strongly suggests that the AoA effect results from the construction of perceptual-semantic representations and the mappings between representations © 2023 American Psychological Association",Age-of-acquisition; Word frequency; Word production; Word recognition,,Article,Article in Press,Scopus,2-s2.0-85147022868,Peter,,
Espineda M.N.,58075058500;,Social inclusion and the use of sign language inset during the 2019 Philippine SONA,2023,Universal Access in the Information Society,,,,,,,,10.1007/s10209-023-00968-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146692289&doi=10.1007%2fs10209-023-00968-1&partnerID=40&md5=737bf43baded8970d35b4c955047053c,"This phenomenological study, guided by Radcliffe-(1881–1955) Brown's Structural Functionalism Theory, investigated how sign language inset was used for social inclusion during the 2019 Philippine State of the Nation Address on GMA Network, Inc. Eight participants' lived experiences were examined through in-depth interviews. Four major themes emerged: exposure and familiarization with the deaf community culture, which is the Filipino Sign Language (FSL); adjustment of the TV inset size for deaf visual signs recognition and understanding; validation of TV insets interpreting with a deaf consultant; accessibility to communication through clear policy and guidelines of TV inset interpreting. The study concludes that even though the sign language interpreters use FSL, we can only elicit social inclusion by adjusting the size of the TV inset; since the sign language insets require the visual signs of the SLIs, which include hand gestures and facial expressions. Moreover, a better understanding of the signs that consider both schooled and non-schooled Deaf requires the exposure and familiarization of SLIs with the deaf culture. Meanwhile, TV networks should consider adjusting the size of the TV inset, hiring a deaf consultant to validate signing, and ensuring deaf access to communication to integrate them socially. Sign language inset implementation requires a model to follow structurally to be functional. © 2023, The Author(s).",Adjustment of TV inset; Deaf community; Social inclusion; Structural functionalism; Validation of TV inset,Adjustment of TV inset; Deaf community; In-depth interviews; Philippines; Sign language; Sign recognition; Sign understanding; Social inclusion; Structural functionalism; Validation of TV inset; Visual languages,Article,Article in Press,Scopus,2-s2.0-85146692289,Peter,,
"Donaldson A.L., Corbin E., Zisk A.H., Eddy B.",35834000900;58070318700;57211591615;57204708977;,"Promotion of Communication Access, Choice, and Agency for Autistic Students",2023,"Language, Speech, and Hearing Services in Schools",54,1,,140,155,,2.0,10.1044/2022_LSHSS-22-00031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146484561&doi=10.1044%2f2022_LSHSS-22-00031&partnerID=40&md5=fcdcd284b4c3bd3ebe70bb154593fc4e,"Purpose: Families and professionals often consider augmentative and alternative communication (AAC) a “last resort” for persons with communication chal-lenges; however, speaking autistic adults have reported that they would have benefited from access to AAC as children. This tutorial discusses the history of this “last resort” practice and its perpetuation within the medical model of dis-ability. The tutorial focuses on communication access, choice, and agency for autistic students. Method: We provide a brief overview of the AAC community and their preferred terminology, review the history of traditional approaches to research on AAC and autism, and then examine the relationship between disability models and ableism to views of spoken language as a priority of intervention. Studies on this topic are rare, and resisting ableism requires acknowledging and honoring disabled people’s experiences and expertise. Therefore, we promote autistic expertise within the framework of evidence-based practice and discuss the experiences of autistic people and spoken language. Finally, we consider the role of the speech-language pathologist (SLP) in assessment and offer autistic-based strategies and recommendations for communication support. Conclusions: Speaking autistic students who could benefit from AAC may not have access to AAC due to the prioritization of spoken language and lack of awareness of the benefit of AAC. We recommend that SLPs and school-based professionals support and facilitate access, communicative choice, and agency by implementing multimodal communication strategies to include AAC use for autistic students regardless of their spoken language status. Promoting all types of communication and ensuring opportunities for communication across multiple modalities are paramount to a child’s agency and self-determination, as is normalization of AAC. © The Authors.",,adult; autism; child; communication aid; communication disorder; education; human; interpersonal communication; speech disorder; student; Adult; Autistic Disorder; Child; Communication; Communication Aids for Disabled; Communication Disorders; Humans; Speech-Language Pathology; Students,Article,Final,Scopus,2-s2.0-85146484561,Peter,,
"Schlippe T., Fritsche K., Sun Y., Wölfel M.",42062203500;57220856224;57960937900;8870295600;,AI-Based Visualization of Voice Characteristics in Lecture Videos’ Captions,2023,Lecture Notes on Data Engineering and Communications Technologies,154,,,111,124,,,10.1007/978-981-19-8040-4_8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145775125&doi=10.1007%2f978-981-19-8040-4_8&partnerID=40&md5=b9ea2e9657458e53d46d6f34b51af6e4,"More and more educational institutions are making lecture videos available online. Since 100+ empirical studies document that captioning a video improves comprehension of, attention to, and memory for the video [1], it makes sense to provide those lecture videos with captions. However, studies also show that the words themselves contribute only 7% and how we say those words with our tone, intonation, and verbal pace contributes 38% to making messages clear in human communication [2]. Consequently, in this paper, we address the question of whether an AI-based visualization of voice characteristics in captions helps students further improve the watching and learning experience in lecture videos. For the AI-based visualization of the speaker’s voice characteristics in the captions we use the WaveFont technology [3–5], which processes the voice signal and intuitively displays loudness, speed and pauses in the subtitle font. In our survey of 48 students, it could be shown that in all surveyed categories—visualization of voice characteristics, understanding the content, following the content, linguistic understanding, and identifying important words—always a significant majority of the participants prefers the WaveFont captions to watch lecture videos. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",AI in education; Closed captions; Digital humanities; Natural language processing; Speech processing; Subtitles,E-learning; Natural language processing systems; Speech processing; Students; AI in education; Closed caption; Digital humanities; Language processing; Lecture video; Natural language processing; Natural languages; Subtitle; Video captions; Voice characteristics; Visualization,Book Chapter,Final,Scopus,2-s2.0-85145775125,Peter,,
"Zhang M., Hu H., Zhang Z., Li X., Wang Q., Bai X., Zang C.",56056456900;57957266900;57958028200;56969990100;57958391100;13612662300;25629213400;,The effect of lexical predictability on word processing in fast and slow readers during Chinese reading,2023,Acta Psychologica Sinica,55,1,,79,93,,,10.3724/SP.J.1041.2023.00079,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141468542&doi=10.3724%2fSP.J.1041.2023.00079&partnerID=40&md5=257ee466653f73c2c38c9eed7eb0b5ca,"According to the lexical quality hypothesis, high proficient (fast) readers have well-specified lexical representations which enable automatic word identification and less context decoding, while low proficient (slow) readers rely on context for word identification during reading due to their imprecise lexical quality. In contrast, the predictive coding framework assumes that high proficient readers rely more on their reading experience to predict the upcoming context compared to low proficient readers. However, it is still unclear how skilled readers with different levels of reading proficiency rely on context information (e.g., predictability) for word processing during Chinese reading. In two experiments, the present study aimed to investigate individual differences in the use of predictability for word identification by using the eye-tracking technique. In Experiment 1, eye movements of fast and slow readers were recorded while they were reading sentences containing predictable or unpredictable target words, with the aim to investigate the differences in predictability effects between the two groups. Sixty pairs of predictable-unpredictable target words were selected, each of which was embedded into the same sentence frame. Fifteen fast and 15 slow readers, selected from a group of 66 participants based on their reading rates, participated in Experiment 1. In Experiment 2, parafoveal previews of the 60 predictable target words (identical word, visually similar pseudocharacter, unpredictable word or visually dissimilar pseudocharacter) were manipulated by using the boundary paradigm to explore how parafoveal preview influences processing of predictability information in the fast and slow readers. The eye movements of 20 fast and 20 slow readers, selected from a group of 80 participants on the basis of their reading rates, were recorded while they were reading sentences containing predictable target words with different previews in Experiment 2. The results showed that fast readers fixated shorter and less on the target words and were more likely to skip the target words than slow readers. In Experiment 1, although reliable predictability effects with shorter fixations for predictable than unpredictable words were found, it did not interact with reading groups. However, results in Experiment 2 showed robust parafoveal preview effects on the target word which interacted with reading groups. In particular, the two groups had the same first-pass fixation times (i.e., FFD, SFD, GD) at the target words under the identical previews, while slow readers made longer fixations than fast readers at the targets with unpredictable previews or unrelated previews. In addition, fast readers skipped target words at a similar probability under both the identical preview and unpredictable preview conditions, while slow readers were less likely to skip target words with unpredictable previews than identical previews. The current findings indicate that fast and slow readers rely on context to a similar degree during their foveal lexical processing whereas the two groups show different utilization of previews of the predictable word during their parafoveal processing. To be specific, compared to fast readers, slow readers are inefficient in activating the predictable word with a visually similar preview; moreover, slow readers are disturbed more by the unpredictable preview or the visually dissimilar preview for their lexical processing, which suggests that slow readers are less effective in suppressing unrelated or inappropriate information during reading. Such findings provide evidence for the lexical quality hypothesis and are in support of the linguistic-proficiency hypothesis related to individual differences in the E-Z reader model. © 2023, Science Press. All rights reserved.",Chinese reading; fast reader; foveal processing; parafoveal preview; predictability; slow reader,,Article,Final,Scopus,2-s2.0-85141468542,Peter,,
"Rösler I.K., van Nunspeet F., Ellemers N.",57223204756;55873499500;7004525217;,Falling on deaf ears: The effects of sender identity and feedback dimension on how people process and respond to negative feedback − An ERP study,2023,Journal of Experimental Social Psychology,104,,104419,,,,,10.1016/j.jesp.2022.104419,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138640924&doi=10.1016%2fj.jesp.2022.104419&partnerID=40&md5=b253ba9dbc0050a582d91a939f2b68d7,"Social contexts can affect how people respond to feedback from others. We investigated how context information modulates the cognitive processing of feedback messages (i.e., external evaluations of one's character). We manipulated two aspects of (positive and negative) feedback messages: The identity of the sender (ingroup vs. outgroup member), and the dimension (one's competence vs. morality) as focal concern addressed in the feedback. We measured affective and behavioral responses after participants received such feedback (Study 1, N = 194), and additionally recorded an EEG in Study 2 (N = 49). In both studies, participants reported being more emotionally affected by negative feedback from ingroup compared to outgroup senders. Participants in Study 1 also reported to perceive feedback on their morality (vs. competence) as more negative. Complementing these findings, ERP results of Study 2 revealed greater preferential processing (i.e., increased P200) of feedback messages delivered by ingroup rather than outgroup members. Additionally, participants paid less sustained attention to feedback on their morality (vs. competence, as indicated by decreased P300- and LPP-amplitudes), and afterward recalled less morality- (vs. competence-) related feedback messages. The ERP findings were more pronounced for negative compared to positive feedback. These results suggest that subtle cues such as the social group-membership of a sender or the dimension addressed in a feedback message can modulate the cognitive processing of that message. Furthermore, our findings may explain why people are inclined to disregard negative feedback from outgroup senders and on their moral character. © 2022 The Authors",Affective responses; ERP; Feedback; Morality,adult; article; attention; controlled study; ear; electroencephalogram; evoked response; female; hearing impaired person; human; human experiment; major clinical study; male; morality; negative feedback; positive feedback,Article,Final,Scopus,2-s2.0-85138640924,Peter,,
"AlMeraj Z., Abu Doush I., Alhuwail D., Shama S., AlBahar A., Al-Ramahi M.",26634750800;36187951200;36454483000;57761308500;57761459300;42161008100;,Access and Experiences of Arabic Native Speakers With Disabilities on Social Media During and After the World Pandemic,2023,International Journal of Human-Computer Interaction,39,4,,923,948,,,10.1080/10447318.2022.2051887,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132650820&doi=10.1080%2f10447318.2022.2051887&partnerID=40&md5=1031de043eaaec5187b381b6a322ace0,"Since the start of the coronavirus 2019 (COVID-19) outbreak, governments across the world have mobilized to inform citizens on the virus spread details, nation-level processes, and best health measures and practices to be taken. A large percentage of the media posted through the COVID-19 crisis has been graphical, which raised the question of whether Arabic-speaking blind and deaf persons were able to independently access reliable information. This article presents the results of two studies. The first study involves a content analysis of official social media posts about COVID-19 during critical phases of the outbreak via heuristic evaluation of WCAG2.1 on an iOS smartphone and an iPad. The second study explores the experiences of native Arabic-speaking blind and deaf persons on social media during the pandemic and curfew or lockdown periods in the State of Kuwait using a semi-structured interview (11 people who are blind/low vision and 7 people who are deaf). Overall, our findings highlight the accessibility gaps in the current government social media information content and its dissemination practices and barriers in providing information and services. Also, it gives insights into how people who are blind and people who are deaf are able to manage their lifestyle within and beyond the COVID-19 pandemic. © 2022 Taylor & Francis Group, LLC.",,Social networking (online); Viruses; Blind person; Content analysis; Coronaviruses; Critical phasis; Deaf persons; Health measures; Health practices; Level process; Social media; Virus spreads; COVID-19,Article,Final,Scopus,2-s2.0-85132650820,Peter,,
"Lai V.T., van Berkum J., Hagoort P.",56524175500;7004323856;7003301986;,Negative affect increases reanalysis of conflicts between discourse context and world knowledge,2022,Frontiers in Communication,7,,910482,,,,,10.3389/fcomm.2022.910482,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145080356&doi=10.3389%2ffcomm.2022.910482&partnerID=40&md5=e40ae276612a050271a3d60baef74125,"Introduction: Mood is a constant in our daily life and can permeate all levels of cognition. We examined whether and how mood influences the processing of discourse content that is relatively neutral and not loaded with emotion. During discourse processing, readers have to constantly strike a balance between what they know in long term memory and what the current discourse is about. Our general hypothesis is that mood states would affect this balance. We hypothesized that readers in a positive mood would rely more on default world knowledge, whereas readers in a negative mood would be more inclined to analyze the details in the current discourse. Methods: Participants were put in a positive and a negative mood via film clips, one week apart. In each session, after mood manipulation, they were presented with sentences in discourse materials. We created sentences such as “With the lights on you can see..” that end with critical words (CWs) “more” or “less”, where general knowledge supports “more”, not “less”. We then embedded each of these sentences in a wider discourse that does/does not support the CWs (a story about driving in the night vs. stargazing). EEG was recorded throughout. Results: The results showed that first, mood manipulation was successful in that there was a significant mood difference between sessions. Second, mood did not modulate the N400 effects. Participants in both moods detected outright semantic violations and allowed world knowledge to be overridden by discourse context. Third, mood modulated the LPC (Late Positive Component) effects, distributed in the frontal region. In negative moods, the LPC was sensitive to one-level violation. That is, CWs that were supported by only world knowledge, only discourse, and neither, elicited larger frontal LPCs, in comparison to the condition where CWs were supported by both world knowledge and discourse. Discussion: These results suggest that mood does not influence all processes involved in discourse processing. Specifically, mood does not influence lexical-semantic retrieval (N400), but it does influence elaborative processes for sensemaking (P600) during discourse processing. These results advance our understanding of the impact and time course of mood on discourse. Copyright © 2022 Lai, van Berkum and Hagoort.",discourse; ERP; LPC (late positive component); mood; N400; semantics; world knowledge,,Article,Final,Scopus,2-s2.0-85145080356,Peter,,
"Mashiata M., Ali T., Das P., Tasneem Z., Badal M.F.R., Sarker S.K., Hasan M.M., Abhi S.H., Islam M.R., Ali M.F., Ahamed M.H., Islam M.M., Das S.K.",57949260100;57948894000;57219987957;56412929200;57226452852;57207728027;57213529757;57203129054;57220973727;57210996291;57949260200;55547121000;57217026039;,Towards assisting visually impaired individuals: A review on current status and future prospects,2022,Biosensors and Bioelectronics: X,12,,100265,,,,,10.1016/j.biosx.2022.100265,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140979606&doi=10.1016%2fj.biosx.2022.100265&partnerID=40&md5=859f4464ff7c6caf51c3687bdc35c13d,"Visually impaired people require support with regular tasks including navigating, detecting obstacles, and maintaining safety, especially in both indoor and outdoor environments. As a result of the advancement of assistive technology, their lives have become substantially more convenient. Here, cutting-edge assistive devices and technologies for the visually impaired are reviewed, along with a chronology of their evolution. These methodologies are classified according to their intended applications. The taxonomy is combined with a description of the tests and experiments that can be used to examine the characteristics and assessments of assistive technology. In addition, the algorithms used in assistive devices are examined. This paper looks at solar industry innovations and promotes using renewable energy sources to create assistive devices, as well as, addresses the sudden advent of COVID-19 and the shift in the development of assistive devices. This review can serve as a stepping stone for further research on the topic. © 2022 The Author(s)",,Renewable energy resources; Assistive devices; Assistive technology; Current status; Future prospects; Indoor environment; On currents; On-currents; Outdoor environment; Visually impaired; Visually impaired people; Assistive technology; assistive technology; computer heuristics; coronavirus disease 2019; global positioning system; human; image processing; image segmentation; mobile phone addiction; pattern recognition; quality of life; radiofrequency identification; reaction time; recognition; recognition index; renewable energy; Review; solar energy; visual impairment; visually impaired person,Review,Final,Scopus,2-s2.0-85140979606,Peter,,
"Salminen J., Jung S.-G., Nielsen L., Şengün S., Jansen B.J.",57200315665;57194276330;35561663800;56031272400;7202560690;,How does varying the number of personas affect user perceptions and behavior? Challenging the ‘small personas’ hypothesis!,2022,International Journal of Human Computer Studies,168,,102915,,,,1.0,10.1016/j.ijhcs.2022.102915,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136714610&doi=10.1016%2fj.ijhcs.2022.102915&partnerID=40&md5=a92b0c9e30cb6b9a8a7d5a248bb0c002,"Studies in human-computer interaction recommend creating fewer than ten personas, based on stakeholders’ limitations to cognitively process and use personas. However, no existing studies offer empirical support for having fewer rather than more personas. Investigating this matter, thirty-seven participants interacted with five and fifteen personas using an interactive persona system, choosing one persona to design for. Our study results from eye-tracking and survey data suggest that when using interactive persona systems, the number of personas can be increased from the conventionally suggested ‘less than ten’, without significant negative effects on user perceptions or task performance, and with the positive effects of increasing engagement with the personas, having a more diverse representation of the end-user population, as well as users accessing personas from more varied demographic groups for a design task. Using the interactive persona system, users adjusted their information processing style by spending less time on each persona when presented with fifteen personas, while still absorbing a similar amount of information than with five personas, implying that more efficient information processing strategies are applied with more personas. The results highlight the importance of designing interactive persona systems to support users’ browsing of more personas. © 2022 The Author(s)",Number of personas; Personas; User segmentation; User study,Behavioral research; Human computer interaction; Population statistics; Surveys; Eye-tracking; Number of persona; Persona; Personas systems; Survey data; Tracking data; User behaviors; User perceptions; User segmentation; User study; Eye tracking,Article,Final,Scopus,2-s2.0-85136714610,Peter,,
Almalhy K.M.,57243940900;,Effect of video tutorial delivery method on D/HH students’ content comprehension,2022,Frontiers in Psychology,13,,872946,,,,,10.3389/fpsyg.2022.872946,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143785692&doi=10.3389%2ffpsyg.2022.872946&partnerID=40&md5=5869412b5ccaf04df3e9c2a6b71fcaf2,"Introduction: Using video tutorials to deliver instructional content has become common practices nowadays. However, it is required to investigate how to implement new methods to deliver instructional content to deaf students to ensure success of their learning and reduce their reliance on personnel support or consultation from hearing peers. Therefore, and in light of cognitive theory of multimedia learning, this study experimented three different video tutorial methods to deliver instructional content that are tailored to deaf students’ learning needs. The three methods included: (a) sign language only, (b) captioned text only, and (c) sign language and captioned text combined. Methods: The study applied a mixed methods research design using pretest-posttest quasi-experimental design (tests scores) and qualitative research design (interviews). Fifty-four undergraduate deaf students from a large university in Saudi Arabia participated in this study, and of those participants, fifteen deaf students participated in semistructured interviews. Results and discussion: One-way analysis of variance results showed that using video tutorial that presents declarative content with captions only was significantly effective in comparison with the other methods. While the video tutorial that presents procedural content with sign language only was significantly effective in comparison with the other methods. Interview results confirmed the quantitative results. Practical and theoretical implications are discussed. Copyright © 2022 Almalhy.",captioned text; deaf and hard of hearing students; deaf education; multimedia; Saudi Arabia Sign Language; sign language interpreter; video tutorials,,Article,Final,Scopus,2-s2.0-85143785692,Peter,,
"Andrew S., Tigwell G.W.",57220117831;57191504112;,Accessible Design is Mediated by Job Support Structures and Knowledge Gained Through Design Career Pathways,2022,Proceedings of the ACM on Human-Computer Interaction,6,2 CSCW,487,,,,,10.1145/3555588,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146440097&doi=10.1145%2f3555588&partnerID=40&md5=00863585f147cf941c568da739a59eb1,"Digital designers often do not make their work accessible (e.g., websites failing criteria set by the W3C's Web Content Accessibility Guidelines), and accessible design research discusses many solutions to address this problem (e.g., teaching accessibility within university design and technical courses). However, prior research in this area typically does not acknowledge whether recommendations and resources to support accessible design are suitable for all digital designers due to different training pathways and job support structures (e.g., large-company vs. rural and self-employed designers or designers who learned their skills outside of formal education settings). We interviewed 20 digital designers from rural and urban areas, as well as working from home and remotely, to understand the challenges they experience in making accessible content within the context of their workplace. We find that job support structures mediate the effectiveness of current accessible design recommendations and resources, and we suggest how to improve accessible design support to meet the needs of under-resourced designers. © 2022 ACM.",accessible design; remote work; resources; rural; workplace,Curricula; Ground supports; Human engineering; Rural areas; Accessible design; Design course; Design research; Digital designers; Remote work; Resource; Support knowledge; Support structures; Web content accessibility guidelines; Workplace; Websites,Article,Final,Scopus,2-s2.0-85146440097,Peter,,
"Bragg D., Glasser A., Minakov F., Caselli N., Thies W.",43660991100;57195128152;57203000945;56286073000;7005832437;,Exploring Collection of Sign Language Videos through Crowdsourcing,2022,Proceedings of the ACM on Human-Computer Interaction,6,CSCW2,514,,,,,10.1145/3555627,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146432699&doi=10.1145%2f3555627&partnerID=40&md5=636b16973d6695e10077f096c38a7f28,"Inadequate sign language data currently impedes advancement of sign language ML and AI. Training on existing datasets results in limited models due to small size, and lack of diverse signers in real-world settings. Complex labeling problems in particular often limit scale. In this work, we explore the potential for crowdsourcing to help overcome these barriers. To do this, we ran a user study with exploratory crowdsourcing tasks designed to support scalability: 1) to record videos of specific content - thereby enabling automatic, scalable labeling - and 2) to perform quality control checks for execution consistency - further reducing post-processing requirements. We also provided workers with a searchable view of the crowdsourced dataset, to boost engagement and transparency and align with Deaf community values. Our user study included 29 participants using our exploratory tasks to record 1906 videos and perform 2331 quality control checks. Our results suggest that a crowd of signers may be able to generate high-quality recordings and perform reliable quality control, and that the signing community values visibility into the resulting dataset. © 2022 ACM.",citizen science; corpus; crowdsourcing; data; dataset; education; machine learning; sign language,Machine learning; Quality assurance; Quality control; Citizen science; Community values; Corpus; Data; Dataset; Labelings; Machine-learning; Real world setting; Sign language; User study; Crowdsourcing,Article,Final,Scopus,2-s2.0-85146432699,Peter,,
Hermosa-Ramírez I.,57219651262;,Physiological instruments meet mixed methods in Media Accessibility,2022,Translation Spaces(Netherland),11,1,,38,59,,,10.1075/ts.21020.her,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147315652&doi=10.1075%2fts.21020.her&partnerID=40&md5=07d40ad9d30083be25a9df2680a700fd,"Mixed methods have an established trajectory in the social sciences. Audiovisual Translation and Media Accessibility (MA) Studies are also increasingly applying the ""third research paradigm""(Johnson et al., 2007,112). Yet, publications in our field often fail to discuss the mixed-method nature of the study in depth, be it because of space limitations or a lack of deliberate integration of the methods. Concurrently, MA has seen a boom in experimental research, as descriptive approaches have given way to reception and usercentred studies that engage in the cognitive processes and immersion of audiences (Orero et al. 2018). This article proposes a methodological basis for MA researchers to design studies employing physiological instruments within a mixed methods framework. The core mixed methods designs (convergent, explanatory, and exploratory) are presented, and examples of their applications to research employing physiological instruments are discussed. © 2022 John Benjamins Publishing Company.",experimental research; Media Accessibility; methodology; mixed methods; physiological instruments; research desig,,Article,Final,Scopus,2-s2.0-85147315652,Peter,,
"Silva B.B., Orrego-Carmona D., Szarkowska A.",57202391092;57201389752;54416458200;,Using linear mixed models to analyze data from eye-tracking research on subtitling,2022,Translation Spaces(Netherland),11,1,,60,88,,1.0,10.1075/ts.21013.sil,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147277017&doi=10.1075%2fts.21013.sil&partnerID=40&md5=dcc16b668a3bc22f1778955d4008ad11,"In this paper, we aim to promote the use of linear mixed models (LMMs) in eye-tracking research on subtitling. Using eye tracking to study viewers' reading of subtitles often warrants controlling for many confounding variables. However, even assuming that these variables are known to researchers, such control may not be possible or desired. Traditional statistical methods such as t-tests or ANOVAs exacerbate the problem due to the use of aggregated data: each participant has one data point per dependent variable. As a solution, we propose the use of LMMs, which are better suited to account for a number of subtitle and participant characteristics, thus explaining more variance. We introduce essential theoretical aspects of LMMs and highlight some of their advantages over traditional statistical methods. To illustrate our point, we compare two analyses of the same dataset: one using a t-test; another using LMMs. © 2022 John Benjamins Publishing Company.",audiovisual translation; confounding variables; eye tracking; linear mixed-effects models; LMMs; reading; subtitling; traditional statistical methods,,Article,Final,Scopus,2-s2.0-85147277017,Peter,,
"Luo J., Xu L., Wang M., Xie D., Li J., Liu X., He S., Spencer L., Rost G., Guo L.-Y.",55482634200;57235962000;57574863100;57574878300;57574829700;57574846300;15839465400;7102251753;36519555600;24464429800;,Characteristics of Early Expressive Vocabulary in Mandarin-Speaking Children With Cochlear Implants,2022,"Journal of Speech, Language, and Hearing Research",65,11,,4369,4384,,,10.1044/2022_JSLHR-22-00183,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142091800&doi=10.1044%2f2022_JSLHR-22-00183&partnerID=40&md5=0578747ad7ab3d61d0354885bc24ea91,"Purpose: The aim of this study is to evaluate whether Mandarin-speaking children with cochlear implants (CIs) demonstrated early lexical composition similar to their hearing peers who were at the same vocabulary level and the extent to which children with CIs were sensitive to linguistic and conceptual properties when developing early lexicon. Method: Participants were 77 Mandarin-speaking children with CIs who received CIs before 30 months of age. Their expressive vocabulary was docu-mented using the Infant Checklist of the Early Vocabulary Inventory for Mandarin Chinese 9 or 12 months after CI activation. Percent social words, common nouns, predicates (verbs, adjectives), and closed-class words in total vocabulary were computed for children at different vocabulary levels. Common nouns and verbs were further coded for their word class (noun, verb), word frequency, word length, and imageability to predict how likely a given noun or verb would be produced by children with CIs. Results: Like children with typical hearing, social words were the most dominant category when vocabulary size in children with CIs was smaller than 20 words; common nouns became the most dominant category when the vocabulary size reached 21 words. The difference in percent common nouns and percent predicates (i.e., noun bias) was similar in children with CIs and their hearing peers. In addition, verbs, common words, monosyllabic words, and more imageable words were more likely to be produced by children with CIs than their counterparts. Conclusions: Mandarin children with CIs showed language-specific patterns in early lexical composition like their hearing peers. They were able to use multiple linguistic and conceptual cues when approaching early expressive vocabulary despite perceptual and processing constraints. Supplemental Material: https://doi.org/10.23641/asha.21357723. © 2022 American Speech-Language-Hearing Association.",,child; cochlea prosthesis; cochlear implantation; human; infant; language; language development; vocabulary; Child; Cochlear Implantation; Cochlear Implants; Humans; Infant; Language; Language Development; Vocabulary,Article,Final,Scopus,2-s2.0-85142091800,Peter,,
"Yıldız G., Şahin F., Doğan E., Okur M.R.",57192067433;57222560071;57216603730;57521114200;,Influential factors on e-learning adoption of university students with disability: Effects of type of disability,2022,British Journal of Educational Technology,53,6,,2029,2049,,,10.1111/bjet.13235,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139215497&doi=10.1111%2fbjet.13235&partnerID=40&md5=2e5228748ae30b65b5609290beb77895,"The number of studies providing empirical evidence on e-learning in the field of special education is limited. From this point of view, the aim of this study is to examine whether the factors influencing the e-learning adoption of university students with disability differ according to the type of disability and gender. The participants of the study consist of 809 university students with a single disability (orthopaedic, visual or hearing). Confirmatory factor analysis and two-way ANOVA were used in the analysis of the study. The factor analysis showed that validity and reliability of the measurement tool was established and the model fit was good. The results revealed that for all dependent variables (social influence, self-efficacy, compatibility and facilitating conditions), students with orthopaedic disability and visual impairment had higher scores than students with hearing impairment. On the other hand, contrary to expectations, there was no difference between students with orthopaedic disability and students with visual impairment in any of the factors according to the type of disability. In addition, none of the factors differed by gender. The findings provide empirical evidence that can contribute to the planning of online education processes of students with disability by revealing the similarities and differences in the use of e-learning systems according to the type of disability. Practitioner notes What is already known about this topic The field of special education deals with the special needs of individuals and frequently uses technology to provide the support they need. It is observed that educational processes have been moved to online environments due to the COVID-19 pandemic, which has affected the field of education nowadays and e-learning systems in online environments have become indispensable for students with special needs. With their flexible structure, e-learning systems create universal and inclusive learning environments for all students. Accessibility and special education adaptations are needed for individuals with special needs to use e-learning systems effectively. For this, first of all, the effects of the disabilities of these individuals in using e-learning systems should be determined. What this paper adds The results showed that social influence, self-efficacy, compatibility, and facilitating conditions all differed significantly by type of disability. The research findings revealed that all factors differed significantly in terms of orthopaedic and visual impairment versus hearing impairment. The main contribution of the study can be summarized as investigating the relationships between technology adoption and types of disability with empirical evidence and the fact that the implications and inferences made for theory and practice have the potential to make critical contributions to technology acceptance and use of individuals with special needs. It can be stated that the tool adapted in this study offers a valid and reliable tool for future studies with university students with special needs. Implications for practice and/or policy There is a need for considering the individual differences of participants and adaptations for hearing in the adoption of e-learning systems. Individual differences should be taken into account in the designs of e-learning systems, and user-oriented designs should be given priority. It can be mentioned that providing subtitle support for live lessons and recorded videos for the hearing impairment and adding sign and text support to the audio stimuli, providing screen readers for the visual impairment in the e-learning system can make valuable contributions to improving the quality of education. © 2022 British Educational Research Association.",disability; e-learning; special education; special needs; technology adoption; university students,Audition; Computer aided instruction; E-learning; Factor analysis; Flexible structures; Learning systems; Multivariant analysis; Online systems; Ophthalmology; Reliability analysis; Students; Disability; E - learning; E-learning systems; Hearing impairments; Social influence; Special education; Special needs; Technology adoption; University students; Visual impairment; Economic and social effects,Article,Final,Scopus,2-s2.0-85139215497,Peter,,
"Liao J., Karim A., Jadon S.S., Kazi R.H., Suzuki R.",57851210200;57545205700;57851629900;39361806100;57193575409;,RealityTalk: Real-Time Speech-Driven Augmented Presentation for AR Live Storytelling,2022,UIST 2022 - Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology,,,17,,,,,10.1145/3526113.3545702,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141680473&doi=10.1145%2f3526113.3545702&partnerID=40&md5=5c8b4857e75296cbf3f0f42bbc6a51cf,"We present RealityTalk, a system that augments real-time live presentations with speech-driven interactive virtual elements. Augmented presentations leverage embedded visuals and animation for engaging and expressive storytelling. However, existing tools for live presentations often lack interactivity and improvisation, while creating such effects in video editing tools require significant time and expertise. RealityTalk enables users to create live augmented presentations with real-time speech-driven interactions. The user can interactively prompt, move, and manipulate graphical elements through real-time speech and supporting modalities. Based on our analysis of 177 existing video-edited augmented presentations, we propose a novel set of interaction techniques and then incorporated them into RealityTalk. We evaluate our tool from a presenter's perspective to demonstrate the effectiveness of our system. © 2022 ACM.",Augmented Presentation; Augmented Reality; Gestural and Speech Input; Mixed Reality; Natural Language Processing; Video,Mixed reality; Natural language processing systems; Speech; User interfaces; Video signal processing; Augmented presentation; Gestural and speech input; Language processing; Mixed reality; Natural language processing; Natural languages; Real- time; Speech input; Video; Virtual elements; Augmented reality,Conference Paper,Final,Scopus,2-s2.0-85141680473,Peter,,
"Seixas Pereira L., Coelho J., Rodrigues A., Guerreiro J., Guerreiro T., Duarte C.",56104146500;57193448346;56577139300;56967056100;23396568900;55938888000;,Authoring accessible media content on social networks,2022,ASSETS 2022 - Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility,,,28,,,,,10.1145/3517428.3544882,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141218445&doi=10.1145%2f3517428.3544882&partnerID=40&md5=35ced6f58c9c6060a0cb2e4938b6857d,"User-generated content plays a key role in social networking, allowing a more active participation, socialisation, and collaboration among users. In particular, media content has been gaining a lot of ground, allowing users to express themselves through different types of formats such as images, GIFs and videos. The majority of this growing type of online visual content remains inaccessible to a part of the population, in particular for those who have a visual disability, despite available tools to mitigate this source of exclusion. We sought to understand how people are perceiving this type of online content in their networks and how support tools are being used. To do so, we conducted a user study, with 258 social network users through an online questionnaire, followed by interviews with 20 of them-7 blind users and 13 sighted users. Results show how the different approaches being employed by major platforms may not be sufficient to address this issue properly. Our findings reveal that users are not always aware of the possibility and the benefits of adopting accessible practices. From the general perspectives of end-users experiencing accessible practices, concerning barriers encountered, and motivational factors, we also discuss further approaches to create more user engagement and awareness. © 2022 Owner/Author.",accessibility; social media; user-generated content; visual content,Social networking (online); Accessibility; Media content; Online content; Social media; Social-networking; Support tool; User-generated; User-generated content; Visual content; Visual disability; Surveys,Conference Paper,Final,Scopus,2-s2.0-85141218445,Peter,,
"Hong J., Gandhi J., Mensah E.E., Zeraati F.Z., Jarjue E., Lee K., Kacorri H.",56154510700;57289006100;57543408000;57205695898;57211748500;57207046391;35487798800;,Blind Users Accessing Their Training Images in Teachable Object Recognizers,2022,ASSETS 2022 - Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility,,,14,,,,,10.1145/3517428.3544824,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141211593&doi=10.1145%2f3517428.3544824&partnerID=40&md5=2a49e11a0bfaeb267d572bb64d855a85,"Teachable object recognizers provide a solution for a very practical need for blind people-instance level object recognition. They assume one can visually inspect the photos they provide for training, a critical and inaccessible step for those who are blind. In this work, we engineer data descriptors that address this challenge. They indicate in real time whether the object in the photo is cropped or too small, a hand is included, the photos is blurred, and how much photos vary from each other. Our descriptors are built into open source testbed iOS app, called MYCam. In a remote user study in (N = 12) blind participants' homes, we show how descriptors, even when error-prone, support experimentation and have a positive impact in the quality of training set that can translate to model performance though this gain is not uniform. Participants found the app simple to use indicating that they could effectively train it and that the descriptors were useful. However, many found the training being tedious, opening discussions around the need for balance between information, time, and cognitive load. © 2022 ACM.",blind; machine teaching; object recognition; participatory machine learning; visual impairment,Machine learning; Blind; Blind people; Blind users; Descriptors; Machine-learning; Objects recognition; Participatory machine learning; Real- time; Training image; Visual impairment; Object recognition,Conference Paper,Final,Scopus,2-s2.0-85141211593,Peter,,
"Iijima R., Shitara A., Ochiai Y.",57558816400;57220120132;36454884800;,Designing Gestures for Digital Musical Instruments: Gesture Elicitation Study with Deaf and Hard of Hearing People,2022,ASSETS 2022 - Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility,,,18,,,,,10.1145/3517428.3544828,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141204353&doi=10.1145%2f3517428.3544828&partnerID=40&md5=9f86d12a214ddb7a84435384f4758788,"When playing musical instruments, deaf and hard-of-hearing (DHH) people typically sense their music from the vibrations transmitted by the instruments or the movements of their bodies while performing. Sensory substitution devices now exist that convert sounds into light and vibrations to support DHH people's musical activities. However, these devices require specialized hardware, and the marketing profiles assume that standard musical instruments are available. Hence, a significant gap remains between DHH people and their musical performance enjoyment. To address this issue, this study identifies end users' preferred gestures when using smartphones to emulate the musical experience based on the instrument selected. This gesture elicitation study applies 10 instrument types. Herein, we present the results and a new taxonomy of musical instrument gestures. The findings will support the design of gesture-based instrument interfaces to enable DHH people to more directly enjoy their musical performances. © 2022 ACM.",Deaf; gesture elicitation study; hard of hearing; mobile; music,Audition; Musical instruments; Deaf; End-users; Gesture elicitation study; Hard of hearings; Mobile; Musical performance; Sensory substitution; Smart phones; Specialized hardware; Music,Conference Paper,Final,Scopus,2-s2.0-85141204353,Peter,,
McDonnell E.,57222060653;,Understanding Social and Environmental Factors to Enable Collective Access Approaches to the Design of Captioning Technology,2022,ASSETS 2022 - Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility,,,100,,,,,10.1145/3517428.3550417,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141176829&doi=10.1145%2f3517428.3550417&partnerID=40&md5=abcc2c0df7e5d9669a926630564dc915,"Oftentimes human computer interactions (HCI) accessibility research designs technology to support Deaf and disabled people in their existing social contexts. I, instead, propose an approach to accessible technology design that follows the disability justice principle of collective access, envisioning hearing and nondisabled people as key participants in making interactions accessible. Using captioning as a case study, I explore ways that technology could support accessible social norms, achieved by first paying close attention to the social, environmental, and technical factors that shape access for d/Deaf and hard of hearing (DHH) captioning users. My dissertation work will consist of four studies; 1) an exploration of the factors that shape DHH people's current experiences with and future preferences for captioning tools, 2) codesigning features to support accessible group communication with mixed groups of DHH and hearing people, 3) understanding TikTok captioning practices and their impact on DHH users, and 4) exploring the factors that influence professional captioners' work. © 2022 Owner/Author.",Accessibility; Captioning; Collective Access; d/Deaf and Hard of Hearing; Disability Studies,Environmental technology; Human computer interaction; Accessibility; Captioning; Collective access; D/deaf and hard of hearing; Disability study; Environmental factors; Hard of hearings; Research designs; Social and environmental; Social factor; Audition,Conference Paper,Final,Scopus,2-s2.0-85141176829,Peter,,
"Kim Y.S., Lee S., Lee S.",57849578900;57209304672;36142032300;,A Participatory Design Approach to Explore Design Directions for Enhancing Videoconferencing Experience for Non-signing Deaf and Hard of Hearing Users,2022,ASSETS 2022 - Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility,,,47,,,,,10.1145/3517428.3550375,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141160715&doi=10.1145%2f3517428.3550375&partnerID=40&md5=deb20b4a2ecef9531dad3336455ed502,"The breakout of the COVID-19 pandemic shifted people's daily activities from in-person to video-mediated ones. Many people with hearing loss encounter cognitive overload due to ineffective visuals of the videoconferencing interface and therefore find meeting contents difficult to comprehend. This research incorporates a participatory design methodology to investigate the Deaf and Hard of Hearing (DHH) users' tacit needs. DHH users demonstrated ways of mitigating their hardships in the workshop, such as emphasizing the visual hierarchy or assigning visual cues to fixed positions. These findings are used in developing design directions for creating a more inclusive online environment. © 2022 Owner/Author.",Accessibility; Hard of hearing; Videoconferencing,Audition; Design; Video conferencing; Accessibility; Cognitive overload; Daily activity; Design approaches; Design Methodology; Hard of hearings; Hearing loss; Participatory design; Videoconferencing; Visual cues; COVID-19,Conference Paper,Final,Scopus,2-s2.0-85141160715,Peter,,
"Bandukda M., Barbareschi G., Singh A., Jain D., Das M., Motahar T., Wiese J., Cockburn L., Prakash A., Frohlich D., Holloway C.",36699313100;57038765200;57199212151;57014317900;57202051135;57202818340;36444373900;6508290876;35727751700;7005428937;7101643482;,A Workshop on Disability Inclusive Remote Co-Design,2022,ASSETS 2022 - Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility,,,103,,,,,10.1145/3517428.3550403,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141159781&doi=10.1145%2f3517428.3550403&partnerID=40&md5=ce2cbbb63b8b6edfc203415bf7b47019,"The COVID-19 pandemic forced researchers to find new ways to continue research, as universities and laboratories experienced closure due to nationwide lockdowns in many countries worldwide, including conducting experiments, workshops, and ethnographic work online. While this had a significant impact on the majority of research work across SIGCHI, research relating to disability and ageing was most impacted due to the additional challenges of recruiting participants, finding accessible online platforms, and ensuring seamless participation while juggling platform accessibility issues, facilitation, and supporting participants' needs. These challenges were more extreme for disabled researchers. In this workshop, we aim to bring together researchers, designers, and practitioners to explore effective strategies and brainstorm actionable guidelines for supporting disability inclusive online research methods and platforms. © 2022 ACM.",accessibility; co-design; disability inclusion; Online research,Accessibility; Co-designs; Disability inclusion; Online platforms; Online research; Research method; Research platforms; COVID-19,Conference Paper,Final,Scopus,2-s2.0-85141159781,Peter,,
"Li Z., Connell S., Dannels W., Peiris R.",57951294700;57952388100;56301125700;23397978800;,SoundVizVR: Sound Indicators for Accessible Sounds in Virtual Reality for Deaf or Hard-of-Hearing Users,2022,ASSETS 2022 - Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility,,,5,,,,,10.1145/3517428.3544817,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141158399&doi=10.1145%2f3517428.3544817&partnerID=40&md5=ae0fe38bf57f8d6db57df4b0bd4627b8,"Sounds provide vital information such as spatial and interaction cues in virtual reality (VR) applications to convey more immersive experiences to VR users. However, it may be a challenge for deaf or hard-of-hearing (DHH) VR users to access the information given by sounds, which could limit their VR experience. To address this limitation, we present ""SoundVizVR"", which explores visualizing sound characteristics and sound types for several types of sounds in VR experience. SoundVizVR uses Sound-Characteristic Indicators to visualize loudness, duration, and location of sound sources in VR and Sound-Type Indicators to present more information about the type of the sound. First, we examined three types of Sound-Characteristic Indicators (On-Object Indicators, Full Mini-Maps and Partial Mini-Maps) and their combinations in a study with 11 DHH participants. We identified that the combination of Full Mini-Map technique and On-Object Indicator was the most preferred visualization and performed best at locating sound sources in VR. Next, we explored presenting more information about the sounds using text and icons as Sound-Type Indicators. A second study with 14 DHH participants found that all Sound-Type Indicator combinations were successful at locating sound sources. © 2022 ACM.",accessibility; audio visualization; deaf and hard-of-hearing; virtual reality,Acoustic generators; Audio acoustics; Audition; Visualization; Accessibility; Audio visualization; Deaf and hard-of-hearing; Hard of hearings; Immersive; Sound indicators; Sound source; Virtual reality experiences; Virtual reality,Conference Paper,Final,Scopus,2-s2.0-85141158399,Peter,,
"Zhang K., Deldari E., Lu Z., Yao Y., Zhao Y.",57868129700;57869153600;57194274057;57193547232;50862300900;,It's Just Part of Me: Understanding Avatar Diversity and Self-presentation of People with Disabilities in Social Virtual Reality,2022,ASSETS 2022 - Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility,,,4,,,,,10.1145/3517428.3544829,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141158032&doi=10.1145%2f3517428.3544829&partnerID=40&md5=c19c89addcfcc3e8f74c26ed67c026fd,"In social Virtual Reality (VR), users are embodied in avatars and interact with other users in a face-to-face manner using avatars as the medium. With the advent of social VR, people with disabilities (PWD) have shown an increasing presence on this new social media. With their unique disability identity, it is not clear how PWD perceive their avatars and whether and how they prefer to disclose their disability when presenting themselves in social VR. We fill this gap by exploring PWD's avatar perception and disability disclosure preferences in social VR. Our study involved two steps. We first conducted a systematic review of fifteen popular social VR applications to evaluate their avatar diversity and accessibility support. We then conducted an in-depth interview study with 19 participants who had different disabilities to understand their avatar experiences. Our research revealed a number of disability disclosure preferences and strategies adopted by PWD (e.g., reflect selective disabilities, present a capable self). We also identified several challenges faced by PWD during their avatar customization process. We discuss the design implications to promote avatar accessibility and diversity for future social VR platforms. © 2022 ACM.",avatar; d/Deaf and heard of hearing; disability disclosure; self-perception; Social VR; visual impairments,Virtual reality; Avatar; D/deaf and heard of hearing; Disability disclosure; Face to face; People with disabilities; Self presentations; Self-perception; Social media; Social virtual reality; Visual impairment; Audition,Conference Paper,Final,Scopus,2-s2.0-85141158032,Peter,,
"Glasser A., Minakov F., Bragg D.",57195128152;57203000945;43660991100;,ASL Wiki: An Exploratory Interface for Crowdsourcing ASL Translations,2022,ASSETS 2022 - Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility,,,16,,,,,10.1145/3517428.3544827,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141143613&doi=10.1145%2f3517428.3544827&partnerID=40&md5=4fdf9e6a88246b759c15ca8ef2c82515,"The Deaf and Hard-of-hearing (DHH) community faces a lack of information in American Sign Language (ASL) and other signed languages. Most informational resources are text-based (e.g. books, encyclopedias, newspapers, magazines, etc.). Because DHH signers typically prefer ASL and are often less fluent in written English, text is often insufficient. At the same time, there is also a lack of large continuous sign language datasets from representative signers, which are essential to advancing sign langauge research and technology. In this work, we explore the possibility of crowdsourcing English-to-ASL translations to help address these barriers. To do this, we present a novel bilingual interface that enables the community to both contribute and consume translations. To shed light on the user experience with such an interface, we present a user study with 19 participants using the interface to both generate and consume content. To better understand the potential impact of the interface on translation quality, we also present a preliminary translation quality analysis. Our results suggest that DHH community members find real-world value in the interface, that the quality of translations is comparable to those created with state-of-the-art setups, and shed light on future research avenues. © 2022 ACM.",ASL data collection; Bilingual; Corpus; Crowdsourcing; Deaf and Hard-of-Hearing; Education; Interface; Sign Language,Audition; Interface states; Large dataset; Translation (languages); User interfaces; American sign language; American sign language data collection; Bilinguals; Corpus; Data collection; Deaf and hard-of-hearing; Hard of hearings; Language translation; Sign language; Translation quality; Crowdsourcing,Conference Paper,Final,Scopus,2-s2.0-85141143613,Peter,,
"Hossain E., Cahoon M.L., Liu Y., Kurumada C., Bai Z.",57804975900;57952609500;57952167100;55640019700;57220554432;,Context-responsive ASL Recommendation for Parent-Child Interaction,2022,ASSETS 2022 - Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility,,,76,,,,1.0,10.1145/3517428.3550366,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141143253&doi=10.1145%2f3517428.3550366&partnerID=40&md5=8cb461bc0431e7c7ccded19173c6b5bf,"Parental language input in early childhood plays a critical role in lifelong neuro-cognitive and social development. Deaf and Hard of Hearing (DHH) children are often at risk of language deprivation due to hearing parents' limited knowledge of sign language-the natural language for DHH children at birth. To offer an immersive sign language environment for DHH children, we designed a novel computer-mediated communication technology named Table Top Interactive System (TIPS). It aims to provide context-responsive recommendation of American Sign Language (ASL) in real-time for hearing parents during face-to-face joint play with their DHH children. The system emphasizes supporting parent autonomy by adapting ASL recommendations using parent's speech during play, and minimizes obtrusion for face-to-face interaction through an Augmented Reality (AR) display. This paper describes the design and development of an initial working prototype of TIPS and preliminary results of the system's efficiency regarding system latency and accuracy for ASL recommendation and visualization. Next, we plan to conduct a user study to gather expert and parent feedback about the system design and ASL recommendation strategies for long-term and personalized usage. © 2022 Owner/Author.",American Sign Language; Augmented Reality; Computer-mediated communication; Parent-Child Interaction,Augmented reality; User interfaces; American sign language; Cognitive development; Computer-mediated communication; Early childhoods; Hard of hearings; Immersive; Natural languages; Parent-child interactions; Sign language; Social development; Audition,Conference Paper,Final,Scopus,2-s2.0-85141143253,Peter,,
"Alonzo O., Shin H.V., Li D.",57215303209;57188550691;55367150700;,Beyond Subtitles: Captioning and Visualizing Non-speech Sounds to Improve Accessibility of User-Generated Videos,2022,ASSETS 2022 - Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility,,,26,,,,,10.1145/3517428.3544808,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141141660&doi=10.1145%2f3517428.3544808&partnerID=40&md5=68d3016c35238ad4cc97e71f04af7f4f,"Captioning provides access to sounds in audio-visual content for people who are Deaf or Hard-of-hearing (DHH). As user-generated content in online videos grows in prevalence, researchers have explored using automatic speech recognition (ASR) to automate captioning. However, definitions of captions (as compared to subtitles) include non-speech sounds, which ASR typically does not capture as it focuses on speech. Thus, we explore DHH viewers' and hearing video creators' perspectives on captioning non-speech sounds in user-generated online videos using text or graphics. Formative interviews with 11 DHH participants informed the design and implementation of a prototype interface for authoring text-based and graphic captions using automatic sound event detection, which was then evaluated with 10 hearing video creators. Our findings include identifying DHH viewers' interests in having important non-speech sounds included in captions, as well as various criteria for sound selection and the appropriateness of text-based versus graphic captions of non-speech sounds. Our findings also include hearing creators' requirements for automatic tools to assist them in captioning non-speech sounds. © 2022 ACM.",accessibility; audio tagging; automatic captions; non-speech sounds,Audio acoustics; Audition; Accessibility; Audio tagging; Automatic caption; Automatic speech recognition; Hard of hearings; Non speech; Non-speech sound; Online video; Speech sounds; User-generated; Speech recognition,Conference Paper,Final,Scopus,2-s2.0-85141141660,Peter,,
"Ohshiro K., Cartwright M.",57952634300;53979459000;,"How people who are deaf, Deaf, and hard of hearing use technology in creative sound activities",2022,ASSETS 2022 - Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility,,,66,,,,,10.1145/3517428.3550396,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141131990&doi=10.1145%2f3517428.3550396&partnerID=40&md5=fbbb78c2e271d4352e5949f2588bcac8,"Creative sound activities, such as music playing and audio engineering, are said to have been democratized with the development of technology. Yet, the use of technology in creative sound activities by people who are deaf, Deaf, and hard of hearing (DHH) has been underexplored by the research community. To address this gap, we conducted an online survey with 50 DHH participants to understand their use of technology and barriers they face in their creative sound activities. We find DHH people use four types of technology-hearing devices, sound manipulation, sound visualization, and speech-To-Text-for three purposes-To improve sound perception via auditory and visual means, to avoid hearing fatigue, and to better communicate with hearing people. We also find their barriers to technology: unknown availability, limited options, and limitations that technology can solve. We discuss opportunities for more inclusive design specific to DHH people's creative sound activities, as well as facilitating access to information about technology. © 2022 Owner/Author.",accessibility; audio engineering; deaf; Deaf; hard of hearing; music,Audio acoustics; Audition; Surveys; Accessibility; Audio engineering; Creatives; Deaf; Hard of hearings; Hearing devices; Online surveys; Research communities; Type of technology; Music,Conference Paper,Final,Scopus,2-s2.0-85141131990,Peter,,
Anderton C.,57952867000;,Investigating Sign Language Interpreter Rendering and Guiding Methods in Virtual Reality 360-Degree Content,2022,ASSETS 2022 - Proceedings of the 24th International ACM SIGACCESS Conference on Computers and Accessibility,,,88,,,,,10.1145/3517428.3563373,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141129192&doi=10.1145%2f3517428.3563373&partnerID=40&md5=c8839e6b780fffccdabe3afca2ae3273,"As sign languages are often the native languages for members of the Deaf community, text captioning can be inaccessible, highlighting the importance of sign language interpretation. This research explores sign language in virtual reality 360-degree three degrees of freedom videos, exploring two rendering modes, fixed-position and always-visible, and two visual guiding methods, arrows and radar. Findings from testing with eight participants indicates that fixed-position rendering provides participants with a greater sense of presence than always-visible rendering, whilst always-visible rendering produces less of a blocking effect. Arrows appear more usable than radar for visually guiding participants to active speakers, including providing a higher level of sign language understanding. Future research is needed to validate these findings with six degrees of freedom content. © 2022 ACM.",360-Degree Video; Extended Reality; Sign Language; Virtual Reality,Degrees of freedom (mechanics); Radar; Rendering (computer graphics); Visual languages; 360-degree video; Blocking effect; Extended reality; Language interpreters; Language understanding; Native language; Sense of presences; Sign language; Six degrees of freedom; Three degree of freedoms; Virtual reality,Conference Paper,Final,Scopus,2-s2.0-85141129192,Peter,,
"Lu Y.-J., Kuo I.-C., Ho M.-C.",57915599100;57914762400;23469446000;,The effects of emotional films and subtitle types on eye movement patterns,2022,Acta Psychologica,230,,103748,,,,,10.1016/j.actpsy.2022.103748,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139228344&doi=10.1016%2fj.actpsy.2022.103748&partnerID=40&md5=57e96b452b7a9fac77e99e75b4d0a5fb,"Background: In Taiwan, the use of subtitle is common in TV programs and movies. However, studies on subtitles mostly focus on foreign language learning and film subtitle translation. Few studies address how subtitle types and emotion-laden films affect the viewers' eye movement patterns. Purpose: We aim to examine how the emotion type of film (happy, sad, angry, fear, or neutral) and subtitle type (meaningful subtitle, no subtitle, or meaningless subtitle) affect the dwell times and fixation counts in the subtitle area. Methods: This study is a 5 (emotion type of film) × 3 (subtitle type) between-participants design. There were 15 participants per condition, resulting in a total of 225 participants. After watching a film, participants filled out a self-reported questionnaire regarding this film. Results: The subtitled films have more fixation counts and dwell time for the meaningful subtitle compared to meaningless subtitle and no subtitle. The dwell time was longer on the subtitle area for the sad film than the neutral and happy films. Also, the dwell time was longer on the subtitle area for the fear film than the happy film. There were more fixation counts on the subtitle area for the sad film than the angry and happy films. Conclusions: The subtitle meaning is critical in directing overt attention. Also, overt attention directed to the subtitle area is affected by the different emotion types of films. © 2022 The Authors",Emotional film; Eye tracking; Overt attention; Subtitle,attention; emotion; eye movement; facial expression; happiness; human; movie; Attention; Emotions; Eye Movements; Facial Expression; Happiness; Humans; Motion Pictures,Article,Final,Scopus,2-s2.0-85139228344,Peter,,
"Bhavya B., Chen S., Zhang Z., Li W., Zhai C., Angrave L., Huang Y.",57224636404;57218770802;57215652265;57807185100;35232046000;57202547111;8848794300;,Exploring collaborative caption editing to augment video-based learning,2022,Educational Technology Research and Development,70,5,,1755,1779,,,10.1007/s11423-022-10137-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134311910&doi=10.1007%2fs11423-022-10137-5&partnerID=40&md5=6d0698998dadeeb5fb3aef582c2e00e1,"Captions play a major role in making educational videos accessible to all and are known to benefit a wide range of learners. However, many educational videos either do not have captions or have inaccurate captions. Prior work has shown the benefits of using crowdsourcing to obtain accurate captions in a cost-efficient way, though there is a lack of understanding of how learners edit captions of educational videos either individually or collaboratively. In this work, we conducted a user study where 58 learners (in a course of 387 learners) participated in the editing of captions in 89 lecture videos that were generated by Automatic Speech Recognition (ASR) technologies. For each video, different learners conducted two rounds of editing. Based on editing logs, we created a taxonomy of errors in educational video captions (e.g., Discipline-Specific, General, Equations). From the interviews, we identified individual and collaborative error editing strategies. We then further demonstrated the feasibility of applying machine learning models to assist learners in editing. Our work provides practical implications for advancing video-based learning and for educational video caption editing. © 2022, Association for Educational Communications and Technology.",Caption transcription; Collaborative editing; Lecture video caption editing; Technology-assisted editing,,Article,Final,Scopus,2-s2.0-85134311910,Peter,,
"Britain G.W., Martin D., Kwok T., Sumilong A., Starner T.",57218762481;58046184600;58046159700;58046159800;7003469397;,Preferences for Captioning on Emulated Head Worn Displays While in Group Conversation,2022,"Proceedings - International Symposium on Wearable Computers, ISWC",,,,17,22,,,10.1145/3544794.3558468,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145880331&doi=10.1145%2f3544794.3558468&partnerID=40&md5=14ecbfdaf63fef51235d42950355d514,"Head worn displays (HWDs) can provide a discreet method of captioning for people who are d/Deaf or hard of hearing (DHH); however, group conversations remain a difficult scenario as the wearer has difficulty in determining who is speaking and where to look. Using an HWD emulator during a group conversation, we compare eight DHH users' perceptions of four conditions: an 80 degree field-of-view (FOV) HWD that pins captioning text to each speaker (Registered), a HWD where the captioning remains in the same place in the user's visual field (Non-registered), Non-registered plus indicators as to which direction the current speaker is relative to the user's line of sight (Indicators), and a control of captions displayed on a Phone. Preference increased in order of Phone, Non-registered, Indicators, and Registered. While an 80 degree FOV HWD is not practical to create in a pair of normal looking eyeglasses, pilot testing with 12 hearing participants suggests a FOV between 20 and 30 degrees might be sufficient. © 2022 Owner/Author.",accessibility; captioning; Deaf; Hard-of-Hearing; head-worn-display,Telephone sets; 'current; Accessibility; Captioning; Condition; Deaf; Field of views; Hard of hearings; Head-worn displays; User perceptions; Visual fields; Audition,Conference Paper,Final,Scopus,2-s2.0-85145880331,Peter,,
"Chen K., Huang Y., Chen Y., Zhong H., Lin L., Wang L., Wu K.",57222492062;57195481652;57219539813;57914743900;57914311500;57190281570;22982222400;,LiSee: A Headphone that Provides All-day Assistance for Blind and Low-vision Users to Reach Surrounding Objects,2022,"Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",6,3,104,,,,,10.1145/3550282,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139179298&doi=10.1145%2f3550282&partnerID=40&md5=b972c649438072d6c7c5ff15d00f6780,"Reaching surrounding target objects is difficult for blind and low-vision (BLV) users, affecting their daily life. Based on interviews and exchanges, we propose an unobtrusive wearable system called LiSee to provide BLV users with all-day assistance. Following a user-centered design method, we carefully designed the LiSee prototype, which integrates various electronic components and is disguised as a neckband headphone such that it is an extension of the existing headphone. The top-level software includes a series of seamless image processing algorithms to solve the challenges brought by the unconstrained wearable form so as to ensure excellent real-time performance. Moreover, users are provided with a personalized guidance scheme so that they can use LiSee quickly based on their personal expertise. Finally, a system evaluation and a user study were completed in the laboratory and participants' homes. The results show that LiSee works robustly, indicating that it can meet the daily needs of most participants to reach surrounding objects. © 2022 ACM.",All-day Assistance; Reach Object; Speech Interface; Visual Impairments; Wearable System,User centered design; Wearable technology; All-day assistance; Daily lives; Electronic component; Low vision; Reach object; Speech interface; Target object; User centered design methods; Visual impairment; Wearable systems; Image processing,Article,Final,Scopus,2-s2.0-85139179298,Peter,,
"Stinson M., Gamta-Poddar R., Meyer L., Powers-Blom C., Singer S.",7005000200;58017481000;58018139900;58018140000;57219486079;,Effects of Messaging and Communication Strategy Training on Interaction in Teams With Deaf and Hearing College Students,2022,American Annals of the Deaf,167,4,,431,456,,,10.1353/aad.2022.0043,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144050934&doi=10.1353%2faad.2022.0043&partnerID=40&md5=9e0f40999feac3de5b5fbe7e8907364d,"The study investigated effects of computer-based messaging and training in communication strategies on interactions of deaf and typically hearing (TH) teammates in completing decision-making tasks without interpreter support. Fifteen teams, two deaf and two TH college students each, completed three decision-making tasks, one without messaging, one with messaging, and one with messaging and training in communication strategies. Each interaction was coded for (a) communication method, (b) to whom the interaction was directed, and (c) the function of the interaction. Without messaging, teams used speech, sign, or paper and pencil; with messaging, they predominantly used this technology to communicate with each other. Without messaging, teammates directed communications to members of the same hearing status; with messaging, they directed communications to the whole team. Teammates made fewer communication repairs with messaging than without. In focus groups conducted after the decision-making tasks, participants noted messaging’s limitations and benefits. © 2022, Gallaudet University Press. All rights reserved.",mainstream; messaging; postsecondary; problem solving; small groups; technology,hearing; hearing impaired person; hearing impairment; human; interpersonal communication; speech; student; Communication; Deafness; Hearing; Humans; Persons With Hearing Impairments; Speech; Students,Article,Final,Scopus,2-s2.0-85144050934,Peter,,
"Wang Y., Du M., Yu K., Shen G., Deng T., Wang R.",57833370700;57222816036;56364460900;57833074500;57832782700;56128487100;,Bi-directional cross-language activation in Chinese Sign Language (CSL)-Chinese bimodal bilinguals,2022,Acta Psychologica,229,,103693,,,,,10.1016/j.actpsy.2022.103693,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135504509&doi=10.1016%2fj.actpsy.2022.103693&partnerID=40&md5=60668324e1d37a38ad3e138ce6a8d487,"In bilingual word recognition, cross-language activation has been found in unimodal bilinguals (e.g., Chinese-English bilinguals) and bimodal bilinguals (e.g., American Sign language-English bilinguals). However, it remains unclear how signs' phonological parameters, spoken words' orthographic and phonological representation, and language proficiency affect cross-language activation in bimodal bilinguals. To resolve the issues, we recruited deaf Chinese sign language (CSL)-Chinese bimodal bilinguals as participants. We conducted two experiments with the implicit priming paradigm and the semantic relatedness decision task. Experiment 1 first showed cross-language activation from Chinese to CSL, and the CSL words' phonological parameter affected the cross-language activation. Experiment 2 further revealed inverse cross-language activation from CSL to Chinese. The Chinese words' orthographic and phonological representation played a similar role in the cross-language activation. Moreover, a comparison between Experiments 1 and 2 indicated that language proficiency influenced cross-language activation. The findings were further discussed with the Bilingual Interactive Activation Plus (BIA+) model, the deaf BIA+ model, and the Bilingual Language Interaction Network for Comprehension of Speech (BLINCS) model. © 2022 The Author(s)",Bilingual word recognition; Chinese Sign Language (CSL)-Chinese bimodal bilinguals; Cross-language activation,China; human; language; multilingualism; semantics; sign language; China; Humans; Language; Multilingualism; Semantics; Sign Language,Article,Final,Scopus,2-s2.0-85135504509,Peter,,
"Li A., Yang R., Qu J., Dong J., Gu L., Mei L.",57212244321;57223005381;57196150756;57214781227;57678803500;23051195600;,Neural representation of phonological information during Chinese character reading,2022,Human Brain Mapping,43,13,,4013,4029,,2.0,10.1002/hbm.25900,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129841095&doi=10.1002%2fhbm.25900&partnerID=40&md5=b166dbc08f595cadcc438f3cedcac21f,"Previous studies have revealed that phonological processing of Chinese characters elicited activation in the left prefrontal cortex, bilateral parietal cortex, and occipitotemporal regions. However, it is controversial what role the left middle frontal gyrus plays in Chinese character reading, and whether the core regions (e.g., the left superior temporal gyrus and supramarginal gyrus) for phonological processing of alphabetic languages are also involved in Chinese character reading. To address these questions, the present study used both univariate and multivariate analysis (i.e., representational similarity analysis, RSA) to explore neural representations of phonological information during Chinese character reading. Participants were scanned while performing a reading aloud task. Univariate activation analysis revealed a widely distributed network for word reading, including the bilateral inferior frontal gyrus, middle frontal gyrus, lateral temporal cortex, and occipitotemporal cortex. More importantly, RSA showed that the left prefrontal (i.e., the left middle frontal gyrus and left inferior frontal gyrus) and bilateral occipitotemporal areas (i.e., the left inferior and middle temporal gyrus and bilateral fusiform gyrus) represented phonological information of Chinese characters. These results confirmed the importance of the left middle frontal gyrus and regions in ventral pathway in representing phonological information of Chinese characters. © 2022 The Authors. Human Brain Mapping published by Wiley Periodicals LLC.",Chinese character; fMRI; phonological processing; reading; representational similarity analysis,activation analysis; adult; article; Chinese script; female; functional magnetic resonance imaging; fusiform gyrus; human; human experiment; inferior frontal gyrus; male; middle frontal gyrus; middle temporal gyrus; reading; temporal cortex; brain mapping; China; language; nuclear magnetic resonance imaging; procedures; reading; Brain Mapping; China; Humans; Language; Magnetic Resonance Imaging; Reading,Article,Final,Scopus,2-s2.0-85129841095,Peter,,
"Chen X., Li D., Wang X.",57405522400;35215276400;57077301900;,Uighur college students’ irony comprehension in Chinese,2022,International Journal of Bilingualism,26,4,,450,475,,,10.1177/13670069211056128,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124137038&doi=10.1177%2f13670069211056128&partnerID=40&md5=b422829f30a3112059f17c1e8379b615,"Aims and objectives/purpose/research questions: Irony comprehension can be more demanding than literal comprehension in L1. This study aimed to seek an answer to how bilinguals perform in L2 in irony comprehension. Design/methodology/approach: Totally, 85 Uighur College students participated in a self-paced reading task in Chinese, with 81 Chinese native speakers as the controls. In Experiment 1, a scenario was followed by a commentary statement in which the critical word either was literally congruent with the context in meaning or could only be ironically understood. In Experiment 2, the same statement was preceded by three sentences which were either literally consistent with the critical word or created a context for the critical word to be understood ironically. Data and analysis: ANOVAs were conducted on participants reading times (RTs) to the critical words and commentary statement endings in the 14 pairs of discourses. They did not have different RTs for the critical words across the ironic and non-ironic conditions in L1, but had significantly longer RTs in the ironic condition than in the non-ironic condition in L2. Their RTs for the commentary statement endings were significantly longer in the ironic condition than in the non-ironic condition in both experiments, regardless of whether the materials were presented in L1 or L2. Findings/conclusions: Irony comprehension is similar in L2 to how it is in L1. However, salient meaning retrieval (in Experiment 1) and inference-making (in Experiment 2) in irony comprehension, as assumed by the Graded Salience Hypothesis, were more likely to be revealed in L2 than in L1. Originality: This seems to be the first study in the native and non-native domain of irony processing in the procedure of discourse reading. Significance/implications: L2 learners should do as many practices as possible to improve their reading proficiency in the target language. © The Author(s) 2021.",Chinese; irony comprehension; L2 learners; the Graded Salience Hypothesis; Uighur students,,Article,Final,Scopus,2-s2.0-85124137038,Peter,,
"Ma Y., Shi Y., Zhang M., Li W., Ma C., Guo Y.",57875258500;57875498400;57875258600;57875852200;57875381400;57191161506;,Design and Implementation of an Intelligent Assistive Cane for Visually Impaired People Based on an Edge-Cloud Collaboration Scheme,2022,Electronics (Switzerland),11,14,2266,,,,,10.3390/electronics11142266,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137303558&doi=10.3390%2felectronics11142266&partnerID=40&md5=e16ae06fae42ed2fbb0d89613d48d73c,"Visually impaired people face many inconveniences in daily life, and there are problems such as high prices and single functions in the market of assistance tools for visually impaired people. In this work, we designed and implemented a low-cost intelligent assistance cane, particularly for visually impaired individuals, based on computer vision, sensors, and an edge-cloud collaboration scheme. Obstacle detection, fall detection, and traffic light detection functions have been designed and integrated for the convenience of moving for visually impaired people. We have also designed an image captioning function and object detection function with high-speed processing capability based on an edge-cloud collaboration scheme to improve the user experience. Experiments show that the performance metrics have an aerial obstacle detection accuracy of 92.5%, fall detection accuracy of 90%, and average image retrieval period of 1.124 s. It proves the characteristics of low power consumption, strong real-time performance, adaptability to multiple scenarios, and convenience, which can ensure the safety of visually impaired people when moving and can help them better perceive and understand the surrounding environment. © 2022 by the authors.",edge-cloud collaboration; image captioning; intelligent cane; visually impaired,,Article,Final,Scopus,2-s2.0-85137303558,Peter,,
"García López Á., Cerdán V., Ortiz T., Sánchez Pena J.M., Vergaz R.",57854714700;57220204764;7005899324;57200196481;6602783927;,Emotion Elicitation through Vibrotactile Stimulation as an Alternative for Deaf and Hard of Hearing People: An EEG Study,2022,Electronics (Switzerland),11,14,2196,,,,,10.3390/electronics11142196,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136375607&doi=10.3390%2felectronics11142196&partnerID=40&md5=ead8f89b95629a4fb108343d38461310,"Despite technological and accessibility advances, the performing arts and their cultural offerings remain inaccessible to many people. By using vibrotactile stimulation as an alternative channel, we explored a different way to enhance emotional processes produced while watching audiovisual media and, thus, elicit a greater emotional reaction in hearing-impaired people. We recorded the brain activity of 35 participants with normal hearing and 8 participants with severe and total hearing loss. The results showed activation in the same areas both in participants with normal hearing while watching a video, and in hearing-impaired participants while watching the same video with synchronized soft vibrotactile stimulation in both hands, based on a proprietary stimulation glove. These brain areas (bilateral middle frontal orbitofrontal, bilateral superior frontal gyrus, and left cingulum) have been reported as emotional and attentional areas. We conclude that vibrotactile stimulation can elicit the appropriate cortex activation while watching audiovisual media. © 2022 by the authors.",accessibility; audiovisual; electroencephalography; hearing impairment; music emotion elicitation; vibrotactile,,Article,Final,Scopus,2-s2.0-85136375607,Peter,,
"Bai Z., Codick E., Tenesaca A., Hu W., Yu X., Hao P., Kurumada C., Hall W.",57220554432;57802996400;57215280566;57215286954;57802534400;57802760100;55640019700;57192296085;,Signing-on-the-Fly: Technology Preferences to Reduce Communication Gap between Hearing Parents and Deaf Children,2022,"Proceedings of Interaction Design and Children, IDC 2022",,,,26,36,,2.0,10.1145/3501712.3529741,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134184685&doi=10.1145%2f3501712.3529741&partnerID=40&md5=211c261b561e8a2c2dc54ec4b0a0671f,"Over 90 percent of Deaf and Hard of Hearing (DHH) children in the United States are born to hearing parents, who have little to no command of American Sign Language (ASL). This leaves the majority of DHH children at risk of language deprivation in early childhood. This study investigates the design space of Augmented Reality (AR) and wearable technologies in supporting hearing parents to offer sign language environments for young DHH children. We conducted an online survey with 65 participants (hearing/DHH parents and teachers of DHH children aged 6 months to 5 years) to gather preferences and interests of technologies that support hearing parents to deliver ASL on-the-fly, and stay attentive to the DHH child's visual attention during joint toy play. We found that Near-Object Projection is most preferred for real-time ASL delivery, and haptic feedback is most preferred for raising the parent's awareness of a child's attention. Results also show a strong interest in using the proposed technologies in interacting with and maintaining joint attention with DHH children on a daily basis. We discuss key design recommendations that inform the design of future technologies that support just-in-time and contextual-aware communication in ASL, with minimal obtrusion to face-to-face interaction. © 2022 ACM.",American Sign Language; Augmented Reality; Computer-Mediated Communication; Joint Attention; Parent-Child Interaction; Wearable Technologies,Augmented reality; Behavioral research; Surveys; Wearable technology; American sign language; Communication gaps; Computer-mediated communication; Design spaces; Early childhoods; Hard of hearings; Joint attention; Language environment; Parent-child interactions; Sign language; Audition,Conference Paper,Final,Scopus,2-s2.0-85134184685,Peter,,
"Maeda K., Arakawa R., Rekimoto J.",57604912900;57209398650;6603848632;,CalmResponses: Displaying Collective Audience Reactions in Remote Communication,2022,IMX 2022 - Proceedings of the 2022 ACM International Conference on Interactive Media Experiences,,,,193,208,,,10.1145/3505284.3529959,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133914516&doi=10.1145%2f3505284.3529959&partnerID=40&md5=487e11b716cdce2868719e5bae362e5e,"We propose a system displaying audience eye gaze and nod reactions for enhancing synchronous remote communication. Recently, we have had increasing opportunities to speak to others remotely. In contrast to offline situations, however, speakers often have difficulty observing audience reactions at once in remote communication, which makes them feel more anxious and less confident in their speeches. Recent studies have proposed methods of presenting various audience reactions to speakers. Since these methods require additional devices to measure audience reactions, they are not appropriate for practical situations. Moreover, these methods do not present overall audience reactions. In contrast, we design and develop CalmResponses, a browser-based system which measures audience eye gaze and nod reactions only with a built-in webcam and collectively presents them to speakers. The results of our two user studies indicated that the number of fillers in speaker's speech decreases when audiences' eye gaze is presented, and their self-rating score increases when audiences' nodding is presented. Moreover, comments from audiences suggested benefits of CalmResponses for them in terms of co-presence and privacy concerns. © 2022 ACM.",audience sensing; eye gaze; feedback design; nodding; remote communication,Audience reaction; Audience sensing; Eye-gaze; Feed-back designs; Nodding; Offline; Remote communication; Self-ratings; User study; WebCams,Conference Paper,Final,Scopus,2-s2.0-85133914516,Peter,,
"Chen S., Liu Y., Lu R., Zhou Y., Lee Y.-C., Huang Y.",57218770802;57786541400;57787739200;57202882953;56157149200;8848794300;,"""Mirror, Mirror, on the Wall"" - Promoting Self-Regulated Learning using Affective States Recognition via Facial Movements",2022,DIS 2022 - Proceedings of the 2022 ACM Designing Interactive Systems Conference: Digital Wellbeing,,,,1300,1314,,,10.1145/3532106.3533500,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133647567&doi=10.1145%2f3532106.3533500&partnerID=40&md5=22f93843c8c336ada2bb442853618769,"Prior research suggests that affective states of self-regulated learning can be used to improve learners' cognitive processes and their learning outcomes. However, little research explored the effect of using facial movements to detect learners' affective states on self-regulated learning. In this work, we designed, implemented, and evaluated Mirror: a self-regulated learning tool that applies facial expression recognition to support learners' reflections in video-based learning. We conducted two studies to identify user needs (with 12 participants) and to evaluate the tool (with 16 participants). The results show that, after watching a video, participants benefited from using Mirror through different reflection processes, e.g., gaining a deeper understanding of their learning experiences through self-observation and attributing causes for their learning affects through self-judgment. Meanwhile, we also identified several ethical concerns, e.g., users' agency of handling the uncertainty of AI, reactivity towards outcome-based AI, over-reliance on ""positive""AI results, and fairness of AI informed decision-making. © 2022 ACM.",Affective Computing; Emotion; Mixed Methods; Video-based Learning,Decision making; Emotion Recognition; Face recognition; Learning systems; Affective Computing; Affective state; Cognitive process; Emotion; Facial movements; Learning outcome; Mixed method; Self-regulated learning; State recognition; Video-based learning; Mirrors,Conference Paper,Final,Scopus,2-s2.0-85133647567,Peter,,
"Sinclair N., Ballenger S., Linden M.",57868473300;57224096022;7202047579;,Voices from Industry Inclusive Design Thinking for Health Messaging in American Sign Language during the COVID-19 Pandemic: A Case Study Brief,2022,Assistive Technology Outcomes and Benefits,16,Special Issue 2,,74,85,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137042969&partnerID=40&md5=7d1c1b96a4380c3bd60dbd23f5384467,"Health information needs to be accessible to all people, especially in emergencies and critical times of need such as the COVID-19 pandemic. Health information needs to be designed to meet the needs of a broad range of people, including Deaf and hard of hearing people who use American Sign Language. An Inclusive Design Thinking framework provides the process and structure for collaborative teams to work together to produce solutions that meet the needs of diverse audiences, including people with disabilities. Design Thinking is a human-centered problem-solving method that puts users at the center of the design process. Inclusive Design Thinking includes the end users throughout the design process, considers barriers users may face when accessing information, and seeks to remove these barriers through information design that is accessible to the intended audience. This case study provides the details of a collaborative effort by Centers for Disease Control and Prevention (CDC), Georgia Tech Center for Inclusive Design and Innovation (CIDI), ASL interpreters, Deaf and hard of hearing community members and advocates, and other community members to design and disseminate health information during the COVID-19 pandemic while addressing health literacy and digital accessibility best practices. © ATIA 2022.",American Sign Language; Design Thinking; emergency management; health education; health literacy; Inclusive Design; Inclusive Design Thinking; risk communications,,Article,Final,Scopus,2-s2.0-85137042969,Peter,,
Shea P.,57815142600;,"Introduction to OLJ Volume 26, Issue 3",2022,Online Learning Journal,26,2,,1,5,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134714473&partnerID=40&md5=81d763ab1870322ab870334c00667b48,[No abstract available],,,Editorial,Final,Scopus,2-s2.0-85134714473,Peter,,
"Brewer R., Pierce C., Upadhyay P., Park L.",36616888500;56559495900;57210728333;57822089300;,An Empirical Study of Older Adult's Voice Assistant Use for Health Information Seeking,2022,ACM Transactions on Interactive Intelligent Systems,12,2,13,,,,3.0,10.1145/3484507,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134150441&doi=10.1145%2f3484507&partnerID=40&md5=99143f009fa44f8a84fef4e1bc4b992c,"Although voice assistants are increasingly being adopted by older adults, we lack empirical research on how they interact with these devices for health information seeking. Also, prior work shows how voice assistant responses can provide misleading or inaccurate information and be harmful particularly in health contexts. Because of increased health needs while aging, this paper studies older adult's (ages 65+) health-related voice assistant interactions. Motivated by a lack of empirical evidence for how older adults approach information seeking with emerging technologies, we first conducted a survey of n = 201 older adults to understand how they engage voice assistants compared to a range of offline and digital sources for health information seeking. Findings show how voice assistants were used for confirmatory health queries, with users showing signs of distrust. As much prior work focuses on perceptions of voice assistant use, we conducted scenario-based interviews with n = 35 older adults to study health-related voice assistant behavior. In interviews, participants engaged with different health topics (flu, migraine, high blood pressure) and scenario types (symptom-driven, behavior-driven) using a voice assistant. Findings show how conversational and human-like expectations with voice assistants lead to information breakdowns between the older adult and voice assistant. This paper contributes a nuanced query-level analysis of older adults' voice-based health information seeking behaviors. Further, data provide evidence for how query reformulation happens with complex topics in voice-based information seeking. We use our findings to discuss how voice interfaces can better support older adults' health information seeking behaviors and expectations. © 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.",health; interactive systems; older adults; search; Voice assistants,Blood pressure; Information retrieval; Information use; Empirical research; Empirical studies; Health informations; Information seeking; Information seeking behaviors; Interactive system; Misleading informations; Older adults; Search; Voice assistant; Health,Article,Final,Scopus,2-s2.0-85134150441,Peter,,
"Alonzo O., Elliot L., Dingman B., Lee S., Al Amin A., Huenerfauth M.",57215303209;7003340239;57218761507;57189050289;57311937000;12240800100;,"Reading-Assistance Tools among Deaf and Hard-of-Hearing Computing Professionals in the U.S.: Their Reading Experiences, Interests and Perceptions of Social Accessibility",2022,ACM Transactions on Accessible Computing,15,2,16,,,,,10.1145/3520198,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132076819&doi=10.1145%2f3520198&partnerID=40&md5=9d2081507dfdfec7231036f48e66eab3,"Automatic Text Simplification (ATS) software aims at automatically rewrite complex text to make it simpler to read. Prior research has explored the use of ATS as a reading assistance technology, identifying benefits from providing these technologies to different groups of users, including Deaf and Hard-of-hearing (DHH) adults. However, little work has investigated the interests and requirements of specific groups of potential users of this technology. Considering prior work establishing that computing professionals often need to read about new technologies in order to stay current in their profession, in this study, we investigated the reading experiences and interests of DHH individuals with work experience in the computing industry in ATS-based reading assistance tools, as well as their perspective on the social accessibility of those tools. Through a survey and two sets of interviews, we found that these users read relatively often, especially in support of their work, and were interested in tools to assist them with complicated texts; but misperceptions arising from public use of these tools may conflict with participants' desired image in a professional context. This empirical contribution motivates further research into ATS-based reading assistance tools for these users, prioritizing which reading activities users are most interested in seeing the application of this technology, and highlighting design considerations for creating ATS tools for DHH adults, including considerations for social accessibility. © 2022 Association for Computing Machinery.",Automatic text simplification; people who are deaf or hard of hearing; reading assistance; social accessibility,Audition; 'current; Automatic text simplification; Computing industry; Hard of hearings; People who be deaf or hard of hearing; Potential users; Reading assistance; Simple++; Social accessibility; Work experience; Surveys,Article,Final,Scopus,2-s2.0-85132076819,Peter,,
"Yang F.-C., Mousas C., Adamo N.",57216934071;55481721000;6505769924;,Holographic sign language avatar interpreter: A user interaction study in a mixed reality classroom,2022,Computer Animation and Virtual Worlds,33,3-4,e2082,,,,1.0,10.1002/cav.2082,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131568686&doi=10.1002%2fcav.2082&partnerID=40&md5=e3f0b48d398c3ffbce9f4f2629f05b9a,"We explored user interactions with a holographic sign language interpreter in a mixed reality (MR) classroom for deaf and hard of hearing students. The developed MR application projects a holographic signing avatar that translates in real time the lecture while a speaking instructor is teaching. Our study explored user interaction with the MR system, intending to provide design guidelines for digital MR sign language interpreters. We recruited eight participants and conducted a usability test focused on avatar framing (full-body vs. half-body) and avatar manipulation (fixed position, scale, and orientation vs. user-adjustable position, scale, and orientation) in the MR classroom. We used a mixed-method approach to analyze quantitative and qualitative data through recordings, surveys, and interviews. The results show user preferences toward viewing holographic signing avatars in the MR environment and user acceptability toward such applications. © 2022 John Wiley & Sons, Ltd.",holographic avatar; mixed reality; sign language animation; user interaction,Audition; Holography; Students; Surveys; User interfaces; Hard of hearings; Holographic avatar; Interaction studies; Language interpreters; Mixed reality; Position-orientation; Sign language; Sign language animation; Signing avatars; User interaction; Mixed reality,Conference Paper,Final,Scopus,2-s2.0-85131568686,Peter,,
"Kopacz A., Banerski G., Biele C.",55865022600;57210802059;13007244700;,Cognitive and visual processing of 3D enhanced disaster risk communication video,2022,International Journal of Disaster Risk Reduction,75,,102971,,,,1.0,10.1016/j.ijdrr.2022.102971,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129267070&doi=10.1016%2fj.ijdrr.2022.102971&partnerID=40&md5=53cbcd6cf7053d2e6ea9baef683e6fe5,"Using advanced graphic effects in video messages can be an effective tool to enhance a message through more realistic presentation. However, according to Cognitive Load Theory, it may come at the cost of greater effort and extraneous cognitive load generated by the manner in which information is presented. In this paper we determine the cognitive costs of processing instructional material (disaster risk communication) using eye tracking indicators. Our results reveal that enriching instructional material with 3D visual effects increases the number of fixations, as well as shortens fixation duration. These oculomotor responses reflect an increased cognitive load, which is correlated with poorer knowledge regarding the presented material. We discuss the obtained results from the practical perspective, providing insights into effective means of disaster risk communication. © 2022",3D enhanced video; Cognitive load; Disaster risk communication; Eye tracking; Flood warning messages; Instructional design,,Article,Final,Scopus,2-s2.0-85129267070,Peter,,
"Orndorf H.C., Waterman M., Lange D., Kavin D., Johnston S.C., Jenkins K.P.",57217391030;57571679100;57612506500;56166932800;56820780500;56761937300;,Opening the Pathway: An Example of Universal Design for Learning as a Guide to Inclusive Teaching Practices,2022,CBE Life Sciences Education,21,2,ar28,,,,,10.1187/CBE.21-09-0239,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127957189&doi=10.1187%2fCBE.21-09-0239&partnerID=40&md5=2f97a0cf4b78b8cc17800d28351c9cee,"Universal Design for Learning (UDL) provides a flexible framework for supporting a wide variety of learners. We report here on a conference that presented the UDL framework as a way to increase success of deaf and hard-of-hearing (deaf/hh) students in introductory biology courses. The Opening the Pathway conference was an NSF Advanced Technolog-ical Education project focusing on raising awareness about careers in biotechnology and student success in introductory biology, a key gateway course for careers in biotechnolo-gy. The participants were professionals who work with deaf/hh students at pivotal points in students’ educational pathways for raising awareness of biotechnology career options, in-cluding community college faculty, high school faculty at schools for the deaf, and American Sign Language (ASL) interpreters. The conference goal was to provide an effective, meaningful professional development experience in biology instruction. The conference explicitly addressed the role of a UDL approach in building accessible, inclusive, produc-tive learning environments, particularly for deaf/hh students, and demonstrated how to make effective pedagogical practices, specifically case-based learning, inclusive and UDL-aligned in an introductory biology context. We describe the conference, conference outcomes for participants, and in particular the application of the UDL framework to create an inclusive experience. © 2022 H. C. Orndorf et al.",,human; learning; school; student; university; Humans; Learning; Schools; Students; Universal Design; Universities,Article,Final,Scopus,2-s2.0-85127957189,Peter,,
Hughes C.J.,57040431000;,Universal access: user needs for immersive captioning,2022,Universal Access in the Information Society,21,2,,393,403,,,10.1007/s10209-021-00828-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111889560&doi=10.1007%2fs10209-021-00828-w&partnerID=40&md5=e4156af61f065737493c0d3898d25324,"This article focuses on building a prototyping for immersive captioning following a user-centric approach. This methodology is characterised by following a bottom-up approach, where usability and user needs are at the heart of the development. Recent research on user requirements for captioning in immersive environments has shown that there is both a need for improvement and a wealth of research opportunities. The final aim is to identify how to display captions for an optimal viewing experience. This work began four years ago with some partial findings. We build from the lessons learnt, focussing on the user-centric design requirements cornerstone: prototyping. Our prototype framework integrates methods used in existing solutions aiming at instant contrast-and-compare functionalities. The first part of the article presents the state of the art for user requirements identifying the reasons behind the development of the prototyping framework. The second part of the article describes the two-stage framework development. The initial framework concept answered to the challenges resulting from the previous research. As soon as the first framework was developed, it became obvious that a second improved solution was required, almost as a showcase on how ideas can quickly be implemented for user testing, and for users to elicit requirements and creative solutions. The article finishes with a list of functionalities, resulting in new caption modes, and the opportunity of becoming a comprehensive immersive captions testbed, where tools such as eye-tracking, or physiological testing devices could be testing captions across any device with a web browser. © 2021, The Author(s).",Accessibility; Captions; Immersive video; Testing; User-centric requirements; VR,Information science; Information systems; Software engineering; Bottom up approach; Creative solutions; Framework development; Immersive environment; Recent researches; Research opportunities; User requirements; User-centric designs; Eye tracking,Article,Final,Scopus,2-s2.0-85111889560,Peter,,
"Rui Xia Ang J., Liu P., McDonnell E., Coppola S.",57704347400;57224992974;57222060653;57704347500;,"""In this online environment, we're limited': Exploring Inclusive Video Conferencing Design for Signers",2022,Conference on Human Factors in Computing Systems - Proceedings,,,609,,,,1.0,10.1145/3491102.3517488,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130580958&doi=10.1145%2f3491102.3517488&partnerID=40&md5=d7d76a6cb80a341b3d358b0ce9083725,"As video conferencing (VC) has become increasingly necessary for many aspects of daily life, many d/Deaf and hard of hearing people, particularly those who communicate via sign language (signers), face distinct accessibility barriers. To better understand the unique requirements for participating in VC using a visual-gestural language, such as ASL, and to identify practical design considerations for signer-inclusive videoconferencing, we conducted 12 interviews and four co-design sessions with a total of eight d/Deaf signers and eight ASL interpreters. We found that participants' access needs regarding consuming information (e.g., visual clarity of signs), communicating (e.g., getting attention of others), and collaborating (e.g., working with interpreter teams) are not well-met on existing VC platforms. We share novel insights into attending and conducting signer-accessible video conferences, outline considerations for future VC design, and provide guidelines for conducting remote research with d/Deaf signers. © 2022 Owner/Author.",accessibility and inclusive design; accessible groupware; accessible research methods; American Sign Language; d/Deaf and hard-of-hearing; sign language interpreting,Audition; Groupware; Visual communication; Visual languages; Accessibility and inclusive design; Accessible groupware; Accessible research method; American sign language; D/deaf and hard-of-hearing; Hard of hearings; Inclusive design; Research method; Sign language interpreting; Video-conferencing; Video conferencing,Conference Paper,Final,Scopus,2-s2.0-85130580958,Peter,,
"Arakawa R., Yakura H., Kobayashi S.",57209398650;57198885939;57207874442;,VocabEncounter: NMT-powered Vocabulary Learning by Presenting Computer-Generated Usages of Foreign Words into Users' Daily Lives,2022,Conference on Human Factors in Computing Systems - Proceedings,,,6,,,,1.0,10.1145/3491102.3501839,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130572960&doi=10.1145%2f3491102.3501839&partnerID=40&md5=5f3723bd8b5f79cf4ad5bc488955ee96,"We demonstrate that recent natural language processing (NLP) techniques introduce a new paradigm of vocabulary learning that benefits from both micro and usage-based learning by generating and presenting the usages of foreign words based on the learner's context. Then, without allocating dedicated time for studying, the user can become familiarized with how the words are used by seeing the example usages during daily activities, such as Web browsing. To achieve this, we introduce VocabEncounter, a vocabulary-learning system that suitably encapsulates the given words into materials the user is reading in near real time by leveraging recent NLP techniques. After confirming the system's human-comparable quality of generating translated phrases by involving crowdworkers, we conducted a series of user studies, which demonstrated its effectiveness on learning vocabulary and its favorable experiences. Our work shows how NLP-based generation techniques can transform our daily activities into a field for vocabulary learning. © 2022 Owner/Author.",natural language processing; neural mechanical translation; vocabulary learning,Learning systems; Translation (languages); Computer generated; Daily activity; Daily lives; Generation techniques; Language processing techniques; Mechanical; Near-real time; Neural mechanical translation; User study; Vocabulary learning; Natural language processing systems,Conference Paper,Final,Scopus,2-s2.0-85130572960,Peter,,
"Alonzo O., Trussell J., Watkins M., Lee S., Huenerfauth M.",57215303209;7006607897;57703199700;57189050289;12240800100;,Methods for Evaluating the Fluency of Automatically Simplified Texts with Deaf and Hard-of-Hearing Adults at Various Literacy Levels,2022,Conference on Human Factors in Computing Systems - Proceedings,,,267,,,,,10.1145/3491102.3517566,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130566084&doi=10.1145%2f3491102.3517566&partnerID=40&md5=af377ffce7b80ba954d994965ef44c08,"Research has revealed benefits and interest among Deaf and Hard-of-Hearing (DHH) adults in reading-assistance tools powered by Automatic Text Simplification (ATS), a technology whose development benefits from evaluations by specific user groups. While prior work has provided guidance for evaluating text complexity among DHH adults, researchers lack guidance for evaluating the fluency of automatically simplified texts, which may contain errors from the simplification process. Thus, we conduct methodological research on the effectiveness of metrics (including reading speed; comprehension questions; and subjective judgements of understandability, readability, grammaticality, and system performance) for evaluating texts controlled to be at different levels of fluency, when measured among DHH participants at different literacy levels. Reading speed and grammaticality judgements effectively distinguished fluency levels among participants across literacy levels. Readability and understandability judgements, however, only worked among participants with higher literacy. Our findings provide methodological guidance for designing ATS evaluations with DHH participants. © 2022 ACM.",accessibility; automatic text simplification; deaf and hard-of-hearing; methodological research,Automatic text simplification; Deaf and hard-of-hearing; Hard of hearings; Methodological research; Reading speed; Subjective judgement; Systems performance; Understandability; User groups; Audition,Conference Paper,Final,Scopus,2-s2.0-85130566084,Peter,,
"Yamada W., Korogi S., Ochiai K.",36070145700;57219805299;7202772590;,Janus Screen: Screen with Switchable Projection Surfaces Using Wire Grid Polarizer,2022,Conference on Human Factors in Computing Systems - Proceedings,,,259,,,,,10.1145/3491102.3517709,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130554666&doi=10.1145%2f3491102.3517709&partnerID=40&md5=ba295bf0d2266641851c3d3e416523bc,"In this paper, we present a novel screen system employing polarizers that allow switching of the projection surface to the front, rear, or both sides using only two projectors on one side. In this system, we propose a method that employs two projectors equipped with polarizers and a multi-layered screen comprising an anti-reflective plate, transparent screen, and wire grid polarizer. The multi-layered screen changes whether the projected image is shown on the front or rear side of the screen depending on the polarization direction of the incident light. Hence, the proposed method can project images on the front, rear, or both sides of the screen by projecting images from either or both projectors using polarizers. In addition, the proposed method can be easily deployed by simply attaching multiple optical films. We implement a prototype and confirm that the proposed method can selectively switch the projection surface. © 2022 ACM.",projection screen; spatial augmented reality; ubiquitous computing,Augmented reality; Incident light; Optical instruments; Projection screens; Antireflective; Multi-layered; Polarisers; Polarization direction; Rear side; Screen grids; Screen systems; Spatial augmented realities; Switchable; Wire grid polarizers; Ubiquitous computing,Conference Paper,Final,Scopus,2-s2.0-85130554666,Peter,,
"Amin A.A., Hassan S., Lee S., Huenerfauth M.",56623925700;57224309524;57189050289;12240800100;,"Watch It, Don't Imagine It: Creating a Better Caption-Occlusion Metric by Collecting More Ecologically Valid Judgments from DHH Viewers",2022,Conference on Human Factors in Computing Systems - Proceedings,,,459,,,,1.0,10.1145/3491102.3517681,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130553652&doi=10.1145%2f3491102.3517681&partnerID=40&md5=3a598155d0bde0a233d1e3d7dfdba31d,"Television captions blocking visual information causes dissatisfaction among Deaf and Hard of Hearing (DHH) viewers, yet existing caption evaluation metrics do not consider occlusion. To create such a metric, DHH participants in a recent study imagined how bad it would be if captions blocked various on-screen text or visual content. To gather more ecologically valid data for creating an improved metric, we asked 24 DHH participants to give subjective judgments of caption quality after actually watching videos, and a regression analysis revealed which on-screen contents' occlusion related to users' judgments. For several video genres, a metric based on our new dataset out-performed the prior state-of-the-art metric for predicting the severity of captions occluding content during videos, which had been based on that prior study. We contribute empirical findings for improving DHH viewers' experience, guiding the placement of captions to minimize occlusions, and automated evaluation of captioning quality in television broadcasts. © 2022 ACM.",Accessibility; Caption; Metric; Regression,Audition; Quality control; Blockings; Caption; Evaluation metrics; Hard of hearings; Metric; Subjective judgement; Text content; User judgements; Visual content; Visual information; Regression analysis,Conference Paper,Final,Scopus,2-s2.0-85130553652,Peter,,
"Choi D., Lee U., Hong H.",56477900500;12646091900;34881804800;,"""It's not wrong, but I'm quite disappointed"": Toward an Inclusive Algorithmic Experience for Content Creators with Disabilities",2022,Conference on Human Factors in Computing Systems - Proceedings,,,593,,,,4.0,10.1145/3491102.3517574,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130530585&doi=10.1145%2f3491102.3517574&partnerID=40&md5=82be88cf718296c1ba4c442bffa75a05,"YouTube is a space where people with disabilities can reach a wider online audience to present what it is like to have disabilities. Thus, it is imperative to understand how content creators with disabilities strategically interact with algorithms to draw viewers around the world. However, considering that the algorithm carries the risk of making less inclusive decisions for users with disabilities, whether the current algorithmic experiences (AXs) on video platforms is inclusive for creators with disabilities is an open question. To address that, we conducted semi-structured interviews with eight YouTubers with disabilities. We found that they aimed to inform the public of diverse representations of disabilities, which led them to work with algorithms by strategically portraying disability identities. However, they were disappointed that the way the algorithms work did not sufficiently support their goals. Based on findings, we suggest implications for designing inclusive AXs that could embrace creators' subtle needs. © 2022 ACM.",algorithmic experience; content creators; inclusive design; people with disabilities; YouTube,current; Algorithmic experience; Algorithmics; Content creators; Inclusive design; Online audience; People with disabilities; Users with disabilities; Video-platforms; YouTube; Computer programming,Conference Paper,Final,Scopus,2-s2.0-85130530585,Peter,,
"Seita M., Lee S., Andrew S., Shinohara K., Huenerfauth M.",57192541826;57189050289;57220117831;16239695600;12240800100;,Remotely Co-Designing Features for Communication Applications using Automatic Captioning with Deaf and Hearing Pairs,2022,Conference on Human Factors in Computing Systems - Proceedings,,,460,,,,4.0,10.1145/3491102.3501843,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130526085&doi=10.1145%2f3491102.3501843&partnerID=40&md5=ad5bfc32c802d0a5b9bcc8307a75e775,"Deaf and Hard-of-Hearing (DHH) users face accessibility challenges during in-person and remote meetings. While emerging use of applications incorporating automatic speech recognition (ASR) is promising, more user-interface and user-experience research is needed. While co-design methods could elucidate designs for such applications, COVID-19 has interrupted in-person research. This study describes a novel methodology for conducting online co-design workshops with 18 DHH and hearing participant pairs to investigate ASR-supported mobile and videoconferencing technologies along two design dimensions: Correcting errors in ASR output and implementing notification systems for influencing speaker behaviors. Our methodological findings include an analysis of communication modalities and strategies participants used, use of an online collaborative whiteboarding tool, and how participants reconciled differences in ideas. Finally, we present guidelines for researchers interested in online DHH co-design methodologies, enabling greater geographically diversity among study participants even beyond the current pandemic. © 2022 ACM.",Accessibility; Automatic Speech Recognition; Deaf and Hard-of-Hearing; Participatory Design; Videoconferencing,Audition; Design; Online systems; Speech recognition; User interfaces; Automatic speech recognition; Co-designing; Co-designs; Codesign method; Communication application; Deaf and hard-of-hearing; Hard of hearings; Novel methodology; Participatory design; User experience research; Video conferencing,Conference Paper,Final,Scopus,2-s2.0-85130526085,Peter,,
"Colley M., Kränzle T., Rukzio E.",57202047826;57673606200;18233783900;,Accessibility-Related Publication Distribution in HCI Based on a Meta-Analysis,2022,Conference on Human Factors in Computing Systems - Proceedings,,,299,,,,,10.1145/3491101.3519701,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129770757&doi=10.1145%2f3491101.3519701&partnerID=40&md5=f8e976ace7ccc9d6f27da0bbc2e8d8c8,"Accessibility research aims to aid humans that experience minor or major disabilities and conditions. However, researchers might have limited exposure to certain disabilities, therefore, focus on those prevalent in their own lives. This work presents a script-based meta-analysis on addressed populations in accessibility research published on top Human-Computer Interaction (HCI) venues (3617 full papers). We categorize the publications regarding the involved people and their disabilities. We found that work on vision disability makes up for almost one third (28.85%) of the work published in general HCI. In light of these findings, we present possible conference-and funding-related explanatory approaches and argue that disability research could more reflect the prevalence of disabilities in the world. © 2022 Owner/Author.",Accessibility; overview; survey.,Publishing; Condition; Meta-analysis; Overview; Survey.; Human computer interaction,Conference Paper,Final,Scopus,2-s2.0-85129770757,Peter,,
"Luo L., Weng D., Songrui G., Hao J., Tu Z.",57212510914;24386043400;57673899900;57273470700;57212505490;,Avatar Interpreter: Improving Classroom Experiences for Deaf and Hard-of-Hearing People Based on Augmented Reality,2022,Conference on Human Factors in Computing Systems - Proceedings,,,318,,,,2.0,10.1145/3491101.3519799,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129730236&doi=10.1145%2f3491101.3519799&partnerID=40&md5=6debfc135d8c87f56824505c63801a0d,"Deaf and hard-of-hearing (DHH) people experience difficulties in accessing education. One reason is that they miss out on oral information in the classroom. Although some classrooms are equipped with sign language interpreters specifically for deaf students, DHH students have trouble switching their gaze and attention when they cannot see the interpreter and the visual information in the classroom simultaneously. To address this challenge, this paper develops Avatar Interpreter, a visual interface for synchronized sign language interpreters on an augmented reality head-mounted display. Avatar Interpreter can be displayed at different sizes and in different locations in space to fit different scenarios, helping deaf students to better receive sign language information and enhancing the classroom experience for deaf students. In the work presented in this paper, we discuss our prototype, make design recommendations, and discuss configuration and implementation. Finally, we propose questions and research methods for an upcoming user study. © 2022 ACM.",Accessibility; AR; Avatar; DHH; Sign Language,Audition; Helmet mounted displays; Students; Visual languages; AR; Avatar; Deaf and hard-of-hearing; Deaf students; Hard of hearings; Head-mounted-displays; Language interpreters; Sign language; Visual information; Visual Interface; Augmented reality,Conference Paper,Final,Scopus,2-s2.0-85129730236,Peter,,
"Li F.M., Lu C., Lu Z., Carrington P., Truong K.N.",57220116833;57219113508;57194274057;55926389800;7005764228;,An Exploration of Captioning Practices and Challenges of Individual Content Creators on YouTube for People with Hearing Impairments,2022,Proceedings of the ACM on Human-Computer Interaction,6,CSCW1,75,,,,3.0,10.1145/3512922,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127056360&doi=10.1145%2f3512922&partnerID=40&md5=93a2211b1f3ee956aa6cb8eeedcfbbb2,"Deaf and Hard-of-Hearing (DHH) audiences have long complained about caption qualities for many online videos created by individual content creators on video-sharing platforms (e.g., YouTube). However, there lack explorations of practices, challenges, and perceptions of online video captions from the perspectives of both individual content creators and DHH audiences. In this work, we first explore DHH audiences' feedback on and reactions to YouTube video captions through interviews with 13 DHH individuals, and uncover DHH audiences' experiences, challenges, and perceptions on watching videos created by individual content creators (e.g., manually added caption tags could create additional confidence and trust in caption qualities for DHH audiences). We then discover individual content creators' practices, challenges, and perceptions on captioning their videos (e.g., back-captioning problems) by conducting a YouTube video analysis with 189 captioning-related YouTube videos, followed by a survey with 62 individual content creators. Overall, our findings provide an in-depth understanding of captions generated by individual content creators and bridge the knowledge gap mutually between content creators and DHH audiences on captions. © 2022 Owner/Author.",accessibility; caption; content creators; deaf and hard-of-hearing; YouTube,Caption; Content creators; Deaf and hard-of-hearing; Hard of hearings; Hearing impairments; Online video; Sharing platforms; Video captions; Video sharing; YouTube; Audition,Article,Final,Scopus,2-s2.0-85127056360,Peter,,
"Pattemore A., Muñoz C.",57218417947;57200704773;,Captions and learnability factors in learning grammar from audio-visual input,2022,JALT CALL Journal,18,1,,83,109,,,10.29140/JALTCALL.V18N1.564,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132654753&doi=10.29140%2fJALTCALL.V18N1.564&partnerID=40&md5=573d9b1d87f5347fa0330afa1d5fadb2,"This study explores the effects of extensive audio-visual input with three captioning modes – unenhanced captions, textually enhanced captions, and no captions – on learning a variety of L2 grammatical constructions and examines the effects of three learnability factors: construction type, frequency, and recency. A total of 112 participants watched ten full-length TV series episodes over a period of five weeks. The study targeted 27 frequently occurring grammatical constructions categorized as fully-schematic, partially-filled, and fully-filled. The design included a pretest, an immediate posttest to measure the effects of recency, and a delayed posttest. The results indicated mixed effects of captioning: textually enhanced captions – a more salient condition – led to immediate learning outcomes while unenhanced captions resulted in higher long-term effects. A limit to the amount of different textually enhanced constructions presented in the input for effective learning is suggested. In general, unenhanced captions appear sufficient for successful grammar construction learning. © 2022. Anastasia Pattemore & Carmen Muñoz",Audio-visual input; captions; grammatical constructions; textual enhancement,,Article,Final,Scopus,2-s2.0-85132654753,Peter,,
Matthews N.,23767669600;,Commodifying Diversity in the Marketing of a Digital Hearing Start-Up,2022,Television and New Media,23,3,,312,328,,1.0,10.1177/15274764211009601,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104846260&doi=10.1177%2f15274764211009601&partnerID=40&md5=cc42511073b6cdf9a26ab164d9f3f012,"This article will unpack the consequences of the convergence between hearing aids and mobile phones. It deploys an analysis of the marketing of two hearing and communication apps from one start-up: a smartphone-based hearing test and a music app which tweaks users’ audio to accommodate their hearing profile. This article argues that both apps and the marketing for them can be seen as examples of biomediation. Rather than marketing hearing apps through biomedical categories to people who recognize themselves as disabled or “hearing impaired,” these apps are framed as personalizing music to fit a unique “earprint.” The article draws on two concepts used to understand this “disruption of the non-disabled/disabled binary” (Puar 2017, xvii, xviii): debility and diversity. Finally, the article offers a critical perspective on the political meanings of the term “diversity” and argues that the categorization of apps has important political consequences. © The Author(s) 2021.",apps; digital health; disability; hearing; marketing; mobile media,,Article,Final,Scopus,2-s2.0-85104846260,Peter,,
"Garcia F.E., de Almeida Neris V.P.",55802743300;23396897500;,A framework for tailorable games: toward inclusive end-user development of inclusive games,2022,Universal Access in the Information Society,21,1,,193,237,,3.0,10.1007/s10209-020-00779-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096399867&doi=10.1007%2fs10209-020-00779-8&partnerID=40&md5=f42d7eddfcabd87b01abb122687339e0,"One strategy toward universalizing play is enabling more people to develop their own games. In this paper, our efforts toward a framework for inclusive creation of inclusive games are discussed. The hypothesis is that if end-users used creation tools suitable to their interaction needs and followed a collaborative work model to iteratively improve accessibility features to be inserted into a software architecture able to modify human-computer interaction at use-time, then they would be able to create games satisfying heterogeneous interaction needs of possible players. To verify the hypothesis, the architecture, the collaborative work model, and a game creation platform (Lepi) were designed to support game creation and play activities. Abilities were focused to provide opportunities for contributions based on skills, interests, and knowledge of people. The framework was evaluated over ten meetings spanning four months by people with alcohol and drug addiction from a public healthcare service. With the framework, participants were able to create their own games despite their different interaction needs (including low literacy, no previous contact with computers, emotional disabilities). By following the collaborative work model, they enabled people with different interaction needs than their own to play their games. Hence, with the framework, opportunities were provided to enable people with different interaction needs to contribute, create, and play. Game creation became a jigsaw puzzle, on which each piece (contribution) allowed people to create and play according to their abilities and skills. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",End-user development; Game accessibility; Game programming; Human-centered computing; Meta-design; Universal design,Human computer interaction; Alcohol and drugs; Collaborative Work; Emotional disability; End user development; Heterogeneous interactions; Jigsaw puzzles; Low literacies; Public healthcares; Computer games,Article,Final,Scopus,2-s2.0-85096399867,Peter,,
Adigun O.T.,57217036399;,The Experiences of Emergency-Remote Teaching Via Zoom: The Case of Natural-Science Teachers Handling of Deaf/Hard-of-Hearing Learners in South Africa,2022,"International Journal of Learning, Teaching and Educational Research",21,2,,176,194,,1.0,10.26803/ijlter.21.2.10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126817535&doi=10.26803%2fijlter.21.2.10&partnerID=40&md5=6906f6a0931247415f1cecf2e9b0d098,"Information regarding the e-teaching of science subjects to learners who are deaf or hard-of-hearing (LDHH) is somewhat scarce in the existing literature. The COVID-19 pandemic has, however, compelled all to adopt Information-Communication Technologies (ICTs) for teaching during this period. Lamentably, previous studies have advanced some of the challenges associated with teaching science to LDHH during online-science classes. While the pandemic has compelled remote teaching, there is a paucity of research evidence on the experiences of natural-science teachers handling of LDHH learners in emergency-remote teaching via Zoom. Therefore, anchored on the Use-and-Gratification Theory, this study has explored the emergency-remote teaching of natural sciences to deaf learners via Zoom during the pandemic. This study employed a qualitative research design, with seven natural science teachers as participants. The data were gathered via a semi-structured Zoom interview; and they were analysed by using thematic-content analysis. The findings revealed that while the participants appreciated the uninterrupted-learning model presented by ICTs, they had relatively awful experiences when teaching naturalscience subjects to LDHH via Zoom, due to inadequate organisational and individual preparedness, as well as to limited two-way teacher-learners’ communication and interactions. Based on the findings, the appropriate recommendations were made for both policy and practice. ©Author",Coronavirus disease; Learners who are deaf/hard-of-hearing; Natural sciences; Teachers; Zoom,,Article,Final,Scopus,2-s2.0-85126817535,Peter,,
"Lv J., Zhuang B., Chen X., Xue L., Li D.",57404871700;57404871800;57405522400;57405188500;35215276400;,Compositionality of the Constituent Characters in Chinese Two-Character-Word Recognition by Adult Readers of High and Low Chinese Proficiency,2022,Journal of Psycholinguistic Research,51,1,,195,216,,,10.1007/s10936-021-09833-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122535165&doi=10.1007%2fs10936-021-09833-9&partnerID=40&md5=c97550d3a532c8e81e1d318e2f237759,"In Chinese, the graphic units are Chinese characters, most of which are compound characters. Since a compound character can be different from another one in being regarded as composed of components (compositionality), readers might have developed a compositionality awareness of the constituent characters in two-character word (2C-word) recognition. Two experiments were conducted in a lexical decision task on the same set of 2C-words, the first constituent characters of which were manipulated in compositionality. Given that a Chinese character is more difficult to recognize when it is presented upside-down than when it is presented in an upright orientation and that it is inevitable to perceive the constituent characters in 2C-word recognition, we manipulated the first constituent characters’ presentation orientation to increase the task difficulty. The two constituent characters of a 2C-word target were displayed simultaneously in a trial in Experiment 1 but were shown sequentially in Experiment 2. Participants were two cohorts of adult Chinese native speakers (CNS1s and CNS2s). CNS1s had a significantly lower level of reading proficiency than CNS2s. The influence of orientation was observed in both CNS1s and CNS2s’ performance across the two experiments, but only CNS2s’ reaction times seemed to have indicated the effect of compositionality in Experiment 2. Skilled readers are more likely than less skilled readers to be conscious of compositionality of the first constituent characters, which are presented separately from the second ones, in 2C-word recognition. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Chinese proficiency; Compositionality; Constituent characters; Lexical decision; Two-character word recognition,"adult; China; human; pattern recognition; reaction time; reading; Adult; China; Humans; Pattern Recognition, Visual; Reaction Time; Reading; Recognition, Psychology",Article,Final,Scopus,2-s2.0-85122535165,Peter,,
"Ussenova M., Zhiyenbayeva N., Kosshygulova A., Iskakova L., Aigul S., Zhuzimkul B.",57221735349;57223025809;57298507900;57225892497;57454214800;57454214900;,Developing the social activities of primary school students with hearing impairment using technologies,2022,Cypriot Journal of Educational Sciences,17,1,,174,192,,,10.18844/cjes.v17i1.6700,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124655097&doi=10.18844%2fcjes.v17i1.6700&partnerID=40&md5=ea0476c8c9f8087a597744386bef87f9,"It is known that the use of technology to support individualization and motivation in education programs for hearing-impaired individuals has positive results. This research aims to reveal the views of teachers, students, and parents to develop the social activities of hearing-impaired students by supporting them with technology. This research was conducted with the phenomenology pattern, one of the qualitative research designs. Data were collected through semi-structured interview forms prepared separately for teachers, students, and parents. The participants of the study consisted of 20 primary school teachers, 20 hearing-impaired primary school students, and 20 parents of hearing-impaired students, who were working in various primary schools in Kazakhstan in the 2022-2023 academic year. As a result of the research, the majority of the participants believed the communication problems mostly faced by students with hearing impairment outweighed the psychological and school problems. The majority of the participants stated that the students were willing to participate in social activities if they were supported. © 2022 SciencePark Science, Organization and Counseling. All rights reserved.",Hearing impaired students; Social activity; Technology,,Article,Final,Scopus,2-s2.0-85124655097,Peter,,
"Fujiwara R.J.T., Ishiyama G., Ishiyama A.",57193456159;6602573696;7006190164;,Association of Socioeconomic Characteristics with Receipt of Pediatric Cochlear Implantations in California,2022,JAMA Network Open,5,1,43132,,,,2.0,10.1001/jamanetworkopen.2021.43132,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123440133&doi=10.1001%2fjamanetworkopen.2021.43132&partnerID=40&md5=30ff8f9cd8870c840f040ce8ccd6cccb,"Importance: Earlier cochlear implantation among children with bilateral severe to profound sensorineural hearing loss is associated with improved language outcomes. More work is necessary to identify patients at risk for delayed cochlear implantation and understand targets for interventions to improve cochlear implantation rates among children. Objective: To describe the demographics among children receiving cochlear implantations and variability in implantation rates in California and to investigate sociodemographic and parental factors associated with early pediatric cochlear implantation. Design, Setting, and Participants: This retrospective cross-sectional study was conducted using data from the Healthcare Cost and Utilization Project California State Ambulatory Surgery Database in calendar year 2018. Included patients were children aged 9 years old or younger undergoing cochlear implantation. Sociodemographic factors, location of treatment, and parental factors were collected. Data were analyzed from March through August 2021. Main Outcomes and Measures: Binary logistic regression was performed to investigate sociodemographic factors associated with early cochlear implantation (ie, before age 2 years). Geographic variability in pediatric cochlear implantation across hospital referral regions in California was described, and various parental factors associated with implantation before age 2 years were analyzed. Results: Among 182 children receiving cochlear implantations, the median (IQR) age was 3 (1-5) years and 58 children (31.9%) received implantations at ages 2 years or younger. There were 90 girls (49.5%) and 92 boys (50.5%), and among 170 children with race and ethnicity data, there were 27 Asian or Pacific Islander children (15.9%), 63 Hispanic children (37.1%), and 55 White children (32.4%). The risk of CI was significantly decreased among Black children compared with Asian or Pacific Islander children (relative risk [RR], 0.18 [95% CI, 0.07-0.47]; P =.001) and White children (RR, 0.24 [95% CI, 0.10-0.59]; P =.002) and among Hispanic children compared with Asian or Pacific Islander children (RR, 0.32 [95% CI, 0.21-0.50]; P <.001) and White children (RR, 0.42 [95% CI, 0.29-0.59; P <.001). Compared with private insurance, Medicaid insurance was associated with decreased odds of implantation at ages 2 years or younger (odds ratio [OR], 0.19 [95% CI, 0.06-0.64]; P =.007), and every 1 percentage point increase in maternal high school completion percentage in a given California hospital referral region was correlated with a 5-percentage point increase in percentage of cochlear implants performed at age 2 years or younger (b = 5.18 [95% CI, 1.34-9.02]; P =.008). There were no significant differences in rates of early implantation by race or ethnicity. Conclusions and Relevance: This study found significant variability in pediatric cochlear implantation rates in California. These findings suggest that socioeconomic and parental factors may be associated with differences in access to early cochlear implantation and suggest the need to invest in initiatives to address barriers to appropriate and timely access to care. © 2022 American Medical Association. All rights reserved.",,"age; Article; Asian; Black person; California; Caucasian; child; child parent relation; cochlear implantation; correlation analysis; cross-sectional study; demography; education; ethnicity; female; geography; health care access; health care cost; health care disparity; high school; Hispanic; hospital; human; incidence; major clinical study; male; medicaid; odds ratio; Pacific Islander; parent; patient referral; pediatric patient; perception deafness; preschool child; private health insurance; race; retrospective study; risk factor; school child; sociodemographics; statistical analysis; adult; California; cochlear implantation; economics; ethnology; health insurance; infant; patient attitude; perception deafness; socioeconomics; statistical model; United States; Adult; California; Child; Child, Preschool; Cochlear Implantation; Cross-Sectional Studies; Ethnicity; Female; Healthcare Disparities; Hearing Loss, Sensorineural; Humans; Infant; Insurance, Health; Logistic Models; Male; Medicaid; Odds Ratio; Parents; Patient Acceptance of Health Care; Retrospective Studies; Socioeconomic Factors; United States",Article,Final,Scopus,2-s2.0-85123440133,Peter,,
"Kruger J.-L., Wisniewska N., Liao S.",9277428700;57215862601;57203968644;,Why subtitle speed matters: Evidence from word skipping and rereading,2022,Applied Psycholinguistics,43,1,,211,236,,3.0,10.1017/S0142716421000503,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120630460&doi=10.1017%2fS0142716421000503&partnerID=40&md5=3dc2279acd4cb295a837cdd1087bd5b2,"High subtitle speed undoubtedly impacts the viewer experience. However, little is known about how fast subtitles might impact the reading of individual words. This article presents new findings on the effect of subtitle speed on viewers' reading behavior using word-based eye-tracking measures with specific attention to word skipping and rereading. In multimodal reading situations such as reading subtitles in video, rereading allows people to correct for oculomotor error or comprehension failure during linguistic processing or integrate words with elements of the image to build a situation model of the video. However, the opportunity to reread words, to read the majority of the words in the subtitle and to read subtitles to completion, is likely to be compromised when subtitles are too fast. Participants watched videos with subtitles at 12, 20, and 28 characters per second (cps) while their eye movements were recorded. It was found that comprehension declined as speed increased. Eye movement records also showed that faster subtitles resulted in more incomplete reading of subtitles. Furthermore, increased speed also caused fewer words to be reread following both horizontal eye movements (likely resulting in reduced lexical processing) and vertical eye movements (which would likely reduce higher-level comprehension and integration). Copyright © The Author(s), 2021. Published by Cambridge University Press.",Keywords: eye movements; linguistic processing; rereading; subtitle reading; subtitle speed; word skipping,,Article,Final,Scopus,2-s2.0-85120630460,Peter,,
"Im M.S., Park H.",58097374100;57071908700;,A Qualitative Study on the Communication Experiences and Effective Communication Strategies of Adults with Cochlear Implants [인공와우 이식 성인의 의사소통 경험 및 효과적인 의사소통 전략에 관한 질적 연구],2022,Communication Sciences and Disorders,27,4,,925,943,,,10.12963/CSD.22938,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147764909&doi=10.12963%2fCSD.22938&partnerID=40&md5=acbd50cbea9a5dbd8409bc8abd1dfc63,"Objectives: This study aimed to demonstrate the nature and significance of the communication experiences encountered by adults with cochlear implants in the course of their social lives, and to explore effective communication strategies for the same. Methods: Data was collected via one-on-one in-depth interviews with five adults with cochlear implants, then analyzed through the Colaizzi phenomenological methodology. Results: First, the communication experiences of participants were categorized into the two themes of negative and positive experiences, with eight resultant sub-themes. The negative experiences consisted of “emotional distance,” “a lonely island in the crowd,” “difficulties that are not resolved,” and “facing communication barriers,” while positive experiences comprised “climbing the hill of maturity,” “gratitude toward good and beautiful encounters,” “the joy of managing to do my share,” and “the dreams one sets out to realize in their youth.” Next, effective communication strategies were categorized into the two themes of strategies for continuing communication and strategies for overcoming breakdowns in communication, with nine resultant sub-themes. Strategies for continuing communication included “structuralizing situations” and “enthusiastic attitudes,” while strategies for overcoming breakdowns in communication included “recepting through the use of tools” and “expressing oneself to the end.” Conclusion: The participants in this study face restricted situations due to their hearing impairments, but grow through dynamic interaction with society, and continuously employed effective communication strategies to respond efficiently to social situations. Principal factors to be considered and future service strategies for aiding unhindered communication for the hearing-impaired were discussed based on the results of the study © 2022 Korean Academy of Speech-Language Pathology and Audiology",Adult; Cochlear implant; Communication experience; Communication strategies; Hearing impairment,,Article,Final,Scopus,2-s2.0-85147764909,Peter,,
"Mulyani, Yusuf Y.Q., Trisnawati I.K., Syarfuni, Qamariah H., Wahyuni S.",58092816300;55351138500;57219709254;57218888655;57218559998;57218552261;,Watch and Learn: EFL Students' Perceptions of Video Clip Subtitles for Vocabulary Instruction,2022,Pertanika Journal of Social Sciences and Humanities,30,,,1,23,,,10.47836/PJSSH.30.S1.01,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147493079&doi=10.47836%2fPJSSH.30.S1.01&partnerID=40&md5=adc26dc318063769250400df3ccf0cb2,"Implementing the online learning process requires qualified EFL teachers as agents of change to get hold of effective learning resources to help students achieve learning goals. This pilot study explores EFL students' perceptions from two private Islamic senior high schools in urban areas in Aceh Province, Indonesia, towards using video clips with subtitles (bimodal of English-Indonesian) while learning English in the classroom. The study applied the mixed-methods design by disseminating a questionnaire to 78 participants and conducting a semi-structured interview with ten selected participants. Findings revealed that most students positively perceived using subtitled video clips in learning English vocabulary. However, some encountered setbacks in learning English words due to the poor use of the elements in the video clip subtitles. Therefore, the subtitling procedures still need revisiting to assist students better in digesting subtitled movies in a more well-organized manner. © Universiti Putra Malaysia Press.",EFL students; English vocabulary; prototype media; subtitles; video clips,,Article,Final,Scopus,2-s2.0-85147493079,Peter,,
"Lawi F.A., Al-Salman S., Haider A.S.",58088052800;57961473200;57201379366;,Modern Standard Arabic vs. Egyptian Vernacular in Dubbing Animated Movies: A Case Study of A Bug's Life,2022,International Journal of Communication and Linguistic Studies,21,1,,125,141,,,10.18848/2327-7882/CGP/v21i01/125-141,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147260630&doi=10.18848%2f2327-7882%2fCGP%2fv21i01%2f125-141&partnerID=40&md5=3123c66ba0b0e4288e077a3f2cea3f10,"This study investigates the use of the two varieties of Arabic, namely Modern Standard Arabic (MSA) and the Egyptian vernacular (EV), in dubbing animated movies in the Arab world. The corpus of the study consists of the original English dialogue and the two dubbed Arabic versions of the movie “A Bug's Life.” The corpus was analyzed both qualitatively and quantitatively. In the qualitative analysis, the Arabic scripts of the MSA and EV versions were examined to spot the differences between the two rendered translations. This was realized by using corpus manager and text analysis software. For the quantitative analysis, a questionnaire was used as a data-collection instrument to elicit the participants' reactions to the two varieties of Arabic. Quantitatively, the results showed that the participants were more in favor of the animations dubbed in the EV version than in the MSA one. The qualitative analysis showed that the quality of the EV dubbed version is better than its MSA counterpart, both linguistically and technically. The current study is quite significant as it probes into an important aspect of the audiovisual translation (AVT) domains, namely dubbing, which has gained great popularity in the Arab world in the last two decades. Finally, the study provides some implications and recommendations for further research on dubbing animated works. © Common Ground Research Networks, Farah Abu Lawi, Saleh Al-Salman, Ahmad S. Haider, All Rights Reserved.",Animated Movies; AVT; Dubbing; Egyptian Vernacular; MSA,,Article,Final,Scopus,2-s2.0-85147260630,Peter,,
"Kamath S.S., Martin A., Poojary R.",57477559900;58067662200;57190336014;,Effectuating Communication for the Deaf and Hard-of-Hearing: An Ethnographic Review,2022,"2022 International Conference on Electrical and Computing Technologies and Applications, ICECTA 2022",,,,80,83,,,10.1109/ICECTA57148.2022.9990196,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146367302&doi=10.1109%2fICECTA57148.2022.9990196&partnerID=40&md5=62cf2a02e12bc719f7e346d7c72fad1e,"The Deaf and Hard-of-Hearing constitute a significant populace, raising the need for technology, which is becoming ubiquitous in the world, to become more accessible for them. This paper enlists recent efforts and research popular in the accessibility sphere for the DHH and asks research questions based on the review. The DHH Demographic are scattered around the world, and while there exists an International Sign Language, people tend to utilize their regional versions of the Sign Language, impeding solidarity. Systems, representations, and interpretations of the different Sign Languages are divergent. Hence, this paper examines research done solely in the sphere of different renditions of the Sign Language. This paper also discerns certain challenges and suggests how technology can work towards accurate interpretation and communication for the DHH. The use of Computer Vision and Deep Learning Algorithms is especially prevalent in current research, and this paper focuses on identifying some novel frameworks and procedures which have varied and deep applications in the accessibility sphere, not just for the DHH. The purpose of this review paper is to understand current research methods and encourage collaboration of version-specific researchers to further and ameliorate research. © 2022 IEEE.",Accessibility; Computer Vision; Deaf and Hard-of-Hearing (DHH); Human-Computer Interaction; Sign Language,Audition; Computer vision; Deep learning; Human computer interaction; 'current; Accessibility; Deaf and hard-of-hearing; Divergents; Hard of hearings; Research method; Research questions; Review papers; Sign language; System representation; Spheres,Conference Paper,Final,Scopus,2-s2.0-85146367302,Peter,,
"Glorioso I.G., Arevalo S.F.Q., Decena M.B.S., Jolejole T.K.B., Gonzales M.S.",57209053782;58065085100;58066050600;58065218800;57209053690;,Developing and pre-testing of nutrition cartoon video to promote healthy eating among hearing and deaf and mute children,2022,Malaysian Journal of Nutrition,28,3,,409,422,,,10.31246/mjn-2021-0127,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146263063&doi=10.31246%2fmjn-2021-0127&partnerID=40&md5=c78f94590234e30dfae974d87b515c39,"Introduction: A six-minute nutrition cartoon video “The Magical Pinggang Pinoy in Nutrilandia” was developed and pre-tested to encourage hearing and deaf and mute children to eat a variety of foods by following the Pinggang Pinoy® (Healthy Plate). This study described the development process of the nutrition cartoon video and explored the participants’ acceptance towards it. Methods: The video underwent two levels of pre-testing to ensure comprehensibility, attractiveness, acceptability, and self-involvement. The first level was conducted among three DOST-FNRI experts, while the second level was among six deaf-mute school teachers and 30 mothers/ caregivers of 6-9 years old hearing children. Data were collected through an online self-administered questionnaire. Open-ended questions allowed participants to express themselves freely on the given subjects. Data analysis used thematic analysis. Results: The video conveyed clear information on the Pinggang Pinoy®, and the inclusion of animation, subtitles, visuals, and voice-over made the video easier to understand. Participants stated that the message of the video was directed to children, teens, adults, malnourished people, and everyone in general. Pre-testing the nutrition cartoon video before final production identified terminologies and concepts that participants found unfamiliar, confusing and unacceptable; offered suggestions for improvement and made pre-tested video appropriate for hearing and deaf-mute children. Conclusion: Overall, the participants had positive perceptions on the nutrition cartoon video. The video can be used in nutrition education classes among hearing and deaf and mute children, and serves as a tool to measure children’s nutrition knowledge on healthy eating. © 2022, Malaysian Journal of Nutrition. All Rights Reserved.",Cartoon video; Healthy eating; Hearing and deaf and mute children; Nutrition education; Pre-testing,,Article,Final,Scopus,2-s2.0-85146263063,Peter,,
Sanborn F.W.,6602916782;,A Cognitive Psychology of Mass Communication,2022,A Cognitive Psychology of Mass Communication,,,,1,554,,,10.4324/9781003154570,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142821110&doi=10.4324%2f9781003154570&partnerID=40&md5=1dacb44da5f9c6fb4b3e5c4c3206c92c,"The eighth edition of this text remains an indispensable resource for mass communication psychology and media effects courses. This book gives readers an in-depth understanding of how media affect our attitudes, thinking, and behavior. Continuing its academically rigorous yet student-friendly approach to this subject, the new edition has been thoroughly updated to reflect our current media landscape. Updates include new research and examples for an increasingly global perspective, an increased focus on social media, additional graphics, special end-of-chapter application sections, and an expansion in the list of references to reflect the latest research discussed. The book continues to emphasize the power of media, including social media, in affecting our perceptions of reality. There is also a detailed discussion of misinformation, disinformation, and fake news. Written in an engaging, readable style, the text is appropriate for graduate or undergraduate students in media psychology, mass communication psychology, and media effects courses. Accompanying online resources are also available for both students and instructors. For students: chapter outlines, additional review and discussion questions, useful links, and suggested further reading. For instructors: lecture slides, guidelines for in-class discussions, a sample syllabus, chapter summaries, useful links, and suggested further reading. Please visit www.routledge.com/9780367713553. © 2023 Fred W. Sanborn.",,,Book,Final,Scopus,2-s2.0-85142821110,Peter,,
"Gao W., Xiang W., Liu X., Wang X., Sun L.",57960492100;57188982440;57194243805;57960713700;55492960600;,Impacts of Presenting Extra Information in Short Videos via Text and Voice on User Experience,2022,"2022 14th International Conference on Quality of Multimedia Experience, QoMEX 2022",,,,,,,,10.1109/QoMEX55416.2022.9900918,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141687684&doi=10.1109%2fQoMEX55416.2022.9900918&partnerID=40&md5=948bada7bb3c10bae7211624f87a11db,"Short video is an increasingly prevalent medium in online shopping environments to present products. To cope with the great demand for short videos rising from the enormous number and the rapid update of online products, computer-supported video production is becoming a trend. The optimization of short videos considering user experience is essential. Currently, using text and voice to integrate extra information into short videos is a potential and promising approach for optimizing computer-supported video production, while the effects of these elements on user experience remain unclear. In this study, we conducted a questionnaire-based experiment including 580 participants to explore the impacts of presenting extra information in short videos via text and voice on multi-dimensional user experience. Results indicated that these two elements positively impacted user experience from different dimensions. Gender differences were also found in this study. Based on experimental results, we provided suggestions to support the use of text and voice elements in short video production considering user experience. © 2022 IEEE.",short video; short video production; text element; user experience; voice element,Multi dimensional; Online products; Online shopping; Optimisations; Short video; Short video production; Text elements; Users' experiences; Video production; Voice element; Surveys,Conference Paper,Final,Scopus,2-s2.0-85141687684,Peter,,
"Doush I.A., Al-Jarrah A., Alajarmeh N., Alnfiai M.",36187951200;57204517340;42960955500;57226608590;,Learning features and accessibility limitations of video conferencing applications: are people with visual impairment left behind,2022,Universal Access in the Information Society,,,,,,,,10.1007/s10209-022-00917-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138216949&doi=10.1007%2fs10209-022-00917-4&partnerID=40&md5=e7629553a5f178a97e2e81741b26ee56,"The COVID-19 pandemic increases the reliance on video conferencing applications for learning. Accessible video conferencing applications with good learning features can help people with visual impairment when they participate in online classes. This paper investigates the accessibility limitations and the available learning features of the top two current video conferencing applications, namely Zoom and MS Teams. A task-based expert review and a blind user evaluation are conducted using Web Content Accessibility Guidelines 2.1. In addition, the study identifies the application with the better learning features based on Universal Design for Learning guidelines. A set of recommendations are outlined for developing better inclusive video conferencing applications for people with visual impairment. The presented ideas can be applied to enhance the learning experience of people with visual impairment. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Accessibility; E-learning accessibility; Learning features; Universal design for learning; Video conferencing; Web content accessibility guidelines,Video conferencing; Web Design; Accessibility; E - learning; E-learning accessibility; Learning feature; Online class; Universal Design; Universal design for learning; Video-conferencing; Visual impairment; Web content accessibility guidelines; E-learning,Article,Article in Press,Scopus,2-s2.0-85138216949,Peter,,
"Sayed H.A.E., Al-Thubaity D.D., Nahari M.H., Alshahrani M.A., Ibrahim H.A., Elgzar W.T., Zaien S.Z., Alqahtani H., Bazuhair N.A., Said S.A.",57889991500;57203099811;57219330501;57226313657;57211561937;57211550985;57223114240;57606239200;57889283900;57889584100;,"Impact of an educational intervention on deaf and hard hearing females' knowledge and health beliefs regarding cervical cancer in Tabuk, Saudi Arabia: A theory-based study",2022,African Journal of Reproductive Health,26,7s,,52,60,,1.0,10.29063/ajrh2022/v26i7s.6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138004725&doi=10.29063%2fajrh2022%2fv26i7s.6&partnerID=40&md5=e8d59d00753a4580aca51cb6da496192,"Deaf people experience barriers to communication that prevent access to health care and information that puts them at increased risk for lack of knowledge about prevention and early detection approaches to cancers. With decreased screening, they may be at a higher risk of discovering cervical cancer (CC) at a late stage. This study aimed to evaluate the impact of an educational intervention on deaf and hard hearing females’ knowledge and health belief regarding cervical cancer in Tabuk, Saudi Arabia. A quasi-experimental study was performed from the beginning of April till the end of October 2021 using a convenience sample of 33 deaf and hard hearing married females students from Al-Amal center for deaf and hard hearing females and Tabuk University. The data were collected using an interview schedule composed of three parts translated to American Sign Language: background variables (basic data), cervical cancer knowledge quiz, and health belief model scale for CC and Pap smear test. About 75.8% of the participants were older than 20 years. The intervention-based HBM showed significant improvement in the overall knowledge score after the intervention compared to pre-intervention (FET = 16.345 P = 0.000). Moreover, significant enhancements (P<0.05) in all HBM construct scores after the HBM intervention compared to the pre-intervention. HBM-based educational interventions can be useful educational modalities for deaf and hard hearing populations. This intervention effectively enhanced the deaf and hard hearing females' overall knowledge and health beliefs scores. © 2022, Women's Health and Action Research Centre. All rights reserved.",cervical cancer; health belief model; Knowledge; persons with hearing impairments,,Article,Final,Scopus,2-s2.0-85138004725,Peter,,
"Muñoz C., Pattemore A., Avello D.",57200704773;57218417947;57859380800;,Exploring repeated captioning viewing as a way to promote vocabulary learning: time lag between repetitions and learner factors,2022,Computer Assisted Language Learning,,,,,,,1.0,10.1080/09588221.2022.2113898,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136575754&doi=10.1080%2f09588221.2022.2113898&partnerID=40&md5=3e892026d1bb78b27c064c2be1013b0f,"Repeated viewing of the same video is a common strategy among autonomous language learners as well as a much used pedagogical strategy among foreign language (FL) teachers. Learners may watch the same video more than once, to increase global comprehension of the target language or to focus their attention on linguistic aspects, such as new vocabulary or pronunciation. This study sought to examine to what extent repetition is more efficient for vocabulary learning if the second viewing follows the first immediately, or a week later. Participants were upper intermediate-level college learners who were distributed into three groups, one watched a TV series episode twice in the same session, one also watched the same episode twice but one week apart, and the last (control) group did not watch the video. Tests of word meaning recognition and word meaning recall were administered before and after viewing (pretest-immediate posttest-delayed posttest). The pretest and posttests contained 23 target items and 17 distracters (single words and multi-word expressions). In addition, the study explored the influence of two learner factors, each related to one of two verbal input channels: sound recognition for the audio and reading efficacy (reading speed and comprehension) for the onscreen text. The results indicated significant vocabulary learning from viewing and slightly higher benefits for the spaced repetition group at immediate posttest. The results also showed a significant influence of previous target vocabulary knowledge and of aptitude, as measured by the LLAMA D test, but not of reading efficacy. © 2022 Informa UK Limited, trading as Taylor & Francis Group.",aptitude; Audiovisual input; captions; reading efficacy; repeated viewing,,Article,Article in Press,Scopus,2-s2.0-85136575754,Peter,,
"Akgul O., Roberts R., Namara M., Levin D., Mazurek M.L.",36781766200;57212102174;57195480951;57213243673;36095672600;,Investigating Influencer VPN Ads on YouTube,2022,Proceedings - IEEE Symposium on Security and Privacy,2022-May,,,876,892,,,10.1109/SP46214.2022.9833633,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135901608&doi=10.1109%2fSP46214.2022.9833633&partnerID=40&md5=1eb2f6fdcbd74b73fac2378e8943e64c,"One widespread, but frequently overlooked, source of security information is influencer marketing ads on YouTube for security and privacy products such as VPNs. This paper examines how widespread these ads are, where on YouTube they are found, and what kind of information they convey. Starting from a random sample of 1.4% of YouTube, we identify 243 videos containing VPN ads with a total of 63 million views. Using qualitative analysis, we find that these ads commonly discuss broad security guarantees as well as specific technical features, frequently focus on internet threats, and sometimes emphasize accessing otherwise unavailable content. Different VPN companies tend to advertise in different categories of channels and emphasize different messages. We find a number of potentially misleading claims, including overpromises and exaggerations that could negatively influence viewers' mental models of internet safety. © 2022 IEEE.",advertising; security education; VPNs; YouTube,Virtual private networks; Advertizing; Mental model; Qualitative analysis; Random sample; Security and privacy; Security education; Technical features; VPN; YouTube; Marketing,Conference Paper,Final,Scopus,2-s2.0-85135901608,Peter,,
"McClenaghan I., Pardoe L., Ward L.",57852837900;57219177546;57190667190;,The next generation of audio accessibility,2022,AES Europe Spring 2022 - 152nd Audio Engineering Society Convention 2022,,,,469,478,,1.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135750001&partnerID=40&md5=12580fbc396706ebd96d9e78435455db,"Technological advances have enabled new approaches to broadcast audio accessibility, leveraging metadata generated in production and machine learning to improve blind source separation (BSS). This work presents two contributions to accessibility knowledge: first, a quantitative comparison of two audio accessibility methods, Narrative Importance (NI) and Dolby AC-4 BSS. Secondly, an evaluation of the audio access needs of neurodivergent audiences. The paper presents two comparative studies. The first study shows that the AC-4 BSS and NI methods are ranked consistently higher for clarity of dialogue (compared to the original mix) whilst improving, or retaining, perceived quality. A second study quantifies the effect of these methods on word recognition, quality and listening effort for a cohort including normal hearing, d/Deaf, hard of hearing and neurodivergent individuals, with NI showing a significant improvement in all metrics. Surveys of participants indicated some overlap between Neurodivergent and d/Deaf and hard of hearing participants' access needs, with similar levels of subtitle usage in both groups. © (2022) by the Audio Engineering Society All rights reserved.",,Audition; Surveys; Audio access; Comparatives studies; Hard of hearings; Machine-learning; New approaches; Normal hearing; Perceived quality; Quantitative comparison; Technological advances; Word recognition; Blind source separation,Conference Paper,Final,Scopus,2-s2.0-85135750001,Peter,,
"Saad M., Zia A., Raza M., Kundi M., Haleem M.",57202743015;57838004100;55761281200;57221951401;57221948008;,"A Comprehensive Analysis of Healthcare Websites Usability Features, Testing Techniques and Issues",2022,IEEE Access,10,,,97701,97718,,,10.1109/ACCESS.2022.3193378,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135736350&doi=10.1109%2fACCESS.2022.3193378&partnerID=40&md5=9c1ce276aa748abb81f8bd350fd7f591,"Healthcare has evolved significantly over time, from traditional healthcare systems to cutting-edge medical technologies. As these technologies advance, researchers have become interested in their usability. The usefulness of healthcare websites helps to provide more precise medical information. A comprehensive review of the literature is required to identify usability features, techniques, and issues in healthcare websites over a specified time period. In this study, articles from the years 2017-2021 are reviewed from well-known digital libraries i.e, IEEE, ACM, and ScienceDirect that include papers from various conferences, magazines, books, and journals. Initially, the study found 10,512 titles based on the search string developed from the proposed research questions which were then further filtered down to a total of 55 papers. This systematic literature review (SLR) summarises and collects relevant data in response to pre-defined research questions. This analysis of existing research will help website designers and developers, in developing more user-friendly healthcare websites for the users. In the future, this SLR will help in determining the optimal solutions and developing a framework for the identified usability challenges and limitations. It also includes employing the usability evaluation tools discovered by researchers to identify and fix usability issues on websites. © 2022 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.",Healthcare websites; human-computer interaction; usability; usability features; usability problems; usability testing,Digital libraries; Health care; Human computer interaction; Medical problems; Testing; Usability engineering; Healthcare website; Medical services; Research questions; Systematic; Systematic literature review; Usability; Usability feature; Usability problems; Usability testing; Websites,Article,Final,Scopus,2-s2.0-85135736350,Peter,,
"Wells T., Christoffels D., Vogler C., Kushalnagar R.",57807614700;57807614800;7005789370;36142036500;,Comparing the Accuracy of ACE and WER Caption Metrics When Applied to Live Television Captioning,2022,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),13341 LNCS,,,522,528,,,10.1007/978-3-031-08648-9_61,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134315487&doi=10.1007%2f978-3-031-08648-9_61&partnerID=40&md5=6a6609a7c41024ecfed2b7dfcac7b68c,"The development of caption metrics is relatively new in the accessibility research community. However, little work has been done comparing the effectiveness of newly developed caption metrics. More specifically, in low accuracy settings such as live television, where users report the most difficulty using captions. Through a user study with fifteen participants, we compared two caption metrics systems, Word Error Rate (WER) and Automated-Caption Evaluation (ACE), for their accuracy in evaluating caption quality in live television. We compared human-perceived quality statistics with each caption metric’s data. Analysis of the correlation between human statistics and each caption metric found that WER had a slightly higher correlation with participants. We found that ACE was more sensitive to errors that WER, and penalized captions more than participants. However, the difference in performance between WER and ACE was not statistically significant, and neither WER nor ACE are optimized for use with live television captioning. Future work should explore how caption metrics could be better optimized for use with live television. © 2022, Springer Nature Switzerland AG.",Accessibility; Automated-Caption Evaluation (ACE); Caption metrics; Deaf or Hard of Hearing (DHH); Live television; Word Error Rate (WER),Audition; Quality control; Speech recognition; Accessibility; Automated-caption evaluation; Caption metric; Deaf or hard of hearing; Hard of hearings; Live television; Research communities; User study; Word error rate; Errors,Conference Paper,Final,Scopus,2-s2.0-85134315487,Peter,,
"Cambra C., Silvestre N.",7801358308;15137439300;,How Children with and without Hearing Loss Describe Audiovisual Content,2022,"International Journal of Disability, Development and Education",,,,,,,,10.1080/1034912X.2022.2095354,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133509422&doi=10.1080%2f1034912X.2022.2095354&partnerID=40&md5=9fe4e762e2315fd1385b48706ed84857,"The main objective of this study was to explore the benefits of the use of educational audio-visual materials in facilitating learning for students with hearing loss. The study analysed whether students with hearing loss had a visual learning preference when they watched an audio-visual and if the images present contributed to the retention of more information by this group. The study sample was made up of 28 participants, from 7 to 9 years old, of whom 14 had prelingual hearing loss and 14 were age- and sex-matched students without hearing loss. They were all schooled together in general education classrooms in an oral modality. They were asked to watch an educational video and then to describe its contents orally. The results obtained from analysing the references to the video content indicate that, despite the fact that the hard of hearing group made greater reference to video content transmitted from the images than the group without hearing loss, the effect does not achieve statistical significance. The study reinforces the idea that deafness does not determine a specific learning preference. © 2022 Informa UK Limited, trading as Taylor & Francis Group.",Audiovisual material; documentary; education; hearing loss; images; language; learning; subtitles; video,,Article,Article in Press,Scopus,2-s2.0-85133509422,Peter,,
"Amin A.A., Mendis J., Kushalnagar R., Vogler C., Lee S., Huenerfauth M.",57767707700;57768191900;36142036500;7005789370;57189050289;12240800100;,Deaf and Hard of Hearing Viewers’ Preference for Speaker Identifier Type in Live TV Programming,2022,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),13308 LNCS,,,200,211,,,10.1007/978-3-031-05028-2_13,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133002592&doi=10.1007%2f978-3-031-05028-2_13&partnerID=40&md5=0b97a70e560473f146c7130134b04b0f,"When there are multiple people shown onscreen at one time, people who are Deaf and Hard of Hearing (DHH) viewing captions may find it challenging to determine who the current speaker is, especially when speakers interrupt each other abruptly or when there is a lot of turn-taking. Prior research has proposed several methods of indicating speakers, including in-text methods and methods in which the caption is dynamically located onscreen. However, prior work has not examined the effectiveness of various speaker-identifier methods for conveying who is speaking when the number of speakers on the screen increases. To determine which speaker-identifier methods are effective for DHH viewers, as the number of speakers on screen varies, we have conducted an empirical study with 31 DHH participants. We observed DHH viewers preference for speaker-identifier types, for videos that vary in the number of speakers shown onscreen. Determining the relationship between DHH viewers’ preference for speaker-identifier methods and the number of onscreen speakers can guide broadcasters to select appropriate speaker-identifier methods based on the number of speakers that appear on the screen. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Caption; Live-TV; Speaker-identifier,Conveying; 'current; Caption; Empirical studies; Hard of hearings; Live-TV; Multiple people; One-time; Speaker-identifier; Turn-taking; Audition,Conference Paper,Final,Scopus,2-s2.0-85133002592,Peter,,
"Umamaheswaran S., John R., Deepthi S.S., Dharmarajlu S.M.",56505429300;57751918800;57751666500;57577302200;,Caption positioning structure for hard of hearing people using deep learning method,2022,Journal of Discrete Mathematical Sciences and Cryptography,25,3,,623,633,,,10.1080/09720529.2021.2014126,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132252723&doi=10.1080%2f09720529.2021.2014126&partnerID=40&md5=fb42670a7873824086fb7226c6015dca,"Captioning for hard of hearing people is one of the priceless assistances to watch and understand various communication medium like TV, movies and others. The time presently appears to be ideal to respond to urgent inquiries and recognize the addressees and their needs/inclinations. Considering in terms of deafened, they will not understand who is asking the question and who is answering them back. This is the major drawback where a person with hearing disability will be unsure of the scene and about the whole movie. To overcome from this drawback, we propose a comprehensive captioning framework which is designed to extend the availability and interaction of the hard of hearing and hearing impaired: A full-fledged capture positioning structure that supports any multimedia file and a sensible inscription region movement method that lets the subtitle to be fittingly chosen the screen, considering the perceptible bit of the video scene and the dynamic/alive spokesperson perceived. The proposed framework for captioning is developed using Open CV and R-CNN. It consists of three modules as: 1. Audio Processing, 2. Video Processing, 3. Subtitle Positioning. The goal of this project is to give a better visual experience for the deafened and hard of hearing people. © 2022 Taru Publications.",68W99; Region of interest (ROI); Regions with convolutional neural networks (R-CNN),,Article,Final,Scopus,2-s2.0-85132252723,Peter,,
"Njelesani J., Si J., Swarm D.",42862096800;57686342400;57686342500;,Unreported and unaddressed: Students with disabilities experience of school violence in Zambia,2022,African Journal of Disability,11,,,1,7,,2.0,10.4102/AJOD.V11I0.849,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130047113&doi=10.4102%2fAJOD.V11I0.849&partnerID=40&md5=356bfd61d91d4ebb80427c6172c65dc5,"Background: Violence against school children is a prevalent global issue. Despite the high prevalence of school violence in Zambia, there is limited research on students with disabilities' experiences of school violence. Objectives: Guided by the socio-ecological model for bullying, the aim of this study was to understand students with disabilities' experiences of school violence in the Lusaka and Southern provinces of Zambia. Methods: A qualitative descriptive study was conducted with 14 purposively sampled boys (n = 6) and girls (n = 8) with disabilities. Data were generated using semi-structured interviews and child-friendly methods. Child-friendly methods were co-constructed with Zambian youth with disabilities in order to ensure cultural appropriateness and included vignettes, cartoon captioning, photograph elicitation, drawings, and sentence starters. Qualitative data were analysed by thematic analysis. Results: The themes illuminated that violence against students with disabilities occurs frequently but goes unaddressed. Moreover, students with disabilities were being blamed for causing the violence, and therefore, considered a risk to others. Participants reported that they turn to trusted teachers for support. Conclusion: This study illuminates the violence students with disabilities experience within the Zambian education system, with implications for school policies and programmes, peer education, and teacher training to create a safer education environment for students with disabilities. © 2022. The Authors. Licensee: AOSIS. This work. All Rights Reserved.",bullying; disability; inclusive education; qualitative; violence; Zambia,,Article,Final,Scopus,2-s2.0-85130047113,Peter,,
"Conde Ruano J.T., Tamayo A.",55216627200;56732667300;,Creation of an audio guide of the faculty of arts at the UPV/EHU: satisfaction of translation and interpreting students,2022,Interpreter and Translator Trainer,16,4,,540,557,,,10.1080/1750399X.2022.2051974,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126774690&doi=10.1080%2f1750399X.2022.2051974&partnerID=40&md5=1a58dbdfc34e65d182ad1d74daec751f,"This paper presents the perceived satisfaction of undergraduate students of translation and interpreting who have taken part in the creation of a multilingual, accessible and inclusive audio guide for the Faculty of Arts at the University of the Basque Country (UPV/EHU). A selection of students carried out the translation and proofreading of texts describing architectural spaces of the building, while others provided the recording in one of the languages of the audio guide (Spanish, Basque or English). Data on their satisfaction were collected through a questionnaire circulated in 2020. Results show that students involved in the project differed in their knowledge on accessibility and audio guides, but completed the project generally satisfied with the process and product. In addition, participants were aware of the benefits of such participation for their professional careers, for the institution as well as for the blind and people with low vision that may have need of this audio guide. The questionnaire served as the culmination of the students’ learning process and helped them reflect on the experience, which serves as an example of a learning process aiming to serve the community and easily exportable to other scenarios. © 2022 Informa UK Limited, trading as Taylor & Francis Group.",accessibility; audio description; audio guide; educational innovation project; questionnaire; satisfaction,,Article,Final,Scopus,2-s2.0-85126774690,Peter,,
Leis A.,57193817319;,Flipped Learning and Linguistic Self-Confidence,2022,International Journal of Computer-Assisted Language Learning and Teaching,12,1,,,,,,10.4018/IJCALLT.291107,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125443884&doi=10.4018%2fIJCALLT.291107&partnerID=40&md5=9eafef558eecfedf124e271e28fde63b,"The purpose of this study was to gain an understanding of what kind of student benefits most from studying under the flipped learning method. A total of 43 Japanese university students studying in a language pedagogy course participated in this quasi-experimental study. Qualitative data was taken from 385 study journal entries and interviews with 15 of the participants. The language (i.e., English or Japanese) used by students in the journals and interviews was used to measure their linguistic self-confidence. The results suggested that students with high linguistic self-confidence perceive the videos used for the flipped class as beneficial for their learning but that they preferred to challenge themselves by reading the textbook without scaffolding from the videos. Students with medium and low linguistic self-confidence, however, indicated that they found the videos were beneficial for increasing their understanding of the content of the textbook and thus participate actively in discussions held during class. Copyright © 2022, IGI Global.",CALL; Japan; University students,,Article,Final,Scopus,2-s2.0-85125443884,Peter,,
"Brinberg M., Ram N., Wang J., Sundar S.S., Cummings J.J., Yeykelis L., Reeves B.",57195485888;8613196000;57214140184;7103328070;35172471800;54421572700;7102248857;,Screenertia: Understanding “Stickiness” of Media Through Temporal Changes in Screen Use,2022,Communication Research,,,,,,,4.0,10.1177/00936502211062778,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125098121&doi=10.1177%2f00936502211062778&partnerID=40&md5=0c1bd2741507db957f0e3f7c79d1b8e6,"Descriptions of moment-by-moment changes in attention contribute critical elements to theory and practice about how people process media. We introduce a new concept called screenertia and use new screen-capture methodology to empirically evaluate its occurrence. We unobtrusively obtained 400,000+ screenshots of 30 participants’ laptop screens every 5 seconds for 4 days to examine individuals’ attention to their screens and how the distribution of attention differs across media content. All individuals’ screen segments were best described by a log-normal survival function—evidence of screenertia. Consistent with the literature on uses and gratifications of media, news/entertainment activities were the most “sticky.” These findings indicate that screenertia is not only related to the level of interactivity of media content but is also related to its modality and agency. Discussion of the findings highlights the importance of theorizing, examining, and modeling the specific time scales at which media behaviors manifest and evolve. © The Author(s) 2022.",intensive longitudinal data; laptop use; media attention; screenomics; survival analysis,,Article,Article in Press,Scopus,2-s2.0-85125098121,Peter,,
"Mussallem A., Panko T.L., Contreras J.M., Plegue M.A., Dannels W.A., Roman G., Hauser P.C., McKee M.M.",57226522152;55881480400;57057032000;55574300200;56301125700;57207047634;36058939700;37124462900;,Making virtual health care accessible to the deaf community: Findings from the telehealth survey,2022,Journal of Telemedicine and Telecare,,,,,,,3.0,10.1177/1357633X221074863,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124075833&doi=10.1177%2f1357633X221074863&partnerID=40&md5=d3634f492a93c9f84784c2ac23771503,"Introduction: To reduce COVID-19 exposure risk, virtual visits became widely adopted as a common form of healthcare delivery for the general population. It is unknown how this affected the deaf population, a sociolinguistic minority group that continues to face communication and healthcare barriers. The survey's objective was to describe the deaf participants’ experiences with telehealth visits. Methods: A 28-item online survey, available in American Sign Language and English, was developed and disseminated between November 2020 and January 2021. Ninety-nine deaf participants responded. Descriptive statistics were performed to assess the participant's virtual health care use, experiences, and communication approaches. Results: Seventy-five percent of respondents used telehealth at least once in the past 12 months (n = 74; age = 37.6 ± 14.5 years). Of those who used telehealth, nearly two-thirds experienced communication challenges (65.3%; n = 49). Half of the participants reported having to connect via a video relay service that employs interpreters who maintain general certification instead of a remote interpreter with specialized health care interpreting certifications for video visits with their health care providers (n = 37) and a third of participants reported needing to use their residual hearing to communicate with their providers (n = 25). Conclusion: Standard protocols for health care systems and providers are needed to minimize the burden of access on deaf patients and ensure virtual visits are equitable. It is recommended these visits be offered on Health Insurance Portability and Accountability Act-compliant platforms and include multi-way video to allow for the inclusion of remote medical interpreters and/or real-time captionists to ensure effective communication between the provider and the deaf patient occurs. © The Author(s) 2022.",access; equity; hearing loss; interpreter services; Telehealth; telemedicine; virtual care,,Article,Article in Press,Scopus,2-s2.0-85124075833,Peter,,
"Al-Abbas L.S., Haider A.S., Saideen B.",57219725689;57201379366;57348590300;,A quantitative analysis of the reactions of viewers with hearing impairment to the intralingual subtitling of Egyptian movies,2022,Heliyon,8,1,e08728,,,,5.0,10.1016/j.heliyon.2022.e08728,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122469373&doi=10.1016%2fj.heliyon.2022.e08728&partnerID=40&md5=d5ea9870fd3ea390495ac78359b0b6c5,"This study investigates how the viewers with hearing impairment reacted to the Modern Standard Arabic (MSA) subtitles added to some Vernacular Arabic movies during the COVID-19 stay-at-home period. A sample group of 106 deaf participants was asked to watch the MSA subtitled version of the Egyptian vernacular movie, Boushkash, and fill in an 18-item questionnaire of five constructs, namely, (1) movie watching habits, (2) technical aspects, (3) linguistic and paralinguistic information, (4) attitude, and (5) future actions and recommendations. The analysis showed that the intralingual subtitling of vernacular Arabic comedy movies was received positively by the participants. The technical specifications of the subtitles were satisfactory and adequate. The paralinguistic information was helpful as it offers a better understanding of the movie and creates a sense of reality in the movie's scenes. This indicates that intralingual subtitling is a step in the right direction that makes audiovisual materials accessible to people with hearing impairment and enhances their feeling of social inclusion. The study concludes that more governmental care in the Arab countries should be directed towards this minority group by urging national TV channels to add intralingual translation to their various programs. © 2022 The Author(s)",Accessibility; Hearing-impaired; Movies; SDH; Subtitling,,Article,Final,Scopus,2-s2.0-85122469373,Peter,,
"Dhanjal A.S., Singh W.",57211488147;41662186800;,An automatic machine translation system for multi-lingual speech to Indian sign language,2022,Multimedia Tools and Applications,81,3,,4283,4321,,7.0,10.1007/s11042-021-11706-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120675721&doi=10.1007%2fs11042-021-11706-1&partnerID=40&md5=4e590f60af6c8af767b250589aeecfa9,"Sign language (SL) is the best suited communication medium for hearing impaired people. Even with the advancement of technology, there is a communication gap between the hearing impaired and hearing people. The aim of this research work is to bridge this gap by developing an automatic system that translates the speech to Indian Sign Language using Avatar (SISLA). The whole system works in three phases: (i) The first phase includes the speech recognition (SR) of isolated words for English, Hindi and Punjabi in speaker independent environment (ii) The second phase translates the source language into Indian Sign Language (ISL) (iii) HamNoSys based 3D avatar represents the ISL gestures. The four major implementation modules for SISLA include: requirement analysis, data collection, technical development and evaluation. The multi-lingual feature makes the system more efficient. The training and testing speech sample files for English (12,660, 4218), Hindi (12,610, 4211) and Punjabi (12,600, 4193) have been used to train and test the SR models. Empirical results of automatic machine translation show that the proposed trained models have achieved the minimum accuracy of 91%, 89% and 89% for English, Punjabi and Hindi respectively. Sign language experts have also been used to evaluate the sign error rate through feedback. Future directions to enhance the proposed system using non-manual SL features along with the sentence level translation has been suggested. Usability testing based on survey results confirm that the proposed SISLA system is suitable for education as well as communication purpose for hearing impaired people. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",HamNoSys; Indian sign language; Machine translation; Multi-lingual speech; Speech recognition; Speech to Indian sign language,Audition; Computational linguistics; Computer aided language translation; Machine translation; Speech; Speech transmission; Surveys; Three dimensional computer graphics; Automatic machines; Hamnosys; Hearing impaired; Impaired people; Indian sign languages; Machine translation systems; Multi-lingual speech; Sign language; Speech to indian sign language; Speech recognition,Article,Final,Scopus,2-s2.0-85120675721,Peter,,
"Cui L., Zang C., Xu X., Zhang W., Su Y., Liversedge S.P.",36833471700;25629213400;57283968600;57775368000;57283521800;6701527784;,Predictability effects and parafoveal processing of compound words in natural Chinese reading,2022,Quarterly Journal of Experimental Psychology,75,1,,18,29,,,10.1177/17470218211048193,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116310348&doi=10.1177%2f17470218211048193&partnerID=40&md5=c6ad709ebdffb915e20db0e14c8671a1,"We report a boundary paradigm eye movement experiment to investigate whether the predictability of the second character of a two-character compound word affects how it is processed prior to direct fixation during reading. The boundary was positioned immediately prior to the second character of the target word, which itself was either predictable or unpredictable. The preview was either a pseudocharacter (nonsense preview) or an identity preview. We obtained clear preview effects in all conditions, but more importantly, skipping probability for the second character of the target word and the whole target word from pretarget was greater when it was predictable than when it was not predictable from the preceding context. Interactive effects for later measures on the whole target word (gaze duration and go-past time) were also obtained. These results demonstrate that predictability information from preceding sentential context and information regarding the likely identity of upcoming characters are used concurrently to constrain the nature of lexical processing during natural Chinese reading. © Experimental Psychology Society 2021.",Chinese reading; compound word; Predictability; preview effects,"attention; China; eye fixation; eye movement; human; reading; retina fovea; Attention; China; Eye Movements; Fixation, Ocular; Fovea Centralis; Humans; Reading",Article,Final,Scopus,2-s2.0-85116310348,Peter,,
"Zhong L., Noud B.P., Pruitt H., Marcrum S.C., Picou E.M.",57224929879;57224941945;57224919982;56534302900;36026893100;,Effects of text supplementation on speech intelligibility for listeners with normal and impaired hearing: a systematic review with implications for telecommunication,2022,International Journal of Audiology,61,1,,1,11,,3.0,10.1080/14992027.2021.1937346,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108601233&doi=10.1080%2f14992027.2021.1937346&partnerID=40&md5=330175e96cc2ff9277c5f54f0cb4f7df,"Objective: Telecommunication can be difficult in the presence of noise or hearing loss. The purpose of this study was to systematically review evidence regarding the effects of text supplementation (e.g. captions, subtitles) of auditory or auditory-visual signals on speech intelligibility for listeners with normal or impaired hearing. Design: Three databases were searched. Articles were evaluated for inclusion based on the Population Intervention Comparison Outcome framework. The Effective Public Health Practice Project instrument was used to evaluate the quality of the identified articles. Study sample: After duplicates were removed, the titles and abstracts of 2019 articles were screened. Forty-six full texts were reviewed; ten met inclusion criteria. Results: The quality of all ten articles was moderate or strong. The articles demonstrated that text added to auditory (or auditory-visual) signals improved speech intelligibility and that the benefits were largest when auditory signal integrity was low, accuracy of the text was high, and the auditory signal and text were synchronous. Age and hearing loss did not affect benefits from the addition of text. Conclusions: Although only based on ten studies, these data support the use of text as a supplement during telecommunication, such as while watching television or during telehealth appointments. © 2021 British Society of Audiology, International Society of Audiology, and Nordic Audiological Society.",Aging; behavioral measures; captions; speech perception; tele-audiology/tele-health; text,"auditory threshold; dietary supplement; hearing; hearing impairment; human; perception deafness; speech intelligibility; speech perception; telecommunication; Auditory Threshold; Deafness; Dietary Supplements; Hearing; Hearing Loss; Hearing Loss, Sensorineural; Humans; Speech Intelligibility; Speech Perception; Telecommunications",Review,Final,Scopus,2-s2.0-85108601233,Peter,,
"Szarkowska A., Boczkowska J.",54416458200;57224053960;,Colour coding subtitles in multilingual films–a reception study,2022,Perspectives: Studies in Translation Theory and Practice,30,3,,520,536,,1.0,10.1080/0907676X.2020.1853186,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106835883&doi=10.1080%2f0907676X.2020.1853186&partnerID=40&md5=c85521d86c664e28a76489a3275051b2,"Multilingual films do not always travel well across borders. When translated, multilingualism may become neutralised and washed out. In subtitling, multiple languages used in the original are usually not marked in any way in the target subtitles. This results in neutralisation and linguistic homogenisation, whereby target viewers may not even realise that different languages are spoken in the film. In this study, we test a new solution aimed to help preserve the multilingual nature of films in interlingual subtitling: the strategy of colour coding. To gauge the impact of colour coding on viewers, we conducted a survey-based reception study on a group of 52 Polish hearing speakers who watched a multilingual film with interlingual subtitles in two versions: one where subtitles were coloured, i.e. each language was marked with a different colour, and the other, control condition, where the subtitles were traditionally white. We measured participants’ immersive tendency, immersion, comprehension, and cognitive load. While colour coding did not have a significant impact on immersion, we found that individuals with a lower tendency for immersion reported higher immersion levels when watching coloured subtitles. Our results may contribute to the discussion how to render the multilingual nature of films in interlingual subtitling. © 2020 Informa UK Limited, trading as Taylor & Francis Group.",cognitive load; colour coding; immersion; immersive tendency; Multilingualism; subtitling,,Article,Final,Scopus,2-s2.0-85106835883,Peter,,
"Rodríguez J., Díaz M.V., Collazos O., García-Crespo Á.",57221384621;57198974067;57221377251;57207533415;,GoCC4All a pervasive technology to provide access to TV to the deafblind community,2022,Assistive Technology,34,4,,383,391,,1.0,10.1080/10400435.2020.1829176,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098936010&doi=10.1080%2f10400435.2020.1829176&partnerID=40&md5=4434cf41b6f98665dce0fd5766964928,"Considering the importance of communication and independence for the deafblind community, this work presents findings of the use of technology to address the lack of information due to communication challenges among the deafblind community. Over time, many investigations have been carried out regarding this matter, but very few providing solution, which is why this study emerged, looking to making all the information broadcasted through television accessible for this community. The work team designed a technology (GoCC4All) to address the needs of the deafblind community. GoCC4All provides access to captions available on TV through braille displays and mobile devices. Our research process and results outline the path for creating, adapting, and adopting new technologies for people with disabilities who have the right to access the information just as their peers without disabilities. The information in this paper is based on two surveys, an initial beta testing (BT) and a final survey among a group of 14 users (UT) who tested the GoCC4All application. Our findings support the positive impact of the iterative creation of assistive technology based on users' experience and users’ recommendations to better serve the needs of the deafblind community. © 2020 RESNA.",accessible technology; communication; deafblindness; emergency information,Display devices; Surveys; User experience; Assistive technology; Beta testing; Braille display; People with disabilities; Pervasive technologies; Research process; Users' experiences; Work team; Technology transfer; deafblindness; disabled person; human; interpersonal communication; self help device; technology; Communication; Deaf-Blind Disorders; Disabled Persons; Humans; Self-Help Devices; Technology,Article,Final,Scopus,2-s2.0-85098936010,Peter,,
Black S.,57220177354;,Could integrated subtitles benefit young viewers? Children’s reception of standard and integrated subtitles: a mixed methods approach using eye tracking,2022,Perspectives: Studies in Translation Theory and Practice,30,3,,503,519,,3.0,10.1080/0907676X.2020.1849324,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097143903&doi=10.1080%2f0907676X.2020.1849324&partnerID=40&md5=18256a6ee191031aa561397f8ecaac05,"This paper investigates children’s reception of AV content with standard and integrated interlingual subtitles. To this end, an experimental study was conducted with 17 children aged 8–9 years, to assess their reception of clips of an animated film in Spanish with standard and integrated subtitles in English. A mixed methods approach was adopted with the aim of obtaining a robust, comprehensive understanding of the children’s reception of the subtitled AV content, using eye tracking, scene recognition tests, content comprehension tests, questionnaires, and interviews. It was established that the children spent a significantly larger proportion of their viewing time looking at the images and also fixated more times on the images when the subtitles were in the integrated position. However, the hypothesis that participants would exert lower levels of cognitive effort when watching the AV clips with integrated subtitles was only partially confirmed. The integrated subtitles did not have detrimental effects on their viewing patterns, scene recognition performance, or comprehension of the clips. The majority approved highly of both subtitle positions and perceived both as easy to read and understand. These findings show that further research in this area is warranted. © 2020 Informa UK Limited, trading as Taylor & Francis Group.",children; cognitive processing; eye tracking; Integrated subtitles,,Article,Final,Scopus,2-s2.0-85097143903,Peter,,
"Pearson J., Payne D., Yoshida K., Garrett N.",23571191600;7402805525;57709867900;7006286844;,Access to and engagement with cervical and breast screening services for women with disabilities in Aotearoa New Zealand,2022,Disability and Rehabilitation,44,10,,1984,1995,,2.0,10.1080/09638288.2020.1817158,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091071690&doi=10.1080%2f09638288.2020.1817158&partnerID=40&md5=74ad79b7429928dbf04ef975b344a560,"Purpose: To describe access to and engagement with cervical and breast screening services for women who are Deaf or live with a physical or sensory disability in Aotearoa New Zealand (NZ). Method: We carried out an online survey on a convenience sample of 84 women. Tests of association were undertaken between socio-demographics and cervical and breast screening; and between disability type, and health outcomes and barriers to screening respectively. Participants also reported specific barriers to screening. Results: Living without family/partner and unemployment were associated with never having a cervical smear. Non-English preferred language, and urban residence were related to lower levels of breast self-examination; having insufficient income was related to never having a mammogram. Disability type was not related to either smear or mammogram on eligibility, uptake ever, or uptake timeframe. A higher proportion of those with multiple disability types experienced service environment barriers to having a cervical smear. Specific barriers to screening covered accessibility, service environment, and information. Conclusions: This study, unique in Aotearoa, provides insights into disabled women’s access to and engagement with screening services and suggests factors that may inhibit or facilitate participation. Women with multiple disabilities may be disadvantaged in the seeking and delivery of screening.Implications for rehabilitation Rehabilitation and other practitioners need to be attuned to how women living with multiple disabilities may be disadvantaged in the seeking of, and, more importantly, the delivery of breast or cancer screening. Practitioners need to discuss with disabled women what supports or resources they need to have screening procedures, and to advocate for these supports for their clients. Practitioners need to ensure accessibility that encompasses the whole screening journey from the initial invitation to the obtaining of results. For practitioners to be able to provide equitable service delivery, the government and institutional policies and procedures that are developed must take into consideration the multiple needs of women living with disabilities. © 2020 Informa UK Limited, trading as Taylor & Francis Group.",Breast self-examination; health services accessibility; health services for persons with disabilities; mammography; Papanicolaou test; women’s health,breast tumor; disabled person; early cancer diagnosis; female; health care delivery; human; mass screening; New Zealand; procedures; vagina smear; Breast Neoplasms; Disabled Persons; Early Detection of Cancer; Female; Health Services Accessibility; Humans; Mass Screening; New Zealand; Vaginal Smears,Article,Final,Scopus,2-s2.0-85091071690,Peter,,
"Zahedi S., Khoshsaligheh M.",57556089700;56946904400;,Eyetracking the impact of subtitle length and line number on viewers’ allocation of visual attention,2021,"Translation, Cognition and Behavior",4,2,,331,352,,1.0,10.1075/tcb.00058.zah,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127331161&doi=10.1075%2ftcb.00058.zah&partnerID=40&md5=e841c88981095070de6644797e8531dc,"Compared to one-line subtitles, two-line subtitles are believed to receive more attention from viewers based on previous research. Yet, in the majority of these studies, two-liners are considerably longer than the one-line subtitles. The authors argue that the findings of the previous studies could have been affected by the difference in subtitle length, and there is a need to operationally distinguish between the impact of subtitle length and line number on viewers’ attention allocation. Therefore, an SMI eye tracker was used in this study to record the eye movements of 32 Iranian viewers while reading the Persian subtitles of a short segment of a feature film, A Prophet (Jacques Audiard 2009). The results showed that the viewers’ attention to one-line subtitles was significantly greater than the attention they allotted to two-line subtitles although they were of the same length. The attention allocated to the long subtitles was also significantly greater compared to the attention paid to the short subtitles. Retrospective interviews also showed that the participants favored short and two-line subtitles. © John Benjamins Publishing Company.",Eye tracking; Line number; Subtitle length; Subtitle reception,,Article,Final,Scopus,2-s2.0-85127331161,Peter,,
"Desjardins A., Tomico O., Lucero A., Cecchinato M.E., Neustaedter C.",39061273100;26427847700;16230372800;55851220100;13009033100;,Introduction to the Special Issue on First-Person Methods in HCI,2021,ACM Transactions on Computer-Human Interaction,28,6,37,,,,11.0,10.1145/3492342,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122441234&doi=10.1145%2f3492342&partnerID=40&md5=27c8f55398c0362710c3ca98e3ada967,"In this introduction to the special issue on First-Person Methods in (Human-Computer Interaction) HCI, we present a brief overview of first-person methods, their origin, and their use in Human-Computer Interaction. We also detail the difference between first-person methods, second-person, and third-person methods, as a way to guide the reader when engaging the special issue articles. We articulate our motivation for putting together this special issue: we wanted a collection of works that would allow HCI researchers to develop further, define, and outline practices, techniques and implications of first-person methods. We trace links between the articles in this special issue and conclude with questions and directions for future work in this methodological space: working with boundaries, risk, and accountability. © 2021 Copyright held by the owner/author(s).",autobiographical design; autoethnography; design research; first-person methods; First-person research; HCI research; somaesthetics,Human engineering; Autobiographical design; Autoethnography; Design research; First person; First-person method; First-person research; HCI research; Human computer interaction (HCI); Somesthetic; Human computer interaction,Review,Final,Scopus,2-s2.0-85122441234,Peter,,
"Yi J.H., Kim S., Noh Y.-G., Ok S., Hong J.-H.",57218452396;57222401704;57226867113;57354793000;8850437000;,Design proposal for sign language services in TV broadcasting from the perspective of people who are deaf or hard of hearing,2021,Applied Sciences (Switzerland),11,23,11211,,,,,10.3390/app112311211,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119963385&doi=10.3390%2fapp112311211&partnerID=40&md5=8910f759c6e9d557a12860f08bc94141,"Sign language services are provided so that people with hearing loss are not alienated from socially and politically important information through TV broadcasting. In this paper, we conducted a user survey and evaluation of the current sign language services for deaf or hard-of-hearing (DHH) people, and solutions were proposed for the problems found in the course of the analyses. To this end, a total of five stages of research were conducted. First, the communication problems experienced by DHH individuals and previous studies on their language and information acquisition were investigated. Second, the most typical types of information delivery channels via TV were defined as news, discussions, and weather reports, and by investigating the actual sign language service cases for each type, three visual information delivery elements were identified: sign language interpreters, reference videos, and subtitles. Third, a preference survey, an interview survey, and an eye tracker experiment on the DHH participants were conducted with varying arrangement options of information delivery elements. Fourth, based on the results of the investigations and experiments, the options to be considered when arranging information delivery elements were compiled. The results showed that the sign language interpreter, which is the first element of information delivery, should be presented in a size clearly visible because the visibility of their facial expressions is important. In addition, it is recommended to present the interpreter without a background since DHH participants did not prefer the presence of a background. As for subtitles, which is the third element of information delivery, it was confirmed that the provision of sign language interpretation and subtitles together helped DHH participants to understand the contents more quickly and accurately. Moreover, if there are multiple speakers, individual subtitles for each speaker should be provided so that the viewers can understand who is talking. Reference videos, which are mainly placed on the screen background, the second information delivery element, were considered less important to DHH participants compared to sign language interpreters and subtitles, and it was found that DHH participants preferred reference videos to be visually separated from sign language interpreters. Fifth, based on the overall results of the study, a screen layout design was proposed for each type of information delivery element for DHH people. Contrary to the general conception that there would be no problem in viewing information-delivering TV broadcasts by DHH people simply by placing a sign language interpreter on the screen, the results of this study confirmed that a more delicate screen layout design is necessary for DHH people. It is expected that this study will serve as a helpful guide in providing better sign language services for TV broadcasts that can be conveniently viewed by both DHH and non-disabled people. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Deaf or hard-of-hearing (DHH) people; Layout design; TV sign language service; Usability test,,Article,Final,Scopus,2-s2.0-85119963385,Peter,,
"Kafle S., Dingman B., Huenerfauth M.",57200500342;57218761507;12240800100;,Deaf and Hard-of-hearing Users Evaluating Designs for Highlighting Key Words in Educational Lecture Videos,2021,ACM Transactions on Accessible Computing,14,4,3470651,,,,,10.1145/3470651,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118650839&doi=10.1145%2f3470651&partnerID=40&md5=961a775a8f6a004f4a2a246c5ee72d07,"There are style guidelines for authors who highlight important words in static text, e.g., bolded words in student textbooks, yet little research has investigated highlighting in dynamic texts, e.g., captions during educational videos for Deaf or Hard of Hearing (DHH) users. In our experimental study, DHH participants subjectively compared design parameters for caption highlighting, including: Decoration (underlining vs. italicizing vs. boldfacing), granularity (sentence level vs. word level), and whether to highlight only the first occurrence of a repeating keyword. In partial contrast to recommendations in prior research, which had not been based on experimental studies with DHH users, we found that DHH participants preferred boldface, word-level highlighting in captions. Our empirical results provide guidance for the design of keyword highlighting during captioned videos for DHH users, especially in educational video genres. © 2021 Association for Computing Machinery.",Caption highlighting; captioning system; deaf and hard of hearing; text highlighting; user study,Caption highlighting; Captioning system; Deaf and hard of hearing; Educational videos; Hard of hearings; Key words; Lecture video; Text highlighting; User study; Word level,Article,Final,Scopus,2-s2.0-85118650839,Peter,,
"Tamayo A., Agirrezabalaga E.M.",56732667300;57190673426;,The reception of multilingual films: The case of handia [La recepción de películas multilingües: El caso de Handia],2021,Sendebar,32,,,90,110,,,10.30827/sendebar.v32.13509,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119627146&doi=10.30827%2fsendebar.v32.13509&partnerID=40&md5=3606fa89045cf06370319d8777664fc1,"This paper presents a reception study on a multilingual product in audiovisual translation (AVT), which is, to date, an underresearched field. In this contribution the authors present the reception analysis of the multilingual fiction film Handia (Aitor Arregi and Jon Garaño 2017), which was filmed mainly in Basque and contains various L3. Data were gathered using GoogleForms and contacting participants via social networks. A total of 534 responses were obtained. After a demographic analysis of the participants, information was collected and divided into three blocks: Audiovisual consumption habits, characteristics of the viewing of the film and perception of the film and its translation. The results were analysed both quantitatively and qualitatively and show that participants are not fully aware of the version they have watched. Nevertheless, they have a good perception of the film and its translation, mostly in subtitled versions. © 2021 Universidad de Granada. All rights reserved.",Audiovisual translation; Basque; Multilingualism; Reception studies,,Article,Final,Scopus,2-s2.0-85119627146,Peter,,
"Carter S.M., Shih P., Williams J., Degeling C., Mooney-Somers J.",7402609472;55538917400;56368196200;57202999059;25028045100;,Conducting Qualitative Research Online: Challenges and Solutions,2021,Patient,14,6,,711,718,,13.0,10.1007/s40271-021-00528-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107468909&doi=10.1007%2fs40271-021-00528-w&partnerID=40&md5=301c1d77a2aa9a2104e9923d935eef88,"What ways of thinking and concrete strategies can assist qualitative health researchers to transition their research practice to online environments? We propose that researchers should foreground inclusion when designing online qualitative research, and suggest ethical, technological and social adaptations required to move data collection online. Existing research shows that this move can aid in meeting recruitment targets, but can also reduce the richness of the data generated, as well as how much participants enjoy participating, and the ability to achieve consensus in groups. Mindful and consultative choices are required to prevent these problems. To adapt to ethical challenges, researchers should especially consider participant privacy, and ways to build rapport and show appropriate care for participants, including protocols for dealing with distress or disengagement, managing data, and supporting consent. To adapt to technological challenges, research plans should choose between online modalities and platforms based on a clear understanding of their particular affordances and the implications of these. Finally, successful research in virtual social environments requires new protocols for engagement before data collection, attention to group numbers and dynamics, altered moderator teams and roles, and new logistical tasks for researchers. The increasing centrality of online environments to everyday life is driving traditional qualitative research methods to online environments and generating new qualitative research methods that respond to the particularities of online worlds. With strong design principles and attention to ethical, technical and social challenges, online methods can make a significant contribution to qualitative research in health. © 2021, The Author(s).",,Article; attention; distress syndrome; human; informed consent; medical research; online system; pandemic; qualitative research; research ethics; scientist; social distancing; social environment; information processing; personnel; qualitative research; Data Collection; Humans; Qualitative Research; Research Personnel,Article,Final,Scopus,2-s2.0-85107468909,Peter,,
"Farooq U., Rahim M.S.M., Sabir N., Hussain A., Abid A.",56707014500;57210569784;56699305100;19734290900;36614939400;,"Advances in machine translation for sign language: approaches, limitations, and challenges",2021,Neural Computing and Applications,33,21,,14357,14399,,13.0,10.1007/s00521-021-06079-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106263988&doi=10.1007%2fs00521-021-06079-3&partnerID=40&md5=65dccab4ec2b447317bc2952d1c88a4d,"Sign languages are used by the deaf community around the globe to communicate with one another. These are gesture-based languages where a deaf person performs gestures using hands and facial expressions. Every gesture represents a word or a phrase in the natural language. There are more than 200 different sign languages in the world. In order to facilitate the learning of sign languages by the deaf community, researchers have compiled sign language repositories comprising of gestures. Similarly, algorithms have been proposed to translate the natural language into sign language, which is subsequently converted into gestures using avatar technology. On the other hand, several different approaches for gesture recognition have also been proposed in the literature, many of which use specialized hardware. Similarly, cell phone applications have been developed for learning and translation of sign languages. This article presents a systematic literature review of these multidisciplinary aspects of sign language translation. It provides a detailed analysis of carefully selected 147 high-quality research articles and books related to the subject matter. Specifically, it categorizes different approaches used for each component, discusses their theoretical foundations, and provides a comparative analysis of the proposed approaches. Lastly, open research challenges and future directions for each facet of the sign language translation problem have been discussed. To the best of our knowledge, this is the first comprehensive survey on sign language translation that discusses state-of-the-art research from multi-disciplinary perspectives. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Avatar technology; Gesture recognition; Natural to sign language translation; Sign language; Sign language repositories,Computer aided language translation; Cell phone application; Comparative analysis; Disciplinary perspective; High-quality research; Machine translations; Specialized hardware; Systematic literature review; Theoretical foundations; Quality control,Article,Final,Scopus,2-s2.0-85106263988,Peter,,
"Shirley B., Ward L.",12797467200;57190667190;,Intelligibility versus comprehension: understanding quality of accessible next-generation audio broadcast,2021,Universal Access in the Information Society,20,4,,691,699,,3.0,10.1007/s10209-020-00741-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087987660&doi=10.1007%2fs10209-020-00741-8&partnerID=40&md5=287b82ea9e0ccb85be2feeec4663dcb1,"For traditional broadcasting formats, implementation of accessible audio strategies for hard of hearing people have used a binary, intelligibility-based approach. In this approach, sounds are categorized either as speech, contributing to comprehension of content, or non-speech, which can mask the speech and reduce intelligibility. Audio accessibility solutions have therefore focused on speech enhancement type methods, for which several useful standard objective measures of quality exist. Recent developments in next-generation broadcast audio formats, in particular the roll out of object-based audio, facilitate more in-depth personalisation of the audio experience based on user preferences and needs. Recent research has demonstrated that many non-speech sounds do not strictly behave as maskers but can be critical for comprehension of the narrative for some viewers. This complex relationship between speech, non-speech audio and the viewer necessitate a more holistic approach to understanding quality of experience of accessible media. This paper reviews previous work and outlines such an approach, discussing accessibility strategies using next-generation audio formats and their implications for developing effective assessments of quality. © 2020, The Author(s).",Accessibility; Broadcast; Hearing impairment,Audition; Quality of service; Speech enhancement; User experience; Complex relationships; Hard of hearings; Holistic approach; Non-speech audio; Objective measure; Personalisation; Quality of experience (QoE); Recent researches; Speech intelligibility,Article,Final,Scopus,2-s2.0-85087987660,Peter,,
"Pérez-Martín J., Rodriguez-Ascaso A., Molanes-López E.M.",57192989235;21934965700;24280354200;,Quality of the captions produced by students of an accessibility MOOC using a semi-automatic tool,2021,Universal Access in the Information Society,20,4,,677,690,,1.0,10.1007/s10209-020-00740-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087517984&doi=10.1007%2fs10209-020-00740-9&partnerID=40&md5=7749348abfa5dd264a055ccd0be86fb7,"Some people have problems accessing multimedia services available on the web. Contributions to accessibility made by end users who lack institutional support in the design, production or deployment of media accommodations should be considered. The aim of our paper is to assess the quality of the captions produced using YouTube by non-professional subtitlers who only received basic training. We also identify potential improvements either in the subtitling tool or in the training resources, which could enhance the quality of the captions. We conducted a study in which 53 participants of a MOOC on digital accessibility used the automatic speech recognition (ASR) feature of YouTube to produce the captions for a video provided by the teaching staff. We assessed the quality of the captions produced by the students and then compared it with the quality of the captions produced by: (a) a human expert and (b) the ASR-based subtitling by YouTube. Students’ errors occurred mainly in the number of characters per line, the speed of the captions, failing to use a new line per participant, and not including sound effects. The course should warn students to use a new line per participant, teach them how to subtitle sound effects, specify the maximum number of characters per line of text, and inform that in some countries such as Spain, captions can be edited but in other countries this may not be possible. Our recommendations for the YouTube editor include improving both the user interface and the ASR, with a view to enhancing ongoing and future research. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",Accessibility; Authoring tools; Automatic speech recognition (ASR); Captions; Media; Quality,Multimedia services; Speech recognition; User interfaces; Automatic speech recognition; Basic training; End users; Human expert; Institutional support; Semi-automatic tools; Sound effects; Teaching staff; Students,Article,Final,Scopus,2-s2.0-85087517984,Peter,,
"Jun J., Seo W., Park J., Park S., Jung H.",57312658600;57193711404;57312436700;57212190272;57207486977;,Exploring the Experiences of Streamers with Visual Impairments,2021,Proceedings of the ACM on Human-Computer Interaction,5,CSCW2,297,,,,3.0,10.1145/3476038,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117942211&doi=10.1145%2f3476038&partnerID=40&md5=8c29ff9c77ad6025ade9ac7e1e7429ba,"Live streaming refers to the broadcast of real-time videos, allowing people to have synchronous interactions. While researchers' interest in live streaming has increased recently, the accessibility of live streaming for people with visual impairments is still under-examined. Further studies are necessary to gain a better understanding of how streamers with visual impairments (SVI) engage in various activities on live streaming platforms. Based on semi-structured interviews with 14 participants, we identified SVI's motivations for live streaming, their unique interactions with videos and people on live streaming platforms, and the challenges they face during live streaming. Our analysis of the identified themes revealed the absence of an SVI-centered community and accessibility issues for SVI while learning to live stream, use tools, and interact with people. Based on the results of this study, we present design opportunities to better support SVI on live streaming platforms. © 2021 ACM.",accessibility; live streaming; qualitative; streamers; visual impairments,Live streaming; Qualitative; Real time videos; Semi structured interviews; Streamer; Synchronous interactions; Visual impairment,Article,Final,Scopus,2-s2.0-85117942211,Peter,,
"McDonnell E.J., Liu P., Goodman S.M., Kushalnagar R., Froehlich J.E., Findlater L.",57222060653;57224992974;57219114076;36142036500;7101665384;10040303000;,"Social, Environmental, and Technical: Factors at Play in the Current Use and Future Design of Small-Group Captioning",2021,Proceedings of the ACM on Human-Computer Interaction,5,CSCW2,434,,,,5.0,10.1145/3479578,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117915366&doi=10.1145%2f3479578&partnerID=40&md5=de0a051a7dfa9837a41f5ddfa0b56d3f,"Real-time captioning is a critical accessibility tool for many d/Deaf and hard of hearing (DHH) people. While the vast majority of captioning work has focused on formal settings and technical innovations, in contrast, we investigate captioning for informal, interactive small-group conversations, which have a high degree of spontaneity and foster dynamic social interactions. This paper reports on semi-structured interviews and design probe activities we conducted with 15 DHH participants to understand their use of existing real-time captioning services and future design preferences for both in-person and remote small-group communication. We found that our participants' experiences of captioned small-group conversations are shaped by social, environmental, and technical considerations (e.g., interlocutors' pre-established relationships, the type of captioning displays available, and how far captions lag behind speech). When considering future captioning tools, participants were interested in greater feedback on non-speech elements of conversation (e.g., speaker identity, speech rate, volume) both for their personal use and to guide hearing interlocutors toward more accessible communication. We contribute a qualitative account of DHH people's real-time captioning experiences during small-group conversation and future design considerations to better support the groups being captioned, both in person and online.? © 2021 ACM.",accessibility; captioning; d/deaf and hard of hearing; small-group conversation,Speech communication; 'current; Captioning; D/deaf and hard of hearing; Environmental factors; Future designs; Hard of hearings; Real- time; Small-group conversation; Social factor; Technical factors; Audition,Article,Final,Scopus,2-s2.0-85117915366,Peter,,
"Unger A., Wallach D.P., Jochems N.",57210183103;57193143418;36179986500;,Lost in Translation: Challenges and Barriers to Sign Language-Accessible User Research,2021,ASSETS 2021 - 23rd International ACM SIGACCESS Conference on Computers and Accessibility,,,3476473,,,,4.0,10.1145/3441852.3476473,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119302363&doi=10.1145%2f3441852.3476473&partnerID=40&md5=11f66c4617a86f70f5eb2d907a251642,"In this experience report, we describe an approach to ability-based focus groups with sign language users in a remote environment. We discuss our main lessons learned in terms of requirements for sign language-accessibility within research, calling out issues such as the need to address users in their natural language, ensuring translation for all parts of research processes, and including users not only within the conducted method but already within preparation phases. Based on requirements such as these, we argue that HCI research currently faces a dilemma when it comes to hearing researchers working with the sign language user population-having to handle the increasingly emphasized demand for conducting user research with this specific target group while lacking accessible tools and procedures to do so. Concluding our experience report, we address this dilemma by discussing the two sides of its fundamental challenge: Inadequate communication with and insufficient representation of sign language users within research. © 2021 ACM.",Deaf; Focus Groups; Hard of Hearing; Language-Accessibility; Sign Language,Translation (languages); Deaf; Experience report; Focus groups; Hard of hearings; Language-accessibility; Natural languages; Remote environment; Research process; Sign language; User research; Audition,Conference Paper,Final,Scopus,2-s2.0-85119302363,Peter,,
"Iijima R., Shitara A., Sarcar S., Ochiai Y.",57558816400;57220120132;36139109600;36454884800;,Word Cloud for Meeting: A Visualization System for DHH People in Online Meetings,2021,ASSETS 2021 - 23rd International ACM SIGACCESS Conference on Computers and Accessibility,,,3476547,,,,1.0,10.1145/3441852.3476547,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119285228&doi=10.1145%2f3441852.3476547&partnerID=40&md5=15f49263129b4e41b7ecdfdf8504769b,"Deaf and hard of hearing (DHH) people have limited access to auditory input, so they mainly receive visual information during online meetings. In recent years, the usability of a system that visualizes the ongoing topic in a conference has been confirmed, but it has not been verified in a remote conference that includes DHH people. One possible reason is that visual dispersion occurs when there are multiple sources of visual information. In this study, we introduce ""Word Cloud for Meeting,""a system that generates a separate word cloud for each participant and displays it in the background of each participant's video to visualize who is saying what. We conducted an experiment with seven DHH participants and obtained positive qualitative feedback on the ease of recognizing topic changes. However, when the topic changed in a sequence, it was found to be distracting. Additionally, we discuss the design implications for visualizing topics for DHH people in online meetings. © 2021 Owner/Author.",Deaf; Hard of Hearing; Hearing Loss; Online Meetings; Real-Time Visualization; User Study,Interactive computer systems; Visualization; Deaf; Hard of hearings; Hearing loss; Online meetings; Real time visualization; Remote conference; User study; Visual information; Visualization system; Word clouds; Audition,Conference Paper,Final,Scopus,2-s2.0-85119285228,Peter,,
"Dingman B., Tigwell G.W., Shinohara K.",57218761507;57191504112;16239695600;,Designing a Podcast Platform for Deaf and Hard of Hearing Users,2021,ASSETS 2021 - 23rd International ACM SIGACCESS Conference on Computers and Accessibility,,,3476523,,,,2.0,10.1145/3441852.3476523,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119263858&doi=10.1145%2f3441852.3476523&partnerID=40&md5=71ae45228f41ff1ba6b341f4e17183a1,"Listening to podcasts is a popular way for people to spend their time. However, little focus has been given to how accessible podcast platforms are for Deaf and Hard-of-Hearing (DHH) people. We present a DHH-centered accessible podcast platform prototype developed with user-centered design. Our proposed design was constructed through semi-structured interviews (n=7) and prototype design feedback sessions (n=8) with DHH users. We encourage podcast platform designers to adopt our design recommendations to make podcasts more inclusive for DHH people and recommend how podcast hosts can make their shows more accessible. © 2021 Owner/Author.",Deaf and Hard-of-Hearing; Design; Podcasts,User centered design; Deaf and hard-of-hearing; Design feedbacks; Design recommendations; Hard of hearings; Podcasts; Prototype designs; Semi structured interviews; Audition,Conference Paper,Final,Scopus,2-s2.0-85119263858,Peter,,
"Yip C., Chong J.M., Kwek S.Y., Wang Y., Hara K.",57344137000;57344137100;57343549800;57193775768;55498092800;,Visionary Caption: Improving the Accessibility of Presentation Slides through Highlighting Visualization,2021,ASSETS 2021 - 23rd International ACM SIGACCESS Conference on Computers and Accessibility,,,3476539,,,,,10.1145/3441852.3476539,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119255300&doi=10.1145%2f3441852.3476539&partnerID=40&md5=602a057b17823fbbcf45dd34464b1b2a,"Presentation slides are widely used in occasions such as academic talks and business meetings. Captions placed on slides support deaf and hard of hearing (DHH) people to understand spoken contents, but simultaneously comprehending and associating visual contents on slides and caption text could be challenging. In this paper, we design and develop a visualization technique to highlight and associate chart on a slide and numerical data in caption. We first conduct a small formative study with people with and without hearing impairments to assess the value of the visualization technique using a lo-fidelity video prototype. We then develop Visionary Caption, a visualization technique that uses natural language processing to automatically highlight visual content and numerical phrases, and show the association between them. We present a scenario and personas to showcase the potential utility of Visionary Caption and guide its future development. © 2021 Owner/Author.",Accessibility; deaf and hard of hearing; information visualization,Data visualization; Information systems; Natural language processing systems; Visual languages; Visualization; Business meetings; Caption texts; Deaf and hard of hearing; Hard of hearings; Hearing impairments; Numerical data; Potential utility; Presentation slides; Visual content; Visualization technique; Audition,Conference Paper,Final,Scopus,2-s2.0-85119255300,Peter,,
"Storer K.M., Branham S.M.",57192652259;35071095100;,Deinstitutionalizing Independence: Discourses of Disability and Housing in Accessible Computing,2021,ASSETS 2021 - 23rd International ACM SIGACCESS Conference on Computers and Accessibility,,,3471213,,,,3.0,10.1145/3441852.3471213,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119254269&doi=10.1145%2f3441852.3471213&partnerID=40&md5=39d8eb7db4fd3765cb88c642a252c273,"The meaning of ""homes""is complicated for disabled people because of the historical link between (de)institutionalization, housing, and civil rights. But, it is unclear whether and how this history impacts Accessible Computing (AC) research in domestic spaces. We performed Critical Discourse Analysis on 101 AC articles to explore how (de)institutionalization affects domestic AC research. We found (de)institutionalization motivates goals of ""independence""for disabled people. Yet, discourses of housing reflected institutional logics which are in tension with ""independence""-complicating how goals were set, housing was understood, and design was approached. We outline three discourses of housing in AC and identify parallels to those used to justify institutionalization in the USA. We reflect upon their consequences for AC research. We offer principles derived from the Independent Living Movement as frameworks for challenging institutional conceptions of housing, to open new avenues for more holistic and anti-ableist domestic AC research. © 2021 Owner/Author.",Accessibility; Critical Discourse Analysis; Disability; Homes; Housing; Institutionalization,Computation theory; Semantics; Civil rights; Computing research; Critical discourse analysis; Disability; Disabled people; Home; Independent living; Institutional logic; Institutionalisation; Housing,Conference Paper,Final,Scopus,2-s2.0-85119254269,Peter,,
"Remache-Vinueza B., Trujillo-León A., Zapata M., Sarmiento-Ortiz F., Vidal-Verdú F.",57218198076;56029085400;56818703100;57279194800;6602727727;,Audio-tactile rendering: A review on technology and methods to convey musical information through the sense of touch,2021,Sensors,21,19,6575,,,,6.0,10.3390/s21196575,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116054435&doi=10.3390%2fs21196575&partnerID=40&md5=9fb86be7fc3fd92b71d2f69dd5792878,"Tactile rendering has been implemented in digital musical instruments (DMIs) to offer the musician haptic feedback that enhances his/her music playing experience. Recently, this implementation has expanded to the development of sensory substitution systems known as haptic music players (HMPs) to give the opportunity of experiencing music through touch to the hearing impaired. These devices may also be conceived as vibrotactile music players to enrich music listening activities. In this review, technology and methods to render musical information by means of vibrotactile stimuli are systematically studied. The methodology used to find out relevant literature is first outlined, and a preliminary classification of musical haptics is proposed. A comparison between different technologies and methods for vibrotactile rendering is performed to later organize the information according to the type of HMP. Limitations and advantages are highlighted to find out opportunities for future research. Likewise, methods for music audio-tactile rendering (ATR) are analyzed and, finally, strategies to compose for the sense of touch are summarized. This review is intended for researchers in the fields of haptics, assistive technologies, music, psychology, and human–computer interaction as well as artists that may make use of it as a reference to develop upcoming research on HMPs and ATR. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Haptic music player; Human computer interaction; Musical haptic wearables; Musical haptics; Sensory substitution systems; Tactile rendering; Vibrotactile feedback; Vibrotactile music composition,Audio acoustics; Audition; Human computer interaction; Wearable technology; Haptic music player; Haptics; Music composition; Music players; Musical haptic; Musical haptic wearables; Sensory substitution; Sensory substitution system; Substitution systems; Tactile rendering; Vibro-tactile feedbacks; Vibrotactile; Vibrotactile music composition; Music; female; human; male; music; self help device; technology; touch; Female; Humans; Male; Music; Self-Help Devices; Technology; Touch; Touch Perception,Review,Final,Scopus,2-s2.0-85116054435,Peter,,
[No author name available],[No author id available],"Proceedings of the 21st International Conference on Human-Computer Interaction, INTERACCION 2021",2021,ACM International Conference Proceeding Series,,,,,,163.0,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113258397&partnerID=40&md5=18e99d62449cac66764ad3743becf603,"The proceedings contain 25 papers. The topics discussed include: analysis on the creation of accessible documents with Microsoft word online in the field of education; automatic captions on video calls, a must for the elderly. Using Mozilla DeepSpeech for the STT; towards the creation of accessible charts for the visually impaired; a machine learning based sign language interpretation system for communication with deaf-mute people; designing and evaluating a user interface for people with cognitive disabilities; 3D modeling and printing with vulnerable adults. A participant observational study with immigrants and low-literate older people; effectiveness of a mixed reality system in terms of social interaction behaviors in children with and without autism spectrum condition; and relevance of non-activity representation in traveling user behavior profiling for adaptive gamification.",,,Conference Review,Final,Scopus,2-s2.0-85113258397,Peter,,
"Kaimara P., Deliyannis I., Oikonomou A., Fokides E.",57211531309;7801457487;6701354402;56182186100;,Waking Up In the Morning (WUIM): A Smart Learning Environment for Students with Learning Difficulties,2021,Technologies,9,3,50,,,,,10.3390/technologies9030050,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147703306&doi=10.3390%2ftechnologies9030050&partnerID=40&md5=c2789c92f48006280bb0d0a07c84e159,"Effectiveness, efficiency, scalability, autonomy, engagement, flexibility, adaptiveness, personalization, conversationality, reflectiveness, innovation, and self-organization are some of the fundamental features of smart environments. Smart environments are considered a good learning practice for formal and informal education; however, it is important to point out the pedagogical approaches on which they are based. Smart learning environments (SLEs) underline the flexibility of eclectic pedagogy that places students at the center of any educational process and takes into account the diversity in classrooms. Thus, SLEs incorporate pedagogical principles derived from (1) traditional learning theories, e.g., behaviorism and constructivism, (2) contemporary pedagogical philosophy, e.g., differentiated teaching and universal design for learning, (3) theories that provide specific instructions for educational design, e.g., cognitive theory of multimedia learning and gamification of learning. The innovative concept of transmedia learning is an eclectic pedagogical approach, which in addition to learning principles, blends all available media so far. WUIM is a transmedia program for training independent living skills aimed primarily at children with learning disabilities, which emerged from the composition of pedagogical theories, traditional educational materials and cutting-edge technologies such as augmented and virtual reality, and art-based production methodologies. This paper outlines the development of WUIM, from the prototyping presented at the 4th International Conference in Creative Writing (2019) to the Alpha and Beta stages, including user and expert evaluations. © 2021 by the authors.",cognitive theory of multimedia learning; differentiated instruction; gamification of learning; inclusive education; independent living skills; smart learning environments; special educational needs; transmedia learning; universal design for learning,,Article,Final,Scopus,2-s2.0-85147703306,Peter,,
Strantz A.,57188571638;,Using Web Standards to Design Accessible Data Visualizations in Professional Communication,2021,IEEE Transactions on Professional Communication,64,3,9509359,288,301,,5.0,10.1109/TPC.2021.3091784,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113929107&doi=10.1109%2fTPC.2021.3091784&partnerID=40&md5=45e2d6e861e04ec45325e9c1eac0fbd2,"Introduction: Data visualization is a reliable tool for professional communication practitioners to synthesize and present data for a variety of audiences. However, data visualizations have a range of accessibility concerns, including visual acuity, color/contrast difficulties, color blindness, and size/scale issues. Data visualizations should therefore be designed following web standards for complex images to ensure that they are accessible to audiences with diverse needs. Key concepts: Drawing from work in professional communication and disability studies, practitioners recognize that users have varied accessibility needs. 'Universal design' as a guiding principle is less helpful than targeted approaches to design that reflect actual user needs. Such targeted approaches should follow web standards for accessible design because they enable interaction with newer accessibility technologies and put more control in the hands of users. Key lessons: Follow these best practices to create visually accessible data visualizations. 1. Design the visual for accessibility by using whitespace, creating contrast, maintaining size/scale, and labeling the visual clearly. 2. Implement the visual using web standards to create semantic connections between the visual and text for both users and accessibility technologies. This goal can be achieved with textual description, overview/data/presentation context, or ARIA semantic links. 3. Test the visual for accessibility through user tests and industry-standard tools. Implications for practice: Web standards provide a blueprint for designing accessible data visualizations for online spaces, but professional communicators should be aware of the coding expertise and necessary infrastructure needed to deploy these visuals. Nevertheless, with increasing use of public-facing data visualizations to convey information on global issues, such as COVID-19, the need for these visuals to be accessible to all audiences becomes paramount. © 1988-2012 IEEE.",Accessibility; computer code; data visualization; professional communication; web standards,Color vision; Professional aspects; Semantics; Visualization; Best practices; Guiding principles; Industry standards; Professional communication; Semantic link; Textual description; Universal Design; Web standards; Data visualization,Article,Final,Scopus,2-s2.0-85113929107,Peter,,
"Huang S., Lin W., Xu M., Wang R., Cai Z.G.",57206938991;57226687179;57226694338;56128487100;41661049700;,On the tip of the pen: Effects of character-level lexical variables and handwriter-level individual differences on orthographic retrieval difficulties in Chinese handwriting,2021,Quarterly Journal of Experimental Psychology,74,9,,1497,1511,,6.0,10.1177/17470218211004385,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104075723&doi=10.1177%2f17470218211004385&partnerID=40&md5=a40b004dc59fd9d149d06e36c2398bbc,"In the past few decades, Chinese speakers have suffered from difficulties in handwriting, which include tip-of-the-pen (TOP) states (knowing a character but failing to fully handwrite it) and character amnesia in general (a general inability to handwrite a character despite being able to recognise it). The current study presents a systematic empirical investigation of the effects of character-level lexical characteristics and handwriter-level individual differences on TOP, character amnesia, and partial orthographic access in TOP states. Using a spelling-to-dictation task, we had 64 participants to handwrite 200 simplified Chinese characters. We showed that, at the lexical level, participants experienced more TOP and character amnesia in handwriting if a character was less frequent, was acquired later in life, was embedded in a less familiar word, or had more strokes; TOP but not character amnesia was additionally affected by phonetic radical order and spelling regularity. At the handwriter level, people also experienced more TOP and character amnesia if they had more digital exposure, less pen exposure, or less print exposure. In a TOP state, partial orthographic access was more likely if a character was acquired later in life, had fewer strokes, or had a left-right or top-down composition or, if a handwriter had less digital exposure. © Experimental Psychology Society 2021.",character-level characteristics; Chinese character; handwriting; individual differences; orthography; tip-of-the-pen,China; handwriting; human; individuality; language; phonetics; China; Handwriting; Humans; Individuality; Language; Phonetics,Article,Final,Scopus,2-s2.0-85104075723,Peter,,
"Montagud M., Hurtado C., De Rus J.A., Fernández S.",35868074700;57235613200;57217211808;57189239868;,Subtitling 3D VR content with limited 6DoF: Presentation modes and guiding methods,2021,Applied Sciences (Switzerland),11,16,7472,,,,1.0,10.3390/app11167472,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113791251&doi=10.3390%2fapp11167472&partnerID=40&md5=79835a657d607b9096ee16017fd66734,"All multimedia services must be accessible. Accessibility for multimedia content is typically provided by means of access services, of which subtitling is likely the most widespread approach. To date, numerous recommendations and solutions for subtitling classical 2D audiovisual services have been proposed. Similarly, recent efforts have been devoted to devising adequate subtitling solutions for VR360 video content. This paper, for the first time, extends the existing approaches to address the challenges remaining for efficiently subtitling 3D Virtual Reality (VR) content by exploring two key requirements: presentation modes and guiding methods. By leveraging insights from earlier work on VR360 content, this paper proposes novel presentation modes and guiding methods, to not only provide the freedom to explore omnidirectional scenes, but also to address the additional specificities of 3D VR compared to VR360 content: depth, 6 Degrees of Freedom (6DoF), and viewing perspectives. The obtained results prove that always-visible subtitles and a novel proposed comic-style presentation mode are significantly more appropriate than state-of-the-art fixed-positioned subtitles, particularly in terms of immersion, ease and comfort of reading, and identification of speakers, when applied to professional pieces of content with limited displacement of speakers and limited 6DoF (i.e., users are not expected to navigate around the virtual environment). Similarly, even in such limited movement scenarios, the results show that the use of indicators (arrows), as a guiding method, is well received. Overall, the paper provides relevant insights and paves the way for efficiently subtitling 3D VR content. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Accessibility; Guiding methods; Immersive media; Subtitling; Virtual reality,,Article,Final,Scopus,2-s2.0-85113791251,Peter,,
"Azad M.T., Ahmadian M.",57267063400;37053495200;,Comparing the effect of morphological analysis and incidental learning on the acquisition of toefl vocabulary,2021,Mextesol Journal,45,3,,,,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115415890&partnerID=40&md5=fd28886cb567d09fdbd708fa36a8c0cf,"Morphological analysis and incidental learning are two vocabulary learning strategies that language learners may use in order to acquire the meanings of new words. To date, however, few studies have compared the effectiveness of these two strategies. Hence, the current study was carried out to compare the effect of morphological analysis and incidental learning of words that appeared in ten actual TOEFL reading passages from 2010 to 2018 (henceforth, TOEFL vocabulary/words). Eighty upper-intermediate Iranian foreign language learners participated in the study. A pretest was administered to determine the TOEFL words the participants could not supply a synonym, a similar expression, or a translation for at the onset of the study. Next, the participants were randomly divided into two groups: The Morphological Analysis (MA) group, who learned the vocabulary through learning the meanings of Greco-Latin roots in the words, and the Incidental Learning (IL) group, who learned the TOEFL vocabulary through multiple exposures to the words in different contexts. Then, a posttest was administered to investigate the effect of each of these strategies on recognizing the meanings of the TOEFL words. Running a t-test, the groups' mean scores on the pretest and posttest were compared. The results of the study indicated that the MA had a more significant effect on vocabulary retention than the IL. Based on the findings of the study, it is recommended that MA be employed as a valuable source for TOEFL vocabulary learning and teaching by both teachers and learners, and as a strategy for vocabulary teaching in ESP or EAP courses as the vocabulary in such courses is heavily loaded with Greek and Latin roots. © 2021 Asociacion Mexicana de Maestros de Ingles MEXTESOL A.C. All rights reserved.",,,Review,Final,Scopus,2-s2.0-85115415890,Peter,,
"Prince P., Paul B.T., Chen J., Le T., Lin V., Dimitrijevic A.",57225170803;57615608900;7501883246;57225170018;7005816469;57204253466;,Neural correlates of visual stimulus encoding and verbal working memory differ between cochlear implant users and normal-hearing controls,2021,European Journal of Neuroscience,54,3,,5016,5037,,5.0,10.1111/ejn.15365,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109357904&doi=10.1111%2fejn.15365&partnerID=40&md5=6e29b44e9533370ad222df3b594762dc,"A common concern for individuals with severe-to-profound hearing loss fitted with cochlear implants (CIs) is difficulty following conversations in noisy environments. Recent work has suggested that these difficulties are related to individual differences in brain function, including verbal working memory and the degree of cross-modal reorganization of auditory areas for visual processing. However, the neural basis for these relationships is not fully understood. Here, we investigated neural correlates of visual verbal working memory and sensory plasticity in 14 CI users and age-matched normal-hearing (NH) controls. While we recorded the high-density electroencephalogram (EEG), participants completed a modified Sternberg visual working memory task where sets of letters and numbers were presented visually and then recalled at a later time. Results suggested that CI users had comparable behavioural working memory performance compared with NH. However, CI users had more pronounced neural activity during visual stimulus encoding, including stronger visual-evoked activity in auditory and visual cortices, larger modulations of neural oscillations and increased frontotemporal connectivity. In contrast, during memory retention of the characters, CI users had descriptively weaker neural oscillations and significantly lower frontotemporal connectivity. We interpret the differences in neural correlates of visual stimulus processing in CI users through the lens of cross-modal and intramodal plasticity. © 2021 The Authors. European Journal of Neuroscience published by Federation of European Neuroscience Societies and John Wiley & Sons Ltd.",cochlear implant; connectivity; hearing loss; neural oscillations; verbal working memory; visual processing,"adult; aged; Article; auditory cortex; clinical article; comparative study; controlled study; electroencephalogram; female; functional connectivity; human; male; memory consolidation; middle aged; nerve potential; scoring system; sensorimotor function; Sternberg visual working memory task; verbal memory; visual cortex; visual evoked potential; visual memory; visual stimulation; working memory; young adult; auditory cortex; cochlea prosthesis; cochlear implantation; hearing; hearing impairment; short term memory; Auditory Cortex; Cochlear Implantation; Cochlear Implants; Deafness; Hearing; Humans; Memory, Short-Term",Article,Final,Scopus,2-s2.0-85109357904,Peter,,
"Mohsen M.A., Mahdi H.S.",39962126000;57219420451;,Partial versus full captioning mode to improve L2 vocabulary acquisition in a mobile-assisted language learning setting: words pronunciation domain,2021,Journal of Computing in Higher Education,33,2,,524,543,,3.0,10.1007/s12528-021-09276-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103661408&doi=10.1007%2fs12528-021-09276-0&partnerID=40&md5=e085542ccb02cda128a738e90dd30b3a,"Video captioning has been investigated extensively in the Computer-Assisted Language Learning (CALL) literature to aid second language vocabulary acquisition. However, a little is known about how video captioning could foster learners’ pronunciation, which is a component of second language vocabulary acquisition proposed by Nation (Nation, Learning vocabulary in another language, Cambridge University Press, 2001), when attending to video captioning. Therefore, this study aims to investigate the effect of two types of video captioning, namely, full versus partial captioning, on mastery of word pronunciation. Furthermore, we tested the magnitude of the cognitive load imposed by video captioning types using NASA TLX. A total of 55 Arab English as a Foreign Language learners watched videos with full, partial or no captioning. Their perceptions about learning with captioning were also surveyed. Results of the pre–post-tests indicated that the captioning groups’ performance in the pronunciation tests outscored the no captioning group. In turn, the partial captioning group’s scores were slightly higher than those of the full captioning group. However, this difference was statistically insignificant. Cognitive load was found higher in full captioning and no captioning than that in the partial captioning mode. The participants showed highly positive attitudes towards learning with captions. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Full captioning; Mobile-Assisted Language Learning (MALL); Partial captioning; Pronunciation; Vocabulary acquisition,,Article,Final,Scopus,2-s2.0-85103661408,Peter,,
"Hughes C.J., Montagud M.",57040431000;35868074700;,Accessibility in 360° video players,2021,Multimedia Tools and Applications,80,20,,30993,31020,,7.0,10.1007/s11042-020-10088-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093860751&doi=10.1007%2fs11042-020-10088-0&partnerID=40&md5=eec0297f4a75fdabc5349bdc651b1b48,"Accessibility is a key requirement for any multimedia tool and application. With the current trend towards immersive experiences, such as Virtual Reality (VR) and 360o video, it becomes key that these environments are adapted to be fully accessible. However, until recently the focus has been mostly on adapting the existing techniques to fit immersive displays, rather than considering new approaches for accessibility designed specifically for these increasingly relevant media experiences. This paper surveys a wide range of 360o video players and examines the features they include for dealing with accessibility, such as Subtitles, Audio Description, Sign Language, User Interfaces and other interaction features, like voice control and support for multi-screen scenarios. These features have been chosen based on guidelines from standardization contributions, like in the World Wide Web Consortium (W3C) and the International Communication Union (ITU), and from research contributions for making 360° video consumption experiences accessible. The in-depth analysis has been part of a research effort towards the development of a fully inclusive and accessible 360° video player. The paper concludes by discussing how the newly developed player has gone above and beyond the existing solutions and guidelines, by providing accessibility features that meet the expectations for a widely used immersive medium, like 360° video. © 2020, The Author(s).",360o video; Accessibility; Audio description; Immersive video; Sign language; Subtitling,User interfaces; Audio description; Immersive display; In-depth analysis; Interaction features; International communication; Multimedia tool; Research efforts; World wide web consortiums; Virtual reality,Article,Final,Scopus,2-s2.0-85093860751,Peter,,
"Bragg D., Caselli N., Hochgesang J.A., Huenerfauth M., Katz-Hernandez L., Koller O., Kushalnagar R., Vogler C., Ladner R.E.",43660991100;56286073000;56366807700;12240800100;57226481621;55140799000;36142036500;7005789370;7005099015;,The FATE Landscape of Sign Language AI Datasets: An Interdisciplinary Perspective,2021,ACM Transactions on Accessible Computing,14,2,7,,,,6.0,10.1145/3436996,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111620729&doi=10.1145%2f3436996&partnerID=40&md5=4fd0359263968bc5997a5e01acae68cd,"Sign language datasets are essential to developing many sign language technologies. In particular, datasets are required for training artificial intelligence (AI) and machine learning (ML) systems. Though the idea of using AI/ML for sign languages is not new, technology has now advanced to a point where developing such sign language technologies is becoming increasingly tractable. This critical juncture provides an opportunity to be thoughtful about an array of Fairness, Accountability, Transparency, and Ethics (FATE) considerations. Sign language datasets typically contain recordings of people signing, which is highly personal. The rights and responsibilities of the parties involved in data collection and storage are also complex and involve individual data contributors, data collectors or owners, and data users who may interact through a variety of exchange and access mechanisms. Deaf community members (and signers, more generally) are also central stakeholders in any end applications of sign language data. The centrality of sign language to deaf culture identity, coupled with a history of oppression, makes usage by technologists particularly sensitive. This piece presents many of these issues that characterize working with sign language AI datasets, based on the authors' experiences living, working, and studying in this space. © 2021 ACM.","Artificial intelligence (AI); Dataset; Fairness, accountability, and ethics (FATE); Machine learning (ML); Sign language; Transparency",Digital storage; History; Access mechanism; Culture identities; Data collection; Data collectors; Data contributors; Data users; Rights and responsibilities; Sign language; Artificial intelligence,Article,Final,Scopus,2-s2.0-85111620729,Peter,,
"Saunders B., Camgoz N.C., Bowden R.",57219740038;56565367100;7102165938;,Continuous 3D Multi-Channel Sign Language Production via Progressive Transformers and Mixture Density Networks,2021,International Journal of Computer Vision,129,7,,2113,2135,,13.0,10.1007/s11263-021-01457-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105522926&doi=10.1007%2fs11263-021-01457-9&partnerID=40&md5=fe54ea63ba5607ce90f0ba7fa836c573,"Sign languages are multi-channel visual languages, where signers use a continuous 3D space to communicate. Sign language production (SLP), the automatic translation from spoken to sign languages, must embody both the continuous articulation and full morphology of sign to be truly understandable by the Deaf community. Previous deep learning-based SLP works have produced only a concatenation of isolated signs focusing primarily on the manual features, leading to a robotic and non-expressive production. In this work, we propose a novel Progressive Transformer architecture, the first SLP model to translate from spoken language sentences to continuous 3D multi-channel sign pose sequences in an end-to-end manner. Our transformer network architecture introduces a counter decoding that enables variable length continuous sequence generation by tracking the production progress over time and predicting the end of sequence. We present extensive data augmentation techniques to reduce prediction drift, alongside an adversarial training regime and a mixture density network (MDN) formulation to produce realistic and expressive sign pose sequences. We propose a back translation evaluation mechanism for SLP, presenting benchmark quantitative results on the challenging PHOENIX14T dataset and setting baselines for future research. We further provide a user evaluation of our SLP model, to understand the Deaf reception of our sign pose productions. © 2021, The Author(s).",3D Multi-channel sign language; Continuous sequence generation; Sign language production,Deep learning; Mixtures; Network architecture; Visual languages; Automatic translation; Back translations; Continuous sequences; Data augmentation; Mixture density; Quantitative result; Spoken languages; User evaluations; Translation (languages),Article,Final,Scopus,2-s2.0-85105522926,Peter,,
"Ávila-Cabrera J.J., Corral Esteban A.",56096460600;55193798100;,The project SubESPSKills: Subtitling tasks for students of Business English to improve written production skills,2021,English for Specific Purposes,63,,,33,44,,8.0,10.1016/j.esp.2021.02.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103315583&doi=10.1016%2fj.esp.2021.02.004&partnerID=40&md5=9c8e9d6b65e6058a8619c55781ca94fb,"This study accounts for the results obtained from a teaching innovation project called SubESPSKills (Subtitling tasks in the English for Specific Purposes class to improve written production skills) with a control and experimental group of undergraduates taking a course on Business English. The study was conducted during the 2018–2019 academic year at the Universidad Complutense de Madrid, Spain. Among our main goals was the improvement of writing production skills in the English for Specific Purposes class. In order to do so, reverse subtitling was used as an audiovisual translation tool to enhance participants’ written skills in a course of Business English (Degree in Commerce). With the aim of using reverse subtitling as a tool for the practice of writing in English as a foreign language, the participants were required to submit a number of activities of written production on topics related to business and commerce. Additionally, they had to subtitle two videos from Spanish/Chinese into English, related to the aforementioned area of knowledge. Subsequently, a mixed method was followed since quantitative and qualitative data were gathered and analysed. The results presented here aim to prove the potential of active reverse subtitling as a tool for foreign language learning. © 2021 Elsevier Ltd",Business English; English for specific purposes; English written production; Foreign language learning; Reverse subtitling,,Article,Final,Scopus,2-s2.0-85103315583,Peter,,
"Gourlay A., Crabb M.",57226004785;56421925900;,Graphic novel subtitles: Requirement elicitation and system implementation,2021,IMX 2021 - Proceedings of the 2021 ACM International Conference on Interactive Media Experiences,,,,228,232,,,10.1145/3452918.3465489,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110234130&doi=10.1145%2f3452918.3465489&partnerID=40&md5=43d33a23ea15c047abe28ca0f4742716,"Consuming subtitled video content relies on a viewers ability to match up and understand a number of visual inputs simultaneously. This can create challenges in immersion due to the overall readability of subtitles and the speed at which they are presented. In this paper we introduce Graphic Novel Subtitles as an alternative media consumption method that is based on combining video keyframes with subtitle text to create a comic-type experience. We carry out a requirement elicitation survey with 34 participants in order to explore this concept in more detail and identify key features that we present as system requirements.We then introduce a system that can automatically generate a graphic novel from video and subtitle files, and discuss our future evaluation plans. © 2021 Owner/Author.",Accessibility; Graphic Novels; Subtitles; Survey,Human computer interaction; Requirements engineering; Evaluation plans; Key feature; Key-frames; Media consumption; Requirement elicitation; System implementation; Video contents; Surveys,Conference Paper,Final,Scopus,2-s2.0-85110234130,Peter,,
Zengin B.,36708953100;,How best to use audio/ subtitle combinations in the use of films for nonnative learners of english,2021,Design Solutions for Adaptive Hypermedia Listening Software,,,,45,61,,,10.4018/978-1-7998-7876-6.ch003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137684978&doi=10.4018%2f978-1-7998-7876-6.ch003&partnerID=40&md5=9eb158a219f5d72c33c3b6f7f42965b6,"Use of movies and TV shows for foreign language learning purposes pervades informal settings where viewers enjoy the multiplicity of soundtracks and of subtitles. However, this is not an integral part of formal language education policies. Hoping to inform policy making processes, this study aimed to investigate the most efficient use of audio/subtitle combinations for specific purposes. As a result, reversed subtitling was found to be effective for vocabulary learning whereas, generally, bimodal subtitling was considered to benefit accent-related problems. The participants self-reported preferrence for non-subtitled version in case of listening comprehension in general, which was due to the subtitle effect in the case of students at lower levels. Considering that mostly interlingual subtitled content is the only version in most platforms, these findings make it necessary to prefer a platform presenting a variety of alternative combinations in terms of soundtrack and subtitles. © 2021, IGI Global.",,,Book Chapter,Final,Scopus,2-s2.0-85137684978,Peter,,
"Falk J., Eksvard S., Schenkman B., Andren B., Brunnstrom K.",57238316100;57238316200;6603016133;25623009300;6603646979;,Legibility and readability in Augmented Reality,2021,"2021 13th International Conference on Quality of Multimedia Experience, QoMEX 2021",,,9465455,231,236,,,10.1109/QoMEX51781.2021.9465455,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113895564&doi=10.1109%2fQoMEX51781.2021.9465455&partnerID=40&md5=0a5c6a36c916e6c59ace03fc8a2a828f,"Digital technology offers multimodal presentation of information, that can be used for translating foreign languages or for alleviating hearing impairment or deafness communication problems. Today, there exists various aids that can be used for speech-to text translations, but there are some challenges with these. One potential solution for this is to make use of a combination of Augmented Reality (AR) and speech-to-text systems, where speech is converted into text that is then presented in AR-glasses. In AR, one crucial problem is the legibility and readability of text under different environmental conditions. Different types of AR glasses have different ergonomic characteristics, which implies that certain glasses might be more suitable for such a system than others. In this investigation, two different AR-glasses were evaluated based on among other things their optical, visual, and ergonomic characteristics. User tests were conducted to evaluate the legibility and readability of text under different environmental contexts. The results indicate that legibility and readability are affected by factors such as ambient illuminance, background properties and how the text is presented with respect to polarity, opacity, size, and number of lines. The characteristics of the glasses impacts the user experience, but which glasses that is preferred depends on the individual. © 2021 IEEE.",AR; Augmented Reality; legibility; readability,Audition; Augmented reality; Ergonomics; Multimedia systems; User experience; Ambient illuminance; Communication problems; Digital technologies; Environmental conditions; Environmental contexts; Foreign language; Hearing impairments; Speech-to-text system; Glass,Conference Paper,Final,Scopus,2-s2.0-85113895564,Peter,,
"Jin Y., Gao Y., Zhu Y., Wang W., Li J., Choi S., Li Z., Chauhan J., Dey A.K., Jin Z.",57219372304;57196005338;57219734109;57196023330;57219372136;57219373092;57209686510;57220427184;7101701731;55251775400;,SonicASL: An Acoustic-based Sign Language Gesture Recognizer Using Earphones,2021,"Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",5,2,3463519,,,,7.0,10.1145/3463519,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108889539&doi=10.1145%2f3463519&partnerID=40&md5=cbb36a4744bdd6fc51f59f9ba0530adc,"We propose SonicASL, a real-time gesture recognition system that can recognize sign language gestures on the fly, leveraging front-facing microphones and speakers added to commodity earphones worn by someone facing the person making the gestures. In a user study (N=8), we evaluate the recognition performance of various sign language gestures at both the word and sentence levels. Given 42 frequently used individual words and 30 meaningful sentences, SonicASL can achieve an accuracy of 93.8% and 90.6% for word-level and sentence-level recognition, respectively. The proposed system is tested in two real-world scenarios: indoor (apartment, office, and corridor) and outdoor (sidewalk) environments with pedestrians walking nearby. The results show that our system can provide users with an effective gesture recognition tool with high reliability against environmental factors such as ambient noises and nearby pedestrians. © 2021 ACM.",Acoustic sensing; earphones; sign language gesture recognition,Earphones; Facings; Ambient noise; Environmental factors; High reliability; On the flies; Real time gesture recognition system; Real-world scenario; Sentence level; Sign language; Gesture recognition,Article,Final,Scopus,2-s2.0-85108889539,Peter,,
"Sarkar A., Rintel S., Borowiec D., Bergmann R., Gillett S., Bragg D., Baym N., Sellen A.",56369758900;6507844674;57219256555;57223395713;57225429356;43660991100;6506795882;7004040846;,The promise and peril of parallel chat in video meetings for work,2021,Conference on Human Factors in Computing Systems - Proceedings,,,,,,,13.0,10.1145/3411763.3451793,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105828223&doi=10.1145%2f3411763.3451793&partnerID=40&md5=bf03276ace90bc716215f363ad4c5a1c,"We report the opportunities and challenges of parallel chat in work-related video meetings, drawing on a study of Microsoft employees' remote meeting experiences during the COVID-19 pandemic. We find that parallel chat allows groups to communicate flexibly without interrupting the main conversation, coordinate action around shared resources, and also improves inclusivity. On the other hand, parallel chat can also be distracting, overwhelming, and cause information asymmetries. Further, we find that whether an individual views parallel chat as a net positive in meetings is subject to the complex interactions between meeting type, personal habits, and intentional group practices. We suggest opportunities for tools and practices to capitalise on the strengths of parallel chat and mitigate its weaknesses. © 2021 ACM.",accessibility; cscw; diary; meetings; parallel chat; poll; survey; videoconferencing,Software engineering; Information asymmetry; MicroSoft; Shared resources; Tools and practices; Video meeting; Work-related; Human engineering,Conference Paper,Final,Scopus,2-s2.0-85105828223,Peter,,
"McHugh T.B., Saha A., Bar-El D., Worsley M., Piper A.M.",57220116751;57225767525;57189035060;37012933400;15023169400;,Towards Inclusive Streaming: Building Multimodal Music Experiences for the Deaf and Hard of Hearing,2021,Conference on Human Factors in Computing Systems - Proceedings,,,,,,,,10.1145/3411763.3451690,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105800514&doi=10.1145%2f3411763.3451690&partnerID=40&md5=4044c44adc32d1704dd89f31dd5caed5,"As online streaming becomes a primary method for music consumption, the various modalities that many people with hearing loss rely on for their enjoyment need to be supported. While visual or tactile representations can be used to experience music in a live event or from a recording, DRM anti-piracy encryption restricts access to audio data needed to create these multimodal experiences for music streaming. We introduce BufferBeats, a toolkit for building multimodal music streaming experiences. To explore the flexibility of the toolkit and to exhibit its potential use cases, we introduce and reflect upon building a collection of technical demonstrations that bring previous and new multimodal music experiences to streaming. Grounding our work in critical theories on design, making, and disability, as well as experiences from a small group of community partners, we argue that support for multimodal music streaming experiences will not only be more inclusive to the deaf and hard of hearing, but it will also empower researchers and hobbyist makers to use streaming as a platform to build creative new representations of music. © 2021 ACM.",accessibility; deaf and hard of hearing; music streaming; toolkit,Audition; Computation theory; Cryptography; Human engineering; Anti-piracy; Audio data; Hard of hearings; Hearing loss; Multi-modal; Music streaming; Audio acoustics,Conference Paper,Final,Scopus,2-s2.0-85105800514,Peter,,
"Mack K., McDonnell E.",57219109957;57222060653;,What do we mean by accessibility research? a literature survey of accessibility papers in chi and assets from 1994 to 2019,2021,Conference on Human Factors in Computing Systems - Proceedings,,,,,,,42.0,10.1145/3411764.3445412,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106760675&doi=10.1145%2f3411764.3445412&partnerID=40&md5=264b34a5d34ab6be4dec29538449c6a2,"Accessibility research has grown substantially in the past few decades, yet there has been no literature review of the field. To understand current and historical trends, we created and analyzed a dataset of accessibility papers appearing at CHI and ASSETS since ASSETS' founding in 1994. We qualitatively coded areas of focus and methodological decisions for the past 10 years (20102019, JV=506 papers), and analyzed paper counts and keywords over the full 26 years (A/=836 papers). Our findings highlight areas that have received disproportionate attention and those that are underserved-for example, over 43% of papers in the past 10 years are on accessibility for blind and low vision people. We also capture common study characteristics, such as the roles of disabled and nondisabled participants as well as sample sizes (e.g., a median of 13 for participant groups with disabilities and older adults). We close by critically reflecting on gaps in the literature and offering guidance for future work in the field. © 2021 ACM.",Accessibility; Assistive technology; Disability; Literature review,Human engineering; Historical trends; Literature reviews; Literature survey; Low vision; Older adults; Sample sizes; Paper,Conference Paper,Final,Scopus,2-s2.0-85106760675,Peter,,
"Peng Y.-H., Jang J., Bigham J.P., Pavel A.",57222988295;57222980792;16238221500;56121790600;,Say it all: Feedback for improving non-visual presentation accessibility,2021,Conference on Human Factors in Computing Systems - Proceedings,,,,,,,8.0,10.1145/3411764.3445572,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106759786&doi=10.1145%2f3411764.3445572&partnerID=40&md5=9d8ec44d083d5ea1b92b2d78e77a7e39,"Presenters commonly use slides as visual aids for informative talks. When presenters fail to verbally describe the content on their slides, blind and visually impaired audience members lose access to necessary content, making the presentation difcult to follow. Our analysis of 90 presentation videos revealed that 72% of 610 visual elements (e.g., images, text) were insufciently described. To help presenters create accessible presentations, we introduce Presentation A11y, a system that provides real-time and post-presentation accessibility feedback. Our system analyzes visual elements on the slide and the transcript of the verbal presentation to provide element-level feedback on what visual content needs to be further described or even removed. Presenters using our system with their own slide-based presentations described more of the content on their slides, and identifed 3.26 times more accessibility problems to fx after the talk than when using a traditional slide-based presentation interface. Integrating accessibility feedback into content creation tools will improve the accessibility of informational content for all. © 2021 ACM.",Accessibility; Audio description; Presentation; Slides; Video,Software engineering; Accessibility problems; Blind and visually impaired; Content creation; Element level; Non visuals; Slide-based; Visual content; Visual elements; Human engineering,Conference Paper,Final,Scopus,2-s2.0-85106759786,Peter,,
"Kim D.H., Setlur V., Agrawala M.",57204734384;21743848500;57204250599;,Towards understanding how readers integrate charts and captions: A case study with line charts,2021,Conference on Human Factors in Computing Systems - Proceedings,,,,,,,9.0,10.1145/3411764.3445443,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106720355&doi=10.1145%2f3411764.3445443&partnerID=40&md5=3d7947e4113ab235db841714e22961d7,"Charts often contain visually prominent features that draw atten-tion to aspects of the data and include text captions that emphasize aspects of the data. Through a crowdsourced study, we explore how readers gather takeaways when considering charts and captions to-gether.We frst ask participants to mark visually prominent regions in a set of line charts. We then generate text captions based on the prominent features and ask participants to report their takeaways after observing chart-caption pairs. We fnd that when both the chart and caption describe a high-prominence feature, readers treat the doubly emphasized high-prominence feature as the takeaway; when the caption describes a low-prominence chart feature, read-ers rely on the chart and report a higher-prominence feature as the takeaway. We also fnd that external information that provides context, helps further convey the caption's message to the reader. We use these fndings to provide guidelines for authoring efective chart-caption pairs. © 2021 ACM.",Captions; Line charts; Takeaways; Visually prominent features,Data visualization; Human engineering; External informations; Prominent features; Prominent region; Set of lines; Graphic methods,Conference Paper,Final,Scopus,2-s2.0-85106720355,Peter,,
"Alonzo O., Trussell J., Dingman B., Huenerfauth M.",57215303209;7006607897;57218761507;12240800100;,Comparison of methods for evaluating complexity of simplified texts among deaf and hard-of-hearing adults at diferent literacy levels,2021,Conference on Human Factors in Computing Systems - Proceedings,,,,,,,6.0,10.1145/3411764.3445038,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106714901&doi=10.1145%2f3411764.3445038&partnerID=40&md5=f6964234c6cf699a2379e99dcfef7650,"Research has explored using Automatic Text Simplifcation for reading assistance, with prior work identifying benefts and interests from Deaf and Hard-of-Hearing (DHH) adults. While the evaluation of these technologies remains a crucial aspect of research in the area, researchers lack guidance in terms of how to evaluate text complexity with DHH readers. Thus, in this work we conduct methodological research to evaluate metrics identifed from prior work (including reading speed, comprehension questions, and subjective judgements of understandability and readability) in terms of their efectiveness for evaluating texts modifed to be at various complexity levels with DHH adults at diferent literacy levels. Subjective metrics and low-linguistic-complexity comprehension questions distinguished certain text complexity levels with participants with lower literacy. Among participants with higher literacy, only subjective judgements of text readability distinguished certain text complexity levels. For all metrics, participants with higher literacy scored higher or provided more positive subjective judgements overall. © 2021 ACM.",Accessibility; Automatic text simplifcation; Deaf and hard-of-hearing; Methodological research,Human engineering; Comparison of methods; Complexity levels; Hard of hearings; Linguistic complexity; Methodological research; Reading speed; Subjective judgement; Understandability; Audition,Conference Paper,Final,Scopus,2-s2.0-85106714901,Peter,,
"Gorman B.M., Crabb M., Armstrong M.",56421738300;56421925900;57040522200;,Adaptive subtitles: Preferences and trade-ofs in real-time media adaption,2021,Conference on Human Factors in Computing Systems - Proceedings,,,,,,,5.0,10.1145/3411764.3445509,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106710309&doi=10.1145%2f3411764.3445509&partnerID=40&md5=38bb94332e663d47f07bfab4714ddbb3,"Subtitles can help improve the understanding of media content. People enable subtitles based on individual characteristics (e.g., lan-guage or hearing ability), viewing environment, or media context (e.g., drama, quiz show). However, some people fnd that subtitles can be distracting and that they negatively impact their viewing ex-perience.We explore the challenges and opportunities surrounding interaction with real-time personalisation of subtitled content. To understand how people currently interact with subtitles, we frst conducted an online questionnaire with 102 participants. We used our fndings to elicit requirements for a new approach called Adap-tive Subtitles that allows the viewer to alter which speakers have subtitles displayed in real-time.We evaluated our approach with 19 participants to understand the interaction trade-ofs and challenges within real-time adaptations of subtitled media. Our evaluation fndings suggest that granular controls and structured onboarding allow viewers to make informed trade-ofs when adapting media content, leading to improved viewing experiences. © 2021 ACM.",Adaptive-interfaces; Captions; Closed-captions; Media; Subtitles,Audition; Electronic assessment; Human engineering; Granular control; Individual characteristics; Media content; New approaches; Onboarding; Online questionnaire; Personalisation; Real-time adaptation; Commerce,Conference Paper,Final,Scopus,2-s2.0-85106710309,Peter,,
"Alshawabkeh A.A., Woolsey M.L., Kharbat F.F.",24823746600;6602732509;19639814600;,Using online information technology for deaf students during COVID-19: A closer look from experience,2021,Heliyon,7,5,e06915,,,,12.0,10.1016/j.heliyon.2021.e06915,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105306177&doi=10.1016%2fj.heliyon.2021.e06915&partnerID=40&md5=316e3560ed6659776fd01ca316d595dc,"The COVID-19 pandemic has interrupted the education of millions of students across the world. The purpose of this study was to investigate the perceptions regarding the technological instruction and accommodations provided to deaf students in online distance learning during the COVID-19 pandemic. This study was qualitative in nature and used anonymous, one-to-one semi-structured interviews. In June 2020, we interviewed a convenience sample of deaf students (n = 15) and their instructors (n = 3) and analysed the responses thematically. Upon achieving theme saturation, the thematic structure analysis was finalised. The results revealed five main themes related to deaf students’ experience with online distance learning during COVID-19. The themes are as follows: course content delivered, technology used, delivery method, assessment tools used, and social interactions. Each theme is discussed and compared with the related literature to scientifically encapsulate its suggested dimensions. The interviewed students described their experience of using online technology in both negative and positive terms. Instructors also provided their input to express their experiences during that time. Online distance learning was described as a difficult and challenging experience that lacked efficient communication channels and failed to address the needs of the deaf with respect to the communication medium. The typical course delivery methods were described as challenging, and the lack of social interaction was highlighted as a liability. At the same time, participants acknowledged some ancillary benefits of online distance learning especially that it enhanced their technology skills and their competences in adapting to a new environment. © 2021 The Authors",COVID-19; Deaf students; E-learning; Higher education; Online distance learning,,Article,Final,Scopus,2-s2.0-85105306177,Peter,,
"Glasser A., Garcia J., Hwang C., Vogler C., Kushalnagar R.",57195128152;57224318462;57224316489;7005789370;36142036500;,Effect of caption width on the TV user experience by deaf and hard of hearing viewers,2021,"Proceedings of the 18th International Web for All Conference, W4A 2021",,,3452435,,,,,10.1145/3430263.3452435,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107368257&doi=10.1145%2f3430263.3452435&partnerID=40&md5=bbcea5789cc12b974d6863942a738f7c,"Deaf and hard of hearing (DHH) viewers watch multimedia with captions on devices with widely varying widths. We investigated the impact of caption width on viewers' preferences. Previous research has shown that presenting one word lines allows viewers to read much more quickly than traditional reading, while others have shown that the optimal width for captions is 6 words per line. Our study showed that DHH viewers had no preference difference between 6 and 12 word lines. Furthermore, they significantly preferred 6 and 12 word lines over single word lines due to the need to split attention between the captions and video. © 2021 ACM.",Accessibility guidelines; Captions; Deaf; Hard of hearing; Subtitles,User experience; Hard of hearings; Single words; Split attentions; Audition,Conference Paper,Final,Scopus,2-s2.0-85107368257,Peter,,
"Amin A.A., Hassan S., Huenerfauth M.",56623925700;57224309524;12240800100;,Caption-occlusion severity judgments across live-television genres from deaf and hard-of-hearing viewers,2021,"Proceedings of the 18th International Web for All Conference, W4A 2021",,,3452429,,,,3.0,10.1145/3430263.3452429,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107360130&doi=10.1145%2f3430263.3452429&partnerID=40&md5=964c98bfb277ee3595a8fc3461326cb5,"Prior work has revealed that Deaf and Hard of Hearing (DHH) viewers are concerned about captions occluding other onscreen content, e.g. text or faces, especially for live television programming, for which captions are generally not manually placed. To support evaluation or placement of captions for several genres of live television, empirical evidence is needed on how DHH viewers prioritize onscreen information, and whether this varies by genre. Nineteen DHH participants rated the importance of various onscreen content regions across 6 genres: News, Interviews, Emergency Announcements, Political Debates, Weather News, and Sports. Importance of content regions varied significantly across several genres, motivating genre-specific caption placement. We also demonstrate how the dataset informs creation of importance-weights for a metric to predict the severity of captions occluding onscreen content. This metric correlated significantly better to 23 DHH participants' judgements of caption quality, compared to a metric with uniform importance-weights of content regions. © 2021 ACM.",Accessibility; Caption; Dataset; Genre; Metric,Hard of hearings; Importance weights; Live television; Political debates; Audition,Conference Paper,Final,Scopus,2-s2.0-85107360130,Peter,,
"Seita M., Andrew S., Huenerfauth M.",57192541826;57220117831;12240800100;,Deaf and hard-of-hearing users' preferences for hearing speakers' behavior during technology-mediated in-person and remote conversations,2021,"Proceedings of the 18th International Web for All Conference, W4A 2021",,,3452430,,,,6.0,10.1145/3430263.3452430,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107345790&doi=10.1145%2f3430263.3452430&partnerID=40&md5=d18d2cfad2c2ff2414c1535e593009dc,"Various technologies mediate synchronous audio-visual one-on-one communication (SAVOC) between Deaf and Hard-of-Hearing (DHH) and hearing colleagues, including automatic-captioning smartphone apps for in-person settings, or text-chat features of videoconferencing software in remote settings. Speech and non-verbal behaviors of hearing speakers, e.g. speaking too quietly, can make SAVOC difficult for DHH users, but prior work had not examined technology-mediated contexts. In an in-person study (N=20) with an automatic captioning smartphone app, variations in a hearing actor's enunciation and intonation dynamics affected DHH users' satisfaction. In a remote study (N=23) using a videoconferencing platform with text chat, variations in speech rate, voice intensity, enunciation, intonation dynamics, and eye contact led to such differences. This work contributes empirical evidence that specific behaviors of hearing speakers affect the accessibility of technology-mediated SAVOC for DHH users, providing motivation for future work on detecting or encouraging useful communication behaviors among hearing individuals. © 2021 ACM.",Accessibility; Automatic speech recognition; Conversational behaviors; COVID-19; Deaf; Hard of hearing; Videoconferencing,Smartphones; Video conferencing; Communication behavior; Hard of hearings; Nonverbal behavior; Remote settings; Smartphone apps; Speech rates; Users' satisfactions; Various technologies; Audition,Conference Paper,Final,Scopus,2-s2.0-85107345790,Peter,,
"Park J.S., Bragg D., Kamar E., Morris M.R.",57222403758;43660991100;23009039400;8619759600;,Designing an online infrastructure for collecting AI data from people with disabilities,2021,"FAccT 2021 - Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",,,3445870,52,63,,9.0,10.1145/3442188.3445870,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102622043&doi=10.1145%2f3442188.3445870&partnerID=40&md5=6f6d2512a2efe5de5aade9a3bd0f7fe2,"AI technology offers opportunities to expand virtual and physical access for people with disabilities. However, an important part of bringing these opportunities to fruition is ensuring that upcoming AI technology works well for people with a wide range of abilities. In this paper, we identify the lack of data from disabled populations as one of the challenges to training and benchmarking fair and inclusive AI systems. As a potential solution, we envision an online infrastructure that can enable large-scale, remote data contributions from disability communities. We investigate the motivations, concerns, and challenges that people with disabilities might experience when asked to collect and upload various forms of AI-relevant data through a semi-structured interview and an online survey that simulated a data contribution process by collecting example data files through an online portal. Based on our findings, we outline design guidelines for developers creating online infrastructures for gathering data from people with disabilities. © 2021 ACM.",Accessibility; AI FATE; Datasets; Disability; Inclusion; Representation,Artificial intelligence; Behavioral research; Population statistics; Transparency; AI systems; AI Technologies; Data files; Online surveys; People with disabilities; Remote data; Semi structured interviews; Data acquisition,Conference Paper,Final,Scopus,2-s2.0-85102622043,Peter,,
"Miller S., Wolfe J., Duke M., Schafer E., Agrawal S., Koch D., Neumann S.",36131713600;18438883100;56884055300;7102884473;13807227000;7201689978;56448173700;,Benefits of Bilateral Hearing on the Telephone for Cochlear Implant Recipients,2021,Journal of the American Academy of Audiology,32,3,20061,180,185,,,10.1055/s-0041-1722982,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104757502&doi=10.1055%2fs-0041-1722982&partnerID=40&md5=159b254dcb77a7cbe553884abe26c7ca,"Background  Cochlear implant (CI) recipients frequently experience difficulty understanding speech over the telephone and rely on hearing assistive technology (HAT) to improve performance. Bilateral inter-processor audio streaming technology using nearfield magnetic induction is an advanced technology incorporated within a hearing aid or CI processor that can deliver telephone audio signals captured at one sound processor to the sound processor at the opposite ear. To date, limited data exist examining the efficacy of this technology in CI users to improve speech understanding on the telephone. Purpose  The primary objective of this study was to examine telephone speech recognition outcomes in bilateral CI recipients in a bilateral inter-processor audio streaming condition (DuoPhone) compared with a monaural condition (i.e., telephone listening with one sound processor) in quiet and in background noise. Outcomes in the monaural and bilateral conditions using either a telecoil or T-Mic2 technology were also assessed. The secondary aim was to examine how deactivating microphone input in the contralateral processor in the bilateral wireless streaming conditions, and thereby modifying the signal-to-noise ratio, affected speech recognition in noise. Research Design  A repeated-measures design was used to evaluate speech recognition performance in quiet and competing noise with the telephone signal transmitted acoustically or via the telecoil to the ipsilateral sound processor microphone in monaural and bilateral wireless streaming listening conditions. Study Sample  Nine bilateral CI users with Advanced Bionics HiRes 90K and/or CII devices were included in the study. Data Collection and Analysis  The effects of phone input (monaural [DuoPhone Off] vs. bilateral [DuoPhone on]) and processor input (T-Mic2 vs. telecoil) on word recognition in quiet and noise were assessed using separate repeated-measures analysis of variance. Effect of the contralateral device mic deactivation on speech recognition outcomes for the T-Mic2 DuoPhone conditions was assessed using paired Student's t -tests. Results  Telephone speech recognition was significantly better in the bilateral inter-processor streaming conditions relative to the monaural conditions in both quiet and noise. Speech recognition outcomes were similar in quiet and noise when using the T-Mic2 and telecoil in the monaural and bilateral conditions. For the acoustic DuoPhone conditions using the T-Mic2, speech recognition in noise was significantly better when the microphone of the contralateral processor was disabled. Conclusion  Inter-processor audio streaming allows for bilateral listening on the telephone and produces better speech recognition in quiet and in noise compared with monaural listening conditions for adult CI recipients. © 2021. American Academy of Audiology. All rights reserved.",cochlear implant; noise; speech recognition; telephone,adult; aged; Article; assistive technology; clinical article; cochlear implantation; female; hearing; human; male; noise; outcome assessment; signal noise ratio; speech discrimination; task performance; word recognition; cochlea prosthesis; hearing; speech perception; telephone; Adult; Cochlear Implantation; Cochlear Implants; Hearing; Humans; Speech Perception; Telephone,Article,Final,Scopus,2-s2.0-85104757502,Peter,,
"Wakatsuki D., Arai T., Shionome T.",57499446000;7404175394;55308264400;,Hybrid Caption including Formula or Figure for Deaf and Hard-of-Hearing Students,2021,Journal of Advanced Computational Intelligence and Intelligent Informatics,25,2,,187,194,,,10.20965/jaciii.2021.p0187,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104482852&doi=10.20965%2fjaciii.2021.p0187&partnerID=40&md5=f6ed95e0c0c55d5c3760747eacbc60b0,"Captions, which are used as a means of information support for deaf and hard-of-hearing students, are usually presented only in text form. Therefore, when mathematical equations and figures are frequently used in a classroom, the captions will show several demonstrative words such as ""this equation""or ""that figure.""As there is a delay between the teacher's utterance and the display of the captions sometimes, it is difficult for users to grasp the target of these demonstrative words accurately. In this study, we prepared hybrid captions with mathematical equations and figures and verified their effectiveness via a comparison with conventional text-only captions. The results suggested that the hybrid captions were at least as effective as the conventional captions at helping the students understand the lesson contents. A subjective evaluation with a questionnaire survey also showed that the experimental participants found the hybrid captions to be acceptable without any discomfort. Furthermore, there was no difference in the number of eye movements of the participants during the experiment, suggesting that the physical load was similar for both types of captions. © 2021 Fuji Technology Press. All rights reserved.",deaf and hard-of-hearing; equation; figure; information support; text interpretation,Eye movements; Students; Surveys; Hard of hearings; Information support; Mathematical equations; Physical loads; Questionnaire surveys; Subjective evaluations; Audition,Article,Final,Scopus,2-s2.0-85104482852,Peter,,
"Vulchanova M., Lervåg I.K.",26432360500;57221936498;,Role of subtitles in L2 acquisition and comprehension: A pilot study of hearing-impaired students,2021,Languages,6,1,17,1,12,,,10.3390/languages6010017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100680415&doi=10.3390%2flanguages6010017&partnerID=40&md5=da278a6ab9e39b6d75bf54bb774bf5ff,"The purpose of this study was to investigate whether subtitles can facilitate language processing in English as a second language (L2) and, if so, which subtitles would be more beneficial for hard-of-hearing students with Norwegian as their first language. In total, 14 advanced learners of L2 English were recruited and tested on English comprehension and target vocabulary items based on video material provided with subtitles in English or Norwegian in comparison to no subtitles (control condition). Subtitles aided comprehension of the plot, tested immediately after clip presentation, with an advantage for English subtitles over Norwegian subtitles and no subtitles. Furthermore, subtitles were found to enhance the performance of the participants with moderate hearing loss more than they did for mild hearing loss participants. The inclusion of English subtitles only marginally enhanced vocabulary understanding for both mild and moderate hearing loss students. The findings of this study can be transferred to classrooms and may supplement other methods of adjusting the academic environment, in order to meet the need of students with hearing loss. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Comprehension; Hearing loss; Language development; Second language acquisition; Subtitles,,Article,Final,Scopus,2-s2.0-85100680415,Peter,,
"Mehdizadkhani M., Khoshsaligheh M.",57194062128;56946904400;,Insertion or voice-off in rendition of graphic codes: an experiment in Persian dubbing,2021,Visual Communication,20,1,,81,99,,6.0,10.1177/1470357219838599,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063522620&doi=10.1177%2f1470357219838599&partnerID=40&md5=700fb8d6380ba9aa13eb7f93e07d71d0,"Concerning graphic codes, there are currently three methods for their rendition in dubbing: audio-rendition of the target graphic codes (TGC) with an actor’s voice-off and adding the voice-off when the close shot is on the original graphic code (OGC); subtitling the OGC; and editing and inserting new TGCs replacing the OGCs in the film. The purpose of the current mixed-methods study was to compare the efficiency of voice-off versus insertion, two methods of rendition of graphic codes in dubbing, in terms of viewers’ comprehension and information processing. To this end, the Persian-dubbed versions of the Sherlock TV series broadcast on BBC Persian, using the insertion method, and on Iranian official television, using voice-off, were selected. The findings of the experiment revealed that the participants experienced better information recall and comprehension of the content of the graphic codes when they were translated using the insertion method. In the qualitative phase, the retrospective interview data involved issues in terms of the genre of audiovisual material, dialogue interaction, types of graphic codes, target language cultural references, literacy and viewing ability of the audience as relevant variables in the preference of each of the two methods. © The Author(s) 2019.",audiovisual translation; dubbing; graphic codes; insertion; voice-off,,Article,Final,Scopus,2-s2.0-85063522620,Peter,,
"Reichle E.D., Yu L., Liao S., Kruger J.-L.",6602766587;56336527600;57203968644;9277428700;,Using Simulations to Understand the Reading of Rapidly Displayed Subtitles,2021,"Proceedings of the 43rd Annual Meeting of the Cognitive Science Society: Comparative Cognition: Animal Minds, CogSci 2021",,,,445,451,,1.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139421854&partnerID=40&md5=e6d14b3a7075503ad0f2ed91a9e4b2f8,"Liao et al. (2020) reported an eye-movement experiment in which subtitles were displayed at three different rates, with a key finding being that, with increasing speeds, participants made fewer, shorter fixations and longer saccades. To understand why these eye-movement behaviors might be adaptive, we completed simulations using the E-Z Reader model (Reichle et al., 2012) to examine how subtitle speed might affect word identification and sentence comprehension, as well as the efficacy of six possible compensatory reading strategies. These simulations suggest that the imposition of a lexical-processing deadline and/or strategy of skipping short words may support reading comprehension in impoverished conditions. © Cognitive Science Society: Comparative Cognition: Animal Minds, CogSci 2021.All rights reserved.",E-Z Reader; eye-movement control; reading; strategies; subtitles,Eye movements; Condition; E-Z reader; Eye movement control; Movement behaviour; Reading; Reading comprehension; Reading strategies; Strategy; Subtitle; Word identification; Cognitive systems,Conference Paper,Final,Scopus,2-s2.0-85139421854,Peter,,
"Lee G.-B., Jang H., Jeong H., Woo W.",57488294600;57488728400;57488403200;35575439600;,Designing a Multi-Modal Communication System for the Deaf and Hard-of-Hearing Users,2021,"Proceedings - 2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct, ISMAR-Adjunct 2021",,,,429,434,,,10.1109/ISMAR-Adjunct54149.2021.00097,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126355249&doi=10.1109%2fISMAR-Adjunct54149.2021.00097&partnerID=40&md5=0466f1d0013f5c4b9535a9e5d3f32089,"In remote collaboration using Augmented Reality (AR), speech and gesture are major communication methods for the general public. However, the Deaf and Hard-of-Hearing (DHH) population cannot join in the communication due to the absence of a sign language interface which is their primary language. Recent works have tried to augment spoken language with sign language animations or captions, but the research to convey sign language with spoken language is still very limited. In this paper, we propose a novel multi-modal communication system that integrates sign language translation, speech recognition, and shared object manipulation in the mobile AR environment. Though the system is currently under development, we demonstrated a rapid prototype of the telemedicine app leveraging the video prototyping method to integrate the system modules. We performed preliminary interviews about our approach with DHH users, a sign language interpreter, and a physician. We discuss the insights into the future design of the DHH communication support in the AR collaboration system. This study has a socio-cultural, economic impact on the DHH population as a barrier-free design of a remote collaboration system in a practical scenario. Another contribution of this work is that we suggested a novel user-centered system for DHH users in AR by integrating the existing technologies. © 2021 IEEE.",Human-centered computing-Human computer interaction (HCI); Interaction design; Interaction design process and methods-User centered design; Interaction paradigms; Mixed / augmented reality; Human-centered computing,Audition; Augmented reality; Speech communication; Speech recognition; User centered design; Design method; Design-process; Human-centered computing; Human-centered computing-human computer interaction; Interaction design; Interaction design process and method-user centered design; Interaction paradigm; Mixed / augmented reality;; Human computer interaction,Conference Paper,Final,Scopus,2-s2.0-85126355249,Peter,,
"Miao N., Peng X., Chen X., Ren G., Wang G.",57478859400;57479277400;57478441300;57478859500;57479277500;,Investigating the Situational Awareness with Optical See-Through Augmented Reality in IoT-Based Environments,2021,"2021 IEEE International Conference on Electronic Communications, Internet of Things and Big Data, ICEIB 2021",,,,40,43,,1.0,10.1109/ICEIB53692.2021.9686374,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125877113&doi=10.1109%2fICEIB53692.2021.9686374&partnerID=40&md5=ac04cca3f50d96e522b988eaae9348c9,"The Internet of Things (IoT) combines artificial intelligence and cloud computing technology for intelligent environments such as smart cities, smart campuses, and smart homes. IoT devices collect environmental events through built-in sensors. Such data is visualized using optical see-through augmented reality devices to help people enhance their situational awareness and improve safety, efficiency, and living quality. A system and interaction design are proposed for improving situational awareness with augmented reality. We reported the visual and audio data collection and rendering, as well as initial user evaluation. © 2021 IEEE.",Augmented Reality; Internet of Things (IoTs); Situational Reality,Augmented reality; Automation; Intelligent buildings; Petroleum reservoir evaluation; User interfaces; Built-in sensors; Cloud computing technologies; Intelligent environment; Internet of thing; Living quality; Optical see-through; Safety qualities; Situational awareness; Situational reality; Smart homes; Internet of things,Conference Paper,Final,Scopus,2-s2.0-85125877113,Peter,,
"Markl N., Lai C.",57421991100;56151963800;,Context-sensitive evaluation of automatic speech recognition: considering user experience & language variation,2021,"Bridging Human-Computer Interaction and Natural Language Processing, HCINLP 2021 - Proceedings of the 1st Workshop",,,,34,40,,3.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123306427&partnerID=40&md5=df75797e2514abd5474e240c66ba055e,"Commercial Automatic Speech Recognition (ASR) systems tend to show systemic predictive bias for marginalised speaker/user groups. We highlight the need for an interdisciplinary and context-sensitive approach to documenting this bias incorporating perspectives and methods from sociolinguistics, speech & language technology and human-computer interaction in the context of a case study. We argue evaluation of ASR systems should be disaggregated by speaker group, include qualitative error analysis, and consider user experience in a broader sociolinguistic and social context. © 2021 Association for Computational Linguistics",,Human computer interaction; Linguistics; User interfaces; Automatic speech recognition; Automatic speech recognition system; Case-studies; Context-sensitive; Language technology; Language variation; Social context; Speech technology; User groups; Users' experiences; Speech recognition,Conference Paper,Final,Scopus,2-s2.0-85123306427,Peter,,
McGill E.,57375024600;,Tailoring automatic text simplification output for deaf and hard of hearing adults,2021,CEUR Workshop Proceedings,3030,,,50,59,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121318913&partnerID=40&md5=f1aa3335a2fabc53cf742a7c6cff0190,"This paper presents a research proposal into tuning the outputs of an Automatic Text Simplification (ATS) system to the needs of the deaf and hard-of-hearing (DHH) community. There is an overview of simplification methods, and studies on ATS where it relates to the DHH community. It then presents an experimental design where research participants are presented with simplified text from an established method of ATS, and simplified text which has been post-processed to move the sentence topic to initial position - a common feature of sign languages. Each simplification method is also evaluated by automatic metrics. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).",Accessibility; Natural language processing; Sign language; Text simplification,Audition; Automatic metrics; Common features; Hard of hearings; Research proposals; Sign language; Simplification method; Text simplification; Natural language processing systems,Conference Paper,Final,Scopus,2-s2.0-85121318913,Peter,,
"Tamayo A., de Los Reyes Lozano J., Martí Ferriol J.L.",56732667300;57196152792;35573169100;,The reception of subtitling for the deaf and hard-of-hearing in Spanish tv news programs [Recepción de la subtitulación para personas sordas de los informativos españoles],2021,Comunicacion y Sociedad (Mexico),18,,e7875,,,,,10.32870/cys.v2021.7875,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121055487&doi=10.32870%2fcys.v2021.7875&partnerID=40&md5=207ee585455e68817371df127be84b54,"The current paper presents a study on the subtitling for the deaf and hard-of-hearing (sdh) in Spanish tv news programs (live and semi-live subtitling). The aim of this reception study is to analyze users’ comprehension and assessment of live and semi-live sdh. A contemporary corpus comprising items from real news broadcasts was used for the research, and a sample of 52 deaf and hard-of-hearing participants was recruited for the experiment, which was carried out through a variety of virtual and live sessions. The results show that users consider the quality of sdh acceptable and audiovisual comprehension insufficient. © 2021 Universidad de Guadalajara. All rights reserved.",Accessibility; Audiovisual communication; Reception; SDH; Subtitling,,Article,Final,Scopus,2-s2.0-85121055487,Peter,,
"Cojean S., Martin N.",57194031578;57683621200;,Reducing the split-attention effect of subtitles during video learning: Might the use of occasional keywords be an effective solution?,2021,Annee Psychologique,121,4,,417,442,,1.0,10.3917/anpsy1.214.0417,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120803970&doi=10.3917%2fanpsy1.214.0417&partnerID=40&md5=83b175a8414b4895e4a536b39b98d0b7,"Learning via videos presents many positive aspects (e.g., animation, multi-modality) but also has some constraints. For example, when subtitles are provided, a split-attention effect could occur between the oral narration, written text, and visual illustration. The presentation of only a few written keywords instead of subtitles may be a good solution in terms of how to guide learners into their information selection process. In the current study, 96 participants were distributed among four experimental conditions. They were shown a 12-minutes video with or without subtitles, and with or without highlighted information (i.e., keywords). The results showed no effect of subtitles, but keywords had a negative impact on content memorization, comprehension, and on the time allocated to learning. The results are discussed in terms of metacognition and learners' strategies. It is then hypothesized that learners did not use keywords as relevant scaffolds. Instead of being guided into the selection process, learners may have considered that the keywords replaced it, and overestimated their learning. © 2021 Presses Universitaires de France. All rights reserved.",Keywords; Learning; Split-attention; Subtitles; Video,adult; article; attention; comprehension; female; human; human experiment; major clinical study; male; metacognition; videorecording,Article,Final,Scopus,2-s2.0-85120803970,Peter,,
"Al-Abbas L.S., Haider A.S.",57219725689;57201379366;,Using Modern Standard Arabic in subtitling Egyptian comedy movies for the deaf/ hard of hearing,2021,Cogent Arts and Humanities,8,1,1993597,,,,9.0,10.1080/23311983.2021.1993597,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118451512&doi=10.1080%2f23311983.2021.1993597&partnerID=40&md5=cdc032fcf5ec3f6f4726ba5cb8d814b8,"Audiovisual Translation (AVT) has gained widespread popularity due to various factors, including technology advancement and, more importantly, audience needs. In the COVID-19 pandemic lockdown, Netflix added Modern Standard Arabic (MSA) subtitles to Egyptian Colloquial Arabic movies. This study investigates how the deaf and hard-of-hearing audience received this service in comedy movies. The script of the movie in the vernacular Egyptian was qualitatively compared to Netflix MSA subtitles. A sample group of 40 deaf and hard of hearing participants was asked to watch an Egyptian comedy movie with MSA subtitles and fill in a 12-item questionnaire of four constructs. Since SDH in the Arab World is still relatively new, the quantitative analysis confirmed the expected conclusion that intralingual subtitling of Arabic movies is a step in the right direction to make audiovisual materials accessible to the DHH and enhance their feeling of social inclusion. The qualitative analysis demonstrated the differences between the MSA subtitles and the vernacular Egyptian utterances regarding the information included, whether linguistic or paralinguistic. The qualitative results also showed that the MSA subtitles had additional information, such as speaker tags, sound effects, and other non-linguistic features that helped more than half of the participants gain better access to the different elements of the movie. The analysis also showed that rendering the dialectal expressions and intentional slips of the tongue into MSA seemed odd and less humorous in some cases. The study findings can be helpful for both translator training programs and industry, especially those interested in subtitling audiovisual materials for people with varying sensorial abilities. In the Arab World, the volume of SDH still lags. Therefore, Arab governments are recommended to impose regulations on TV channels to increase subtitling for this group of community in an attempt to be more just and inclusive. © 2021 The Author(s). This open access article is distributed under a Creative Commons Attribution (CC-BY) 4.0 license.",accessibility; Audiovisual translation (AVT); deaf and hard of hearing; intralingual translation; Modern Standard Arabic (MSA); subtitling,,Article,Final,Scopus,2-s2.0-85118451512,Peter,,
"Al Amin A., Glasser A., Kushalnagar R., Vogler C., Huenerfauth M.",57311937000;57195128152;36142036500;7005789370;12240800100;,Preferences of deaf or hard of hearing users for live-tv caption appearance,2021,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12769 LNCS,,,189,201,,2.0,10.1007/978-3-030-78095-1_15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117899232&doi=10.1007%2f978-3-030-78095-1_15&partnerID=40&md5=bd14d36c43a9d4f246d8340883739312,"There is a wide range of visual appearance of captions during television programming (e.g. text color, typeface, caption background, number of lines, caption placement), especially during live or near-live broadcasts in local markets. The effect of these visual properties of captions on Deaf and Hard of Hearing (DHH) users’ TV-watching experience have been less explored in existing research-based guidelines nor in the design of state-of-the-art caption evaluation metrics. Therefore, we empirically investigated what visual attributes of captions are preferred by DHH viewers while watching captioned live TV programs. We convened two focus groups where participants watched videos consisting of captions with various display properties and provided subjective open-ended feedback. By analyzing the focus-group responses, we observed DHH users’ preference for specific contrast between caption text and background color such as, black text on white background or vice-versa, and caption placement not occluding onscreen salient content. Our findings also revealed for preferences genre-adaptive caption typeface and movement during captioned live TV programming. © Springer Nature Switzerland AG 2021.",Caption; Evaluation; Metric,Caption; Evaluation; Focus groups; Hard of hearings; Local markets; Metric; State of the art; Television programming; Visual appearance; Visual properties; Audition,Conference Paper,Final,Scopus,2-s2.0-85117899232,Peter,,
"Al Amin A., Hassan S., Huenerfauth M.",57311937000;57224309524;12240800100;,Effect of occlusion on deaf and hard of hearing users’ perception of captioned video quality,2021,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12769 LNCS,,,202,220,,1.0,10.1007/978-3-030-78095-1_16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117893826&doi=10.1007%2f978-3-030-78095-1_16&partnerID=40&md5=d1525b329db4b402ed272394a8428ce5,"While the availability of captioned television programming has increased, the quality of this captioning is not always acceptable to Deaf and Hard of Hearing (DHH) viewers, especially for live or unscripted content broadcast from local television stations. Although some current caption metrics focus on textual accuracy (comparing caption text with an accurate transcription of what was spoken), other properties may affect DHH viewers’ judgments of caption quality. In fact, U.S. regulatory guidance on caption quality standards includes issues relating to how the placement of captions may occlude other video content. To this end, we conducted an empirical study with 29 DHH participants to investigate the effect on user’s judgements of caption quality or their enjoyment of the video, when captions overlap with an onscreen speaker’s eyes or mouth, or when captions overlap with onscreen text. We observed significantly more negative user-response scores in the case of such overlap. Understanding the relationship between these occlusion features and DHH viewers’ judgments of the quality of captioned video will inform future work towards the creation caption evaluation metrics, to help ensure the accessibility of captioned television or video. © Springer Nature Switzerland AG 2021.",Caption; Metric; Occlusion; Stimuli,current; Caption; Hard of hearings; Local television stations; Metric; Occlusion; Stimulus; Television programming; User perceptions; Video quality; Audition,Conference Paper,Final,Scopus,2-s2.0-85117893826,Peter,,
"Yenkimaleki M., van Heuven V.J., Moradimokhles H.",57194216662;6602785849;57215660161;,The effect of prosody instruction in developing listening comprehension skills by interpreter trainees: does methodology matter?,2021,Computer Assisted Language Learning,,,,,,,4.0,10.1080/09588221.2021.1957942,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114369704&doi=10.1080%2f09588221.2021.1957942&partnerID=40&md5=470d342b4fcf8cd10f24b51e74c96281,"In the present study, three groups of interpreter trainees were formed, two experimental groups, i.e., blended prosody instruction (BPI) and computer-assisted prosody training (CAPT), and one control group (CON). In this experiment the participants took part in a four-week teaching program for 16 sessions (60 minutes per session), i.e., 16 hours in all. The participants were native Persian speakers who studied English interpreting at the BA level in Iran. The control group listened to authentic audio tracks or watched authentic English movies, discussed their contents, and did exercises based on these tasks for developing listening comprehension skills during the full 16 hours. The CAPT group spent one-third of the time (320 minutes) instead on prosody training using Accent Master Software. The BPI group did this for only 160 minutes but spent the other 160 minutes on theoretical explanations of prosody, and did practical exercises with prosodic structures supervised by an expert human instructor. Students then took a posttest in listening comprehension skills. The results revealed that the BPI group outperformed the other groups in developing listening comprehension skills. This conclusion may have pedagogical implications for interpreter training programs, foreign language instructors, and interpreting practitioners. Supplemental data for this article is available online at https://doi.org/10.1080/09588221.2021.1957942. © 2021 Informa UK Limited, trading as Taylor & Francis Group.",BPI; CAPT; interpreter trainees; listening comprehension skills; prosody training,,Article,Article in Press,Scopus,2-s2.0-85114369704,Peter,,
"Senkbeil J.C., Griffin D.J., Sherman-Morris K., Saari J., Brothers K.",13406211800;55170722300;6504672332;57225038367;57212563490;,Improving tornado warning communication for deaf and hard of hearing audiences,2021,Journal of Operational Meteorology,9,,,18,35,,2.0,10.15191/nwajom.2021.0902,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109000995&doi=10.15191%2fnwajom.2021.0902&partnerID=40&md5=1c329ee28922b88ca0bad241c368ab60,"Although specialized personal and residential Deaf warning technologies exist, receipt and comprehension of tornado warning information from local television is often delayed or misunderstood because of closed-captioning deficiencies. In order to suggest improvements for the communication of tornado warnings to Deaf and Hard of Hearing (D/HoH) audiences, interviews and a focus group were conducted within the active tornado counties of Alabama. D/HoH individuals generally use more information sources than the hearing population to better understand their risk. Protective action decision-making by our sample was characterized by more hesitation, uncertainty, and indecision than in the hearing population. The most common suggestion for improving tornado-warning communication was to have an American Sign Language (ASL) interpreter shown on screen with a local television meteorologist during a tornado warning. A split-screen television product with an ASL interpreter in a remote studio was prototyped showing that this type of live broadcast is possible for local tornado-warning coverage. Several screen formats were evaluated by a focus group with the conclusion that the ASL interpreter should be on the left side of the screen without obscuring any part of the weather broadcast. The split-screen product with an ASL interpreter resulted in full access to all broadcast information, the ability to make immediate safety decisions, and was welcomed with excitement by the focus-group participants. This modification, along with the education and preparedness efforts of the National Weather Service, help remedy the information gaps and comprehension delays of this underserved population. © 2021, National Weather Association. All rights reserved.",,,Article,Final,Scopus,2-s2.0-85109000995,Peter,,
Deckert M.,26656622300;,Spelling errors in interlingual subtitles: Do viewers really mind?,2021,GEMA Online Journal of Language Studies,21,2,,135,152,,2.0,10.17576/gema-2021-2102-07,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108158518&doi=10.17576%2fgema-2021-2102-07&partnerID=40&md5=c08dca4e95726633f9ee52bd8f8431b2,"Our overarching objective is to see how unambiguous deficiencies in interlingual subtitles influence the viewing experience. To that end, we conducted a reception experiment in which participants viewed a foreign language film sample with subtitles which were manipulated across conditions for the number of spelling errors. We find that while viewers succeed in identifying spelling errors in subtitles, the presence of errors nonetheless generally has no effect on a range of viewer experience dimension like cognitive load, enjoyment, comprehension or transportation. What is more, while participants were able to make different subtitle authorship attributions (professional subtitler vs. amateur subtitler) depending on the presence of typos, deficient spelling did not shape the viewer’s perception of the subtitler in terms of their estimated amount of experience or their diligence. Critically, the findings also indicate that typos have no effect on translation quality assessment scores which remain high even when there are as many as 20 typos in subtitles for a 14-minute clip. This work therefore offers new insights into translation reception with consequences for the didactic and professional settings. By embedding spelling errors in a dynamic and multimodal context where processing is not self-paced, the study importantly expands our understanding of how spelling errors are received, which has implications beyond translation studies as well. © 2021, Penerbit Universiti Kebangsaan Malaysia. All rights reserved.",Audiovisual content; Cognitive processing; Interlingual subtitling; Reception; Spelling errors,,Article,Final,Scopus,2-s2.0-85108158518,Peter,,
Alper M.,55321948400;,"Critical Media Access Studies: Deconstructing Power, Visibility, and Marginality in Mediated Space",2021,International Journal of Communication,15,,,840,861,,6.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107972823&partnerID=40&md5=21b606dbf4eb967d77eb82c68df00dde,"The benefits of “accessible” media and technology for people with disabilities are rarely questioned, nor considered within broader critical/cultural frameworks. This article makes a contribution to the field of communication by proposing critical media access studies to further define a growing area of inquiry into contested notions of mediated access, drawing on work from disability media studies and critical access studies in architectural design. The proposal for critical media access studies is furthered through a case study of physical spaces designed for media engagement for young people, from museum exhibits to movie theaters, that provide “autism-friendly” programming. Qualitative analysis of interviews and observations with 27 autistic children and their families, as well as participant observation in 7 such sites, reveals ideological assumptions, frictions, and contradictions underpinning cultural accessibility. Critical media access studies can offer communication scholars valuable theoretical and conceptual tools for deconstructing power, visibility, and marginality in mediated space. © 2021. (Meryl Alper). Licensed under the Creative Commons Attribution Non-commercial No Derivatives (by-nc-nd). Available at http://ijoc.org.",accessibility; autism; children; critical media access studies; disability; space,,Article,Final,Scopus,2-s2.0-85107972823,Peter,,
"Tomassi N.E., Castro M.E., Timmons Sund L., Díaz-Cádiz M.E., Buckley† D.P., Stepp C.E.",57223359580;57221562856;57219596286;57208501961;57223362352;35759791500;,Effects of Sidetone Amplification on Vocal Function During Telecommunication,2021,Journal of Voice,,,,,,,2.0,10.1016/j.jvoice.2021.03.027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105757596&doi=10.1016%2fj.jvoice.2021.03.027&partnerID=40&md5=726947bec9f47e39bf569c49ceba1d18,"Purpose: Society has become increasingly dependent on telecommunication, which has been shown to negatively impact vocal function. This study explores the use of sidetone regulation during audio-visual communication as one potential technique to alleviate the effects of telecommunication on the voice. Method: The speech acoustics of 18 participants with typical voices were measured during conversational tasks during three conditions of sidetone amplification: baseline (no sidetone amplification), low sidetone amplification, and high sidetone amplification. Vocal intensity, vocal quality (estimated using acoustic measures of the low-high ratio and the smoothed cepstral peak prominence), and self-perceived vocal effort were used to measure the impacts of sidetone amplification on vocal function. Results: Compared to baseline, there were statistically significant decreases in vocal intensity and increases in low-high ratio in the high level of sidetone amplification condition. Changes in these measures were not significantly correlated. When asked to rank conditions based on their perceived vocal effort, participants most often ranked the high level of sidetone amplification as least effortful; however, the visual-analog ratings of vocal effort were not significantly different between conditions. The smoothed cepstral peak prominence did not change with varying levels of sidetone amplification. Conclusions: Vocal intensity decreased with high levels of sidetone amplification. High levels of sidetone amplification also resulted in increases in the low-high ratio, which were shown to be more than just a byproduct of decreased vocal intensity. The impact of sidetone amplification on vocal effort was less clear, but results suggested that participants generally decreased their vocal effort with increased levels of sidetone amplification. This was a preliminary study and future work is warranted in a population of participants with voice complaints and in a more noisy, realistic environments. © 2021",Auditory feedback; Live-mic monitoring; Speech production; Telecommunication; Vocal function,,Article,Article in Press,Scopus,2-s2.0-85105757596,Peter,,
"Hughes C.J., Paton J.",57040431000;57222516588;,Voice interaction for accessible immersive video players,2021,"VISIGRAPP 2021 - Proceedings of the 16th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications",2,,,182,189,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102973431&partnerID=40&md5=e358185497ffb637ece4c247f8abcb8a,"Immersive environments present new challenges for all users, especially those with accessibility requirements. Once a user is fully immersed in an experience, they no longer have access to the devices that they would have in the real world such as a mouse, keyboard or remote control interface. However these users are often very familiar with new technology, such as voice interfaces. A user study as part of the EC funded Immersive Accessibility (ImAc) project identified the requirement for voice control as part of the projects fully accessible 360o video player in order to be fully accessible to people with sight loss. An assessment of speech recognition and voice control options was made. It was decided to use an Amazon Echo with a node.js gateway to control the player through a web-socket API. This proved popular with users despite problems caused by the learning stage in the command structure required for Alexa, the timeout on the Echo and the difficulty of working with Alexa whilst wearing headphones. The web gateway proved to be a robust control mechanism which lends itself to being extended in various ways. Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.",Accessibility; Immersive video; User interface; Voice interaction,Computer graphics; Computer vision; Remote control; Robust control; Speech recognition; Accessibility requirements; Command structure; Control mechanism; Immersive environment; Remote control interfaces; Voice control; Voice interaction; Voice interfaces; Mammals,Conference Paper,Final,Scopus,2-s2.0-85102973431,Peter,,
Agulló B.,57206483570;,Technology for subtitling: A 360-degree turn [Tecnología para la subtitulación: Un giro de 360 grados],2021,Hermeneus,,22,,11,40,,2.0,10.24197/HER.22.2020.11-40,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102304131&doi=10.24197%2fHER.22.2020.11-40&partnerID=40&md5=93280c3b8aba4f92215cf76b99c279e5,"Subtitling has become one of the most important audiovisual translation modes and cannot be understood outside the context of the technology that makes it possible. New audiovisual media are appearing, such as 360o videos, and the necessity of subtitling this type of content to make it accessible is emerging. In this article, an updated review of current subtitling technology is presented to contextualise the study. Then, a review of main immersive environments (3D, augmented reality and virtual reality) and their implications for subtitling has also been introduced. The focus of the study is on virtual reality and, therefore, the main challenges of subtitling 360o content are presented. To respond to the needs of subtitling this type of content, a prototype version of a subtitle editor has been developed and presented to twenty-seven professional subtitlers who have tested the tool and reported the correspondent feedback on usability and preferences. This study has proven the importance of carrying out usability tests with end users when developing specific software. Finally, the challenges faced by subtitlers in new audiovisual media such as 360o content are presented. © 2021 Universidad de Valladolid. All rights reserved.",360° content; Reception study; Subtitling; Subtitling technology; Usability,,Article,Final,Scopus,2-s2.0-85102304131,Peter,,
"Gerber-Morón O., Soler-Vilageliu O., Castellà J.",57202588774;57222314882;56268951100;,The effects of screen size on subtitle layout preferences and comprehension across devices [Efectos del tamaño de pantalla en las preferencias de presentación de subtítulos y en la comprensión en distintos dispositivos],2021,Hermeneus,,22,,157,182,,,10.24197/HER.22.2020.157-182,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102293488&doi=10.24197%2fHER.22.2020.157-182&partnerID=40&md5=db31baff5f945649c17fd39f3038d937,"The present study sheds light on the possible effects that screen size can have on preferences and comprehension of subtitled audiovisual material. Thirty participants watched three subtitled video excerpts displayed on three devices with different screen sizes (monitor, tablet, and smartphone). After watching each excerpt, they filled out preference and comprehension questionnaires. This study aimed to provide new empirical evidence on viewers' needs and preferences concerning readability by analysing the reception of subtitles across screens. The results obtained indicate that smartphone devices had the most unsatisfactory effects, suggesting the need to undertake further research on small screens to improve subtitle readability. © 2021 Universidad de Valladolid. All rights reserved.",Accessibility; New technologies; Readability; Screen size; Subtitling,,Article,Final,Scopus,2-s2.0-85102293488,Peter,,
"Picallo I., Vidal-Balea A., Blanco-Novoa O., Lopez-Iturri P., Fraga-Lamas P., Klaina H., Fernandez-Carames T.M., Azpilicueta L., Falcone F.",57210819614;57221048329;57193715377;56348853300;56039568800;57203154167;24467756100;54583157100;24467846500;,Design and Experimental Validation of an Augmented Reality System with Wireless Integration for Context Aware Enhanced Show Experience in Auditoriums,2021,IEEE Access,9,,9311110,5466,5484,,4.0,10.1109/ACCESS.2020.3048203,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099102225&doi=10.1109%2fACCESS.2020.3048203&partnerID=40&md5=6590757c8c06bac488d8394d2f02cfff,"The development of multiple cultural and social related activities, such as shows related with the performing arts, conferences or presentations rely on facilities such as auditoriums, theatres and conference sites, which are progressively including multiple technological features in order to enhance user experience. There are still however situations in which user experience is limited owing to lack of environment adaption, such as people with disabilities. In this sense, the adoption of Context Aware paradigms within auditoriums can provide adequate functionalities in order to comply with specific needs. This work is aimed at demonstrating the feasibility in enhancing user experience (e.g., improving the autonomy of disabled people) within auditorium and theatre environments, by means of an Augmented Reality (AR) device (HoloLens smart glasses) with wireless system integration. To carry out the demonstration, different elements to build AR applications are described and tested. First, an intensive measurement campaign was performed in a real auditorium in the city of Pamplona (Baluarte Congress Center) in order to evaluate the feasibility of using Wi-Fi enabled AR devices in a complex wireless propagation scenario. The results show that these environments exhibit high levels of interference, owing to the co-existence and non-coordinated operation of multiple wireless communication systems, such as on site and temporary Wi-Fi access points, wireless microphones or communications systems used by performers, staff and users. Deterministic wireless channel estimation based in volumetric 3D Ray Launching have been obtained for the complete scenario volume, in order to assess quality of service metrics. For illustration purposes, a user-friendly application to help hearing impaired people was developed and its main features were tested in the auditorium. Such an application provides users with a 3D virtual space to visualize useful multimedia content like subtitles or additional information about the show, as well as an integrated call button. © 2013 IEEE.",3D ray launching; Auditorium; augmented reality; enhanced show experience; HoloLens; impaired persons; wireless channel,Audition; Auditoriums; Augmented reality; Multiple access interference; Quality of service; Wi-Fi; Wireless local area networks (WLAN); Augmented reality systems; Communications systems; Experimental validations; Intensive measurements; Non-coordinated operations; People with disabilities; Quality of service metrics; Wireless communication system; User experience,Article,Final,Scopus,2-s2.0-85099102225,Peter,,
"Climent M.M., Soler-Vilageliu O., Vila I.F., Langa S.F.",35868074700;12778196600;57221420566;57219788964;,"VR360 Subtitling: Requirements, Technology and User Experience",2021,IEEE Access,9,,9311182,2819,2838,,2.0,10.1109/ACCESS.2020.3047377,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099090174&doi=10.1109%2fACCESS.2020.3047377&partnerID=40&md5=1cf30dd34d38af1e6ab2e339fa98e57c,"As with every service, immersive media needs to be accessible. While a plethora of accessibility solutions and guidelines exist for traditional media services, this is not yet true when it comes to immersive media. This article focuses on filling this gap for VR360 video subtitling, as a key access service to be integrated in this recent, but widespread, medium. After reviewing the state-of-the-art, the paper elaborates on a set of key challenges and requirements to efficiently subtitle VR360 videos. Based on these findings, the paper presents technological solutions to efficiently address and meet the identified challenges and requirements, respectively, in a standard-compliant manner. Finally, the paper reports on a set of user tests aimed at shedding some light with regard to four key aspects when it comes to VR360 video subtitling: 1) comfortable viewing fields; 2) presentation modes; 3) guiding methods; and 4) (re-)presentation of non-speech information. In conjunction, the contributions of the paper are expected to become a valuable resource for the interested audience in this field, including users with accessibility needs, the scientific and development community, service providers and standardization bodies. © 2013 IEEE.",360° video; accessibility; immersive media; subtitling; virtual reality,Access service; Development community; Immersive media; Media services; Presentation modes; Service provider; State of the art; Technological solution; User experience,Article,Final,Scopus,2-s2.0-85099090174,Peter,,
"Liao S., Yu L., Reichle E.D., Kruger J.-L.",57203968644;56336527600;6602766587;9277428700;,Using Eye Movements to Study the Reading of Subtitles in Video,2021,Scientific Studies of Reading,25,5,,417,435,,9.0,10.1080/10888438.2020.1823986,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096088857&doi=10.1080%2f10888438.2020.1823986&partnerID=40&md5=276a53f138b7a223825f0eb95111a9bb,"This article reports the first eye-movement experiment to examine how the presence versus absence of concurrent video content and presentation speed affect the reading of subtitles. Results indicated that participants adapted their visual routines to examine video content while simultaneously prioritizing the reading of subtitles, especially when the latter was displayed only briefly. Although decisions about when and where to move the eyes largely remained under local (cognitive) control, this control was also modulated by global task demands, suggesting an integration of local and global eye-movement control. The theoretical and pedagogical implications of these findings are discussed, and we also briefly describe a new theoretical framework for understanding all forms of multimodal reading, including the reading of subtitles in video. © 2020 Society for the Scientific Study of Reading.",,,Article,Final,Scopus,2-s2.0-85096088857,Peter,,
"Jorgensen E.J., Stangl E., Chipara O., Hernandez H., Oleson J., Wu Y.-H.",57209842900;55552817500;9334529600;57219466748;14040397100;18038757800;,GPS predicts stability of listening environment characteristics in one location over time among older hearing aid users,2021,International Journal of Audiology,60,5,,328,340,,,10.1080/14992027.2020.1831083,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092891169&doi=10.1080%2f14992027.2020.1831083&partnerID=40&md5=47be6a353efc6949a601a7d6441525a3,"Objective: Hearing aid technology can allow users to “geo-tag” hearing aid preferences using the Global Positioning System (GPS). This technology assumes that listening environment characteristics that affect hearing aid benefit change little in a location over time. The purpose of this study was to investigate whether certain characteristics (reverberation, signal type, listening activity, noise location, noisiness, talker familiarity, talker location, and visual cues) changed in a location over time. Design: Participants completed GPS-tagged surveys on smartphones to report on characteristics of their listening environments. Coordinates were used to create indices that described how much listening environment characteristics changed in a location over time. Indices computed in one location were compared to indices computed across all locations for each participant. Study sample: 54 adults with hearing loss participated in this study (26 males and 38 females; 30 experienced hearing aid users and 24 new users). Results: A location dependency was observed for all characteristics. Characteristics were significantly different from one another in their stability over time. Conclusions: Listening environment characteristics changed less over time in a given location than in participants’ lives generally. The effectiveness of GPS-dependent hearing aid settings likely depends on the accuracy and location definition of the GPS feature. © 2020 British Society of Audiology, International Society of Audiology, and Nordic Audiological Society.",global positioning system; hearing aid outcomes; Hearing aids; listening environment; soundscape,"adult; adverse event; female; geographic information system; hearing aid; human; male; noise; perception deafness; speech perception; Adult; Female; Geographic Information Systems; Hearing Aids; Hearing Loss, Sensorineural; Humans; Male; Noise; Speech Perception",Article,Final,Scopus,2-s2.0-85092891169,Peter,,
"Law E.L.-C., Soleimani S., Watkins D., Barwick J.",55909522400;56928037100;40462538800;57191040781;,Automatic voice emotion recognition of child-parent conversations in natural settings,2021,Behaviour and Information Technology,40,11,,1072,1089,,2.0,10.1080/0144929X.2020.1741684,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082189124&doi=10.1080%2f0144929X.2020.1741684&partnerID=40&md5=f37f33226100fddbb566e11bb3f01e83,"While voice communication of emotion has been researched for decades, the accuracy of automatic voice emotion recognition (AVER) is yet to improve. In particular, the intergenerational communication has been under-researched, as indicated by the lack of an emotion corpus on child–parent conversations. In this paper, we presented our work of applying Support-Vector Machines (SVMs), established machine learning models, to analyze 20 pairs of child–parent dialogues on everyday life scenarios. Among many issues facing the emerging work of AVER, we explored two critical ones: the methodological issue of optimising its performance against computational costs, and the conceptual issue on the state of emotionally neutral. We used the minimalistic/extended acoustic feature set extracted with OpenSMILE and a small/large set of annotated utterances for building models, and analyzed the prevalence of the class neutral. Results indicated that the bigger the combined sets, the better the training outcomes. Regardless, the classification models yielded modest average recall when applied to the child–parent data, indicating their low generalizability. Implications for improving AVER and its potential uses are drawn. © 2020 Informa UK Limited, trading as Taylor & Francis Group.",child-parent conversation; emotion corpora; emotion neutrality; IEMOCAP; recognition accuracy; Vocal emotion,Support vector machines; child-parent conversation; Emotion corpora; emotion neutrality; IEMOCAP; Recognition accuracy; Vocal emotion; Speech recognition; article; child; conversation; female; human; human experiment; male; outcome assessment; prevalence; recall; support vector machine; voice,Article,Final,Scopus,2-s2.0-85082189124,Peter,,
"Hughes C.J., Zapata M.B., Johnston M., Orero P.",57040431000;57221691373;57221750812;24921771100;,Immersive Captioning: Developing a framework for evaluating user needs,2020,"Proceedings - 2020 IEEE International Conference on Artificial Intelligence and Virtual Reality, AIVR 2020",,,9319125,313,318,,2.0,10.1109/AIVR50618.2020.00063,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100049542&doi=10.1109%2fAIVR50618.2020.00063&partnerID=40&md5=f9860c51ddb41c72d10e8dffc88795f1,"This article focuses on captioning for immersive environments and the research aims to identify how to display them for an optimal viewing experience. This work began four years ago with some partial findings. This second stage of research, built from the lessons learnt, focuses on the design requirements cornerstone: prototyping. A tool has been developed towards quick and realistic prototyping and testing. The framework integrates methods used in existing solutions. Given how easy it is to contrast and compare, the need to further the first framework was obvious. A second improved solution was developed, almost as a showcase on how ideas can quickly be implemented for user testing. After an overview on captions in immersive environments, the article describes its implementation, based on web technologies opening for any device with a web browser. This includes desktop computers, mobile devices and head mounted displays. The article finishes with a description of the new caption modes and methods, hoping to be a useful tool towards testing and standardisation. © 2020 IEEE.",Accessibility; Captions; Immersivevideo; Subtitle,Helmet mounted displays; Personal computers; Virtual reality; Head mounted displays; Immersive; Immersive environment; User need; User testing; Web technologies; Artificial intelligence,Conference Paper,Final,Scopus,2-s2.0-85100049542,Peter,,
"Miner A.S., Haque A., Fries J.A., Fleming S.L., Wilfley D.E., Terence Wilson G., Milstein A., Jurafsky D., Arnow B.A., Stewart Agras W., Fei-Fei L., Shah N.H.",56996997300;56678984900;57190403708;57208627380;7004010947;57218149048;7005598768;6602872553;7004311743;57218148877;8952942000;7401823709;,Assessing the accuracy of automatic speech recognition for psychotherapy,2020,npj Digital Medicine,3,1,82,,,,22.0,10.1038/s41746-020-0285-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088065645&doi=10.1038%2fs41746-020-0285-8&partnerID=40&md5=18183d6c904a96efa324bcb9fc5b358a,"Accurate transcription of audio recordings in psychotherapy would improve therapy effectiveness, clinician training, and safety monitoring. Although automatic speech recognition software is commercially available, its accuracy in mental health settings has not been well described. It is unclear which metrics and thresholds are appropriate for different clinical use cases, which may range from population descriptions to individual safety monitoring. Here we show that automatic speech recognition is feasible in psychotherapy, but further improvements in accuracy are needed before widespread use. Our HIPAA-compliant automatic speech recognition system demonstrated a transcription word error rate of 25%. For depression-related utterances, sensitivity was 80% and positive predictive value was 83%. For clinician-identified harm-related sentences, the word error rate was 34%. These results suggest that automatic speech recognition may support understanding of language patterns and subgroup variation in existing treatments but may not be ready for individual-level safety surveillance. © 2020, The Author(s).",,Speech recognition; Automatic speech recognition; Automatic speech recognition system; Clinical use; Individual levels; Language patterns; Mental health; Positive predictive values; Safety monitoring; Speech recognition softwares; Word error rate; Audio recordings; article; automatic speech recognition; controlled study; diagnostic test accuracy study; genetic transcription; human; language; predictive value; psychotherapy,Article,Final,Scopus,2-s2.0-85088065645,Peter,,
"Mohd Hashim M.H., Tasir Z.",55871823700;54889041900;,An e-learning environment embedded with sign language videos: research into its usability and the academic performance and learning patterns of deaf students,2020,Educational Technology Research and Development,68,6,,2873,2911,,9.0,10.1007/s11423-020-09802-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086838124&doi=10.1007%2fs11423-020-09802-4&partnerID=40&md5=d68ceff4d62288eb4b96fe108c26705c,"This research investigates the usability of an e-learning environment that is embedded with sign language videos and deaf students’ related academic performances and learning patterns. A mixed-methods research design was utilized, which involved the use of a usability questionnaire, performance tests, learning activities, e-learning log data, and interviews. The results revealed that the deaf students showed a moderate level of e-learning usability. However, there was a statistically significant difference in the performance tests, and the treatment used had a large effect on the deaf students’ performance. Through decision tree analysis, eleven learning patterns emerged based on the three increment categories of the deaf students’ performance. Briefly, the deaf students who achieved the best performance increment category were those who accessed the sign language videos more frequently compared to other deaf students. The findings and implications are further discussed, and possible future studies are suggested. © 2020, Association for Educational Communications and Technology.",Academic performance; Deaf; Learning patterns; Sign language video; Usability,,Article,Final,Scopus,2-s2.0-85086838124,Peter,,
"Montagud M., Orero P., Matamala A.",35868074700;24921771100;25921557700;,Culture 4 all: accessibility-enabled cultural experiences through immersive VR360 content,2020,Personal and Ubiquitous Computing,24,6,,887,905,,17.0,10.1007/s00779-019-01357-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077547629&doi=10.1007%2fs00779-019-01357-3&partnerID=40&md5=1f92076387dd03e5b1f5a01fa4b4d122,"The use of virtual reality (VR) technologies and, in particular, of VR360 content can provide great benefits in the society. This particularly applies to the culture and heritage sectors, where venues, goods and events can be digitalised to provide hyper-realistic and engaging experiences, anywhere and anytime. Despite significant advances in the field of VR, there is still an unexplored aspect which is crucial for every service and experience where user interaction is expected: accessibility. This article firstly reviews the needs, challenges and limitations for making VR360 experiences accessible. Based on these facts, an end-to-end platform to efficiently integrate accessibility services within VR360 content is presented. The platform encompasses all steps from media authoring to media consumption, but special attention is given to the accessibility-enabled VR360 player, as it is the end user interaction interface. The presentation modes for the supported access services (like subtitling, audio description and sign language), interaction modalities and personalisation features supported by the player are described. To conclude, the availability of newly created and adapted culture-related accessible VR360 content is explored, as a proof of the potential of the contributions work in this sector. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",360° video; Accessibility; Cultural heritage; Culture; User interaction; Virtual reality; VR360,Cell culture; User interfaces; Virtual reality; Accessibility; Audio description; Cultural heritages; Media consumption; Personalisation; Presentation modes; User interaction; VR360; User experience,Article,Final,Scopus,2-s2.0-85077547629,Peter,,
"Borges T.F.C., Guimarães R.L.",57211990044;35183179100;,Toward Assessing the Quality of Subtitles based on SRT Files Processing,2020,ACM International Conference Proceeding Series,,,,153,156,,,10.1145/3428658.3431758,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097356672&doi=10.1145%2f3428658.3431758&partnerID=40&md5=10e11fc9a680cade42c816601a813d64,"In order for subtitles to fulfill their objective of enriching users' experience, it is necessary that, during the subtitling process (creation of subtitles), ""good practices"", usually specified in subtitling guides, are taken into account. In this context, this work uses an approach based on the processing of SRT files to assess the degree of compliance of subtitles in relation to the recommendations of a reference guide considered in the subtitling process. Our preliminary results show how meticulous is the process performed by those who aim at providing a satisfactory quality of experience (QoE) for viewers of the audiovisual content. © 2020 ACM.",Captioning; QoE; SRT; Subtitles; Subtitling guidelines,User experience; Audio-visual content; Good practices; Quality of experience (QoE); Reference guides; Users' experiences; Quality of service,Conference Paper,Final,Scopus,2-s2.0-85097356672,Peter,,
Begum N.N.F.,57195518384;,Novel facial characteristics in congenital rubella syndrome: A study of 115 cases in a cardiac hospital of Bangladesh,2020,BMJ Paediatrics Open,4,1,e000860,,,,1.0,10.1136/bmjpo-2020-000860,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097104222&doi=10.1136%2fbmjpo-2020-000860&partnerID=40&md5=0bec5c20ad6a61d7ecdcda14afe718f3,"Objective To establish novel facial characteristics unique to congenital rubella syndrome (CRS) as prediagnostic criteria to supplement disease diagnosis in patients with or without a history of maternal rubella infection. Design An analysis of 115 CRS case series (2018-2020) based on the presence of any of the triad features. Setting Outpatient department of a tertiary care referral cardiac hospital in Dhaka, Bangladesh. Participants In total, 115 participants (53.1% men) were enrolled. Participants underwent echocardiography if they presented with suspected cardiac symptoms along with deafness, cataract or microcephaly. Main outcome measures Age, sex and socioeconomic status of the participants; history of maternal vaccination and infection; facial characteristics unique to CRS (triangular face, prominent nose, wide forehead and a whorl on either side of the anterior hairline) named â € rubella facies' and frequency of systemic involvements in CRS. Results The median patient age was 2 years. The income of 50.4% of the participating families was <US1500. Further, 32 mothers (27.8%) were infected with rubella during the first trimester of pregnancy, 15 (13.0%) during the second trimester and 3 (2.6%) during the third trimester. The remainder (65.2%) recalled no history of infection during pregnancy. Rubella facies presented as a triangular-shaped face in 95 (82.6%) cases, a broad forehead in 88 (76.5%) and a prominent nose in 75 (65.2%). A rubella whorl was present on the right or left side of the anterior hairline in 80% and 18.2% of cases, respectively. IgG and IgM antibodies were present in 91.3% and 8.6% of children, respectively. Cataract, deafness, microcephaly, and congenital heart disease were detected in 53.0%, 75.6%, 68.6% and 98.2% of cases, respectively. Conclusions Rubella facies, a set of unique facial characteristics, can support early CRS diagnosis and treatment and may supplement the existing CRS triad. ©",cardiology; data collection; deafness; ophthalmology; syndrome,immunoglobulin G antibody; immunoglobulin M antibody; rubella antibody; adolescent; Article; Bangladesh; cataract; child; congenital heart disease; congenital rubella syndrome; developmental delay; echocardiography; face malformation; facies; failure to thrive; female; fever; first trimester pregnancy; forehead; hearing impairment; human; income; infant; jaundice; major clinical study; male; maternal disease; medical history; meningoencephalitis; microcephaly; nose malformation; outpatient department; pregnant woman; priority journal; prominent nose; rash; rubella; rubella facies; second trimester pregnancy; serology; social status; splenomegaly; teratogenicity; tertiary care center; third trimester pregnancy; thrombocytopenia; triangular face; vaccination; wide forehead,Article,Final,Scopus,2-s2.0-85097104222,Peter,,
Butler J.,57202196333;,The Visual Experience of Accessing Captioned Television and Digital Videos,2020,Television and New Media,21,7,,679,696,,1.0,10.1177/1527476418824805,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061189278&doi=10.1177%2f1527476418824805&partnerID=40&md5=212194bcdbeafb4830f87bfdad7a4091,"The increase in video-based communication has made different caption styles more apparent to audiences, including hearing viewers who watch social media videos with colorful open captions. To explore how viewers respond to a variety of caption styles, this article shares findings from three focus group discussions with twenty deaf and hard-of-hearing college students. This article begins by discussing the accessibility of captioned television and digital media and how captions can influence the viewing experience. This article then analyzes deaf and hard-of-hearing focus group participants’ statements about their viewing experiences, reception practices, and critiques of aesthetic and accessible caption styles. The analysis of viewers’ feedback reveals how the tension between various approaches to captions contributes to the reshaping of television and online media: a reshaping in which captions are coming to the forefront of the viewing experience for deaf, hard-of-hearing, and hearing viewers. © The Author(s) 2019.",access; new media; participation; television; video; visual studies,,Article,Final,Scopus,2-s2.0-85061189278,Peter,,
"Jain D., Ngo H., Patel P., Goodman S., Findlater L., Froehlich J.",57014317900;57220116619;57660377500;57219114076;10040303000;7101665384;,SoundWatch: Exploring Smartwatch-based Deep Learning Approaches to Support Sound Awareness for Deaf and Hard of Hearing Users,2020,ASSETS 2020 - 22nd International ACM SIGACCESS Conference on Computers and Accessibility,,,3416991,,,,10.0,10.1145/3373625.3416991,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096992592&doi=10.1145%2f3373625.3416991&partnerID=40&md5=2a89b2a3af85c2db09929aac09ef8122,"Smartwatches have the potential to provide glanceable, always-available sound feedback to people who are deaf or hard of hearing. In this paper, we present a performance evaluation of four low-resource deep learning sound classification models: MobileNet, Inception, ResNet-lite, and VGG-lite across four device architectures: watch-only, watch+phone, watch+phone+cloud, and watch+cloud. While direct comparison with prior work is challenging, our results show that the best model, VGG-lite, performed similar to the state of the art for non-portable devices with an average accuracy of 81.2% (SD=5.8%) across 20 sound classes and 97.6% (SD=1.7%) across the three highest-priority sounds. For device architectures, we found that the watch+phone architecture provided the best balance between CPU, memory, network usage, and classification latency. Based on these experimental results, we built and conducted a qualitative lab evaluation of a smartwatch-based sound awareness app, called SoundWatch (Figure 1), with eight DHH participants. Qualitative findings show support for our sound awareness app but also uncover issues with misclassifications, latency, and privacy concerns. We close by offering design considerations for future wearable sound awareness technology. © 2020 ACM.",Accessibility; CNN; Deaf; deep learning; hard of hearing; smartwatch; sound awareness; sound classification; wearable,Audition; E-learning; Network architecture; Privacy by design; Telephone sets; Watches; Wearable computers; Design considerations; Device architectures; Hard of hearings; Learning approach; Misclassifications; Privacy concerns; Sound classification; State of the art; Deep learning,Conference Paper,Final,Scopus,2-s2.0-85096992592,Peter,,
"Oomori K., Shitara A., Minagawa T., Sarcar S., Ochiai Y.",57207815123;57220120132;57195522733;36139109600;36454884800;,A Preliminary Study on Understanding Voice-only Online Meetings Using Emoji-based Captioning for Deaf or Hard of Hearing Users,2020,ASSETS 2020 - 22nd International ACM SIGACCESS Conference on Computers and Accessibility,,,3418032,,,,2.0,10.1145/3373625.3418032,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096968103&doi=10.1145%2f3373625.3418032&partnerID=40&md5=33baa0659ee5b4abbc2787dbcd0c7a33,"In the midst of the coronavirus disease 2019 pandemic, online meetings are rapidly increasing. Deaf or hard of hearing (DHH) people participating in an online meeting often face difficulties in capturing the affective states of other speakers. Recent studies have shown the effectiveness of emoji-based representation of spoken text to capture such affective states. Nevertheless, in voice-only online meetings, it is still not clear how emoji-based spoken texts can assist DHH people to understand the feelings of speakers without perceiving their facial expressions. We therefore conducted a preliminary experiment to understand the effect of emoji-based text representation during voice-only online meetings by leveraging an emoji-based captioning system. Our preliminary results demonstrate the necessity of designing an advanced system to help DHH people understanding the voice-only online meetings more meaningfully. © 2020 Owner/Author.",Deaf; Emoji-based emotion expression; Hard of hearing; User study; Voice-only online meeting,Advanced systems; Affective state; Coronaviruses; Facial Expressions; Hard of hearings; Online meetings; Text representation; Audition,Conference Paper,Final,Scopus,2-s2.0-85096968103,Peter,,
"Jain D., Potluri V., Sharif A.",57014317900;57202047297;55356469700;,Navigating Graduate School with a Disability,2020,ASSETS 2020 - 22nd International ACM SIGACCESS Conference on Computers and Accessibility,,,3416986,,,,19.0,10.1145/3373625.3416986,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096967837&doi=10.1145%2f3373625.3416986&partnerID=40&md5=e4dbf38ef25781b3c781b3229a10c3f5,"In graduate school, people with disabilities use disability accommodations to learn, network, and do research. However, these accommodations, often scheduled ahead of time, may not work in many situations due to uncertainty and spontaneity of the graduate experience. Through a three-person autoethnography, we present a longitudinal account of our graduate school experiences as people with disabilities, highlighting nuances and tensions of situations when our requested accommodations did not work and the use of alternative coping strategies. We use retrospective journals and field notes to reveal the impact of our self-image, relationships, technologies, and infrastructure on our disabled experience. Using post-hoc reflection on our experiences, we then close with discussing personal and situated ways in which peers, faculty members, universities, and technology designers could improve the graduate school experiences of people with disabilities. © 2020 Owner/Author.",accessibility; accessible technology; Autoethnography; blind; computer science education; engineering; graduate school; hard of hearing; mobility impaired; STEM; trio-ethnography,Coping strategies; Faculty members; Field notes; Graduate schools; People with disabilities; User experience,Conference Paper,Final,Scopus,2-s2.0-85096967837,Peter,,
"Gleason C., Valencia S., Kirabo L., Wu J., Guo A., Jeanne Carter E., Bigham J., Bennett C., Pavel A.",57190855167;57209400435;57218502379;57211842413;56789541900;35975697000;16238221500;55926560900;56121790600;,Disability and the COVID-19 Pandemic: Using Twitter to Understand Accessibility during Rapid Societal Transition,2020,ASSETS 2020 - 22nd International ACM SIGACCESS Conference on Computers and Accessibility,,,3417023,,,,24.0,10.1145/3373625.3417023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096952867&doi=10.1145%2f3373625.3417023&partnerID=40&md5=3a6d3dc00c0de06c1ab42525f5e3e41f,"The COVID-19 pandemic has forced institutions to rapidly alter their behavior, which typically has disproportionate negative effects on people with disabilities as accessibility is overlooked. To investigate these issues, we analyzed Twitter data to examine accessibility problems surfaced by the crisis. We identified three key domains at the intersection of accessibility and technology: (i) the allocation of product delivery services, (ii) the transition to remote education, and (iii) the dissemination of public health information. We found that essential retailers expanded their high-risk customer shopping hours and pick-up and delivery services, but individuals with disabilities still lacked necessary access to goods and services. Long-experienced access barriers to online education were exacerbated by the abrupt transition of in-person to remote instruction. Finally, public health messaging has been inconsistent and inaccessible, which is unacceptable during a rapidly-evolving crisis. We argue that organizations should create flexible, accessible technology and policies in calm times to be adaptable in times of crisis to serve individuals with diverse needs. © 2020 Owner/Author.",Accessibility; COVID-19; Delivery Services; Disability; Emergency; Online Education; Public Health; Social Media; Twitter,Distance education; Social networking (online); Abrupt transition; Accessibility problems; Health informations; On-line education; People with disabilities; Pick-up and delivery service; Remote instruction; Societal transitions; Public health,Conference Paper,Final,Scopus,2-s2.0-85096952867,Peter,,
"Nakao Y., Sugano Y.",57207916666;7005470045;,Use of Machine Learning by Non-Expert DHH People: Technological Understanding and Sound Perception,2020,ACM International Conference Proceeding Series,,,,,,,1.0,10.1145/3419249.3420157,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123041938&doi=10.1145%2f3419249.3420157&partnerID=40&md5=cd7052dd48538bac479efaf28d899fd0,"Recent advances in machine learning demonstrated its potential in accessibility applications. However, recognition models and their application scenarios are often defined by machine learning (ML) experts and cannot fully capture users' diverse demands with disabilities. In order to open up the full potential of ML for accessibility applications, we have to bridge the gap for non-expert people doubly caused by the technological understanding and their disabilities. In this work, we investigate how non-expert deaf and hard-of-hearing (DHH) people understand ML technologies and design ML-based sound recognition systems. We conduct a workshop study consisting of an ML lecture and an interactive learning session using a sound recognition system. Through observations during the workshop and semi-structured interviews, we clarify that non-expert DHH people start to overcome the knowledge gap. They could obtain a more detailed understanding of ML technology and how to use sounds to train ML models. © 2020 ACM.",Accessibility; Machine Learning; Sound Recognition,Audition; Machine learning; Application scenario; Hard of hearings; Interactive learning; Knowledge gaps; Recognition models; Semi structured interviews; Sound perception; Sound recognition; Human computer interaction,Conference Paper,Final,Scopus,2-s2.0-85123041938,Peter,,
"Guerino G.C., Valentim N.M.C.",57190289913;56429611300;,Usability and User eXperience Evaluation of Conversational Systems: A Systematic Mapping Study,2020,ACM International Conference Proceeding Series,,,,427,436,,4.0,10.1145/3422392.3422421,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099338534&doi=10.1145%2f3422392.3422421&partnerID=40&md5=2f8f0feb8903e714909279ff705ef3f9,"Conversational Systems (CSs) are software that uses the user's voice to perform some action. Before these systems were available to users, they must be evaluated. In this sense, Usability and User eXperience (UX) evaluations contribute to the verification of software quality, since they evaluate several aspects such as efficiency, effectiveness, immersion, and user satisfaction. Therefore, the goal of our Systematic Mapping Study (SMS) is to identify the evaluation technologies (methods, techniques, models, among others) used by researchers and professionals to evaluate Usability and UX of CSs. We selected 39 papers for data extraction and, based on these works, we identified 31 different evaluation technologies. Besides, our SMS extracted the characteristics of technologies, CSs, and empirical studies described in the papers. Our results identify a lack of evaluation technologies of CSs that unite the concepts of Usability and UX and undergo empirical evaluations. Moreover, we observed researchers tend to create their questionnaires according to the needs of the study. Overall, our SMS presents data about the researched topic, describing the gaps, and contributing to the scientific community that evaluates Usability and UX of CSs. © 2020 ACM.",Conversational Systems; Software Quality; Systematic Mapping Study; Usability; User eXperience,Computer software selection and evaluation; Mapping; Software quality; Surveys; Usability engineering; User experience; Verification; Conversational systems; Empirical evaluations; Scientific community; Systematic mapping studies; Usability and user experience evaluation; User experiences (ux); User satisfaction; Verification of softwares; Quality control,Conference Paper,Final,Scopus,2-s2.0-85099338534,Peter,,
"Nasser N., Salah J., Sharaf N., Abdennadher S.",57221234627;57191274411;55055753300;55970641400;,Automatic Lecture Annotation,2020,"Proceedings - Frontiers in Education Conference, FIE",2020-October,,9274231,,,,1.0,10.1109/FIE44824.2020.9274231,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098553047&doi=10.1109%2fFIE44824.2020.9274231&partnerID=40&md5=ca780b3742a10ff36cde0840f0851c7e,"This Research to Practice Full Paper presents an application (AutoNotes) that improves the students' learning outcomes by using natural language processing techniques (NLP) such as speech recognition and keywords extraction. AutoNotes is an android application that converts the speech of the instructor to text in real time during the lecture and then it synchronizes this text to the original lecture slides to produce new slides annotated with the explanation of the instructor. These new lecture slides can be displayed in a classroom. Moreover, it can be accessed by the students on their mobile phones. AutoNotes also provides a feature for the students who like to write their notes with their own way. A focus group was conducted to gather target audience opinions. The results have shown that students liked the idea of AutoNotes. They saw it beneficial for them especially if the original lecture slides are not detailed. They also like the idea of putting the notes with its corresponding slide. To evaluate AutoNotes, a between group design setup of two identical lectures with different groups of participants was conducted. In the first lecture, the lecturer used the original lecture slides. However, in the second one the annotated lecture slides was used. The participants were given several tests to compare their overall learning experiences. The results of the tests reveal that the participants that were exposed to the annotated lecture slides had a higher engagement level and learning gain than the other group exposed to the original lecture slides. Additionally, they had a workload level less than the other group. For the system usability test, the average usability scale that AutoNotes got is 78 which can be considered above average. Finally, the results of the accuracy testing reveal that the accuracy of the speech recognition was 83.2%. The precision and recall metrics were used to evaluate the synchronization of the speech with the slides. The average precision score was 75.6%. However, the average recall score was 51.1%. © 2020 IEEE.",Accuracy; Classroom; Learning; Lecture; Slides; Speech Recognition; Usability,Natural language processing systems; Students; Accuracy testing; Android applications; Engagement levels; Keywords extraction; Learning experiences; NAtural language processing; Precision and recall; System usability; Speech recognition,Conference Paper,Final,Scopus,2-s2.0-85098553047,Peter,,
"Olwal A., Balke K., Votintcev D., Starner T., Conn P., Chinh B., Corda B.",7801329807;57225821940;57220119765;7003469397;57216911128;57197731463;57496782800;,Wearable subtitles: Augmenting spoken communication with lightweight eyewear for all-day captioning,2020,UIST 2020 - Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology,,,3415817,1108,1120,,14.0,10.1145/3379337.3415817,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096997026&doi=10.1145%2f3379337.3415817&partnerID=40&md5=042a33a0f6c6448bb54a19bdc437b0e7,"Mobile solutions can help transform speech and sound into visual representations for people who are deaf or hard-of-hearing (DHH). However, where handheld phones present challenges, head-worn displays (HWDs) could further communication through privately transcribed text, hands-free use, improved mobility, and socially acceptable interactions. Wearable Subtitles is a lightweight 3D-printed proof-of-concept HWD that explores augmenting communication through sound transcription for a full workday. Using a low-power microcontroller architecture, we enable up to 15 hours of continuous use. We describe a large survey (n=501) and three user studies with 24 deaf/hard-of-hearing participants which inform our development and help us refine our prototypes. Our studies and prior research identify critical challenges for the adoption of HWDs which we address through extended battery life, lightweight and balanced mechanical design (54 g), fitting options, and form factors that are compatible with current social norms. © 2020 Owner/Author.",All-day; Assistive technology; Captions; Head-worn displays; Hearing accessibility; Low-power system; Wearables,3D printers; Audition; Surveys; Wearable computers; Critical challenges; Hard of hearings; Head-worn displays; Low-power microcontrollers; Mechanical design; Mobile solutions; Proof of concept; Visual representations; User interfaces,Conference Paper,Final,Scopus,2-s2.0-85096997026,Peter,,
"Mack K., Bragg D., Morris M.R., Bos M.W., Albi I., Monroy-Hernández A.",57219109957;43660991100;8619759600;57200964398;57219607522;23973552900;,Social App Accessibility for Deaf Signers,2020,Proceedings of the ACM on Human-Computer Interaction,4,CSCW2,125,,,,9.0,10.1145/3415196,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094190754&doi=10.1145%2f3415196&partnerID=40&md5=a6c2354c0ef0e9ded5cc373acadae06d,"Social media platforms support the sharing of written text, video, and audio. All of these formats may be inaccessible to people who are deaf or hard of hearing (DHH), particularly those who primarily communicate via sign language, people who we call Deaf signers. We study how Deaf signers engage with social platforms, focusing on how they share content and the barriers they face. We employ a mixed-methods approach involving seven in-depth interviews and a survey of a larger population (n = 60). We find that Deaf signers share the most in written English, despite their desire to share in sign language. We further identify key areas of difficulty in consuming content (e.g., lack of captions for spoken content in videos) and producing content (e.g., captioning signed videos, signing into a phone camera) on social media platforms. Our results both provide novel insights into social media use by Deaf signers and reinforce prior findings on DHH communication more generally, while revealing potential ways to make social media platforms more accessible to Deaf signers. © 2020 ACM.",accessibility; asl; deaf culture; sign language; social media,Audition; Hard of hearings; In-depth interviews; Mixed method; Sign language; Social media; Social media platforms; Written texts; Social networking (online),Article,Final,Scopus,2-s2.0-85094190754,Peter,,
"Li D., Lin K.",35215276400;57218957284;,DHH students’ phoneme repetition awareness in sentence reading,2020,Journal of Deaf Studies and Deaf Education,25,4,,505,516,,1.0,10.1093/deafed/enaa011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090881539&doi=10.1093%2fdeafed%2fenaa011&partnerID=40&md5=a5a5287f60d04312c6254f61f9b4d995,"To examine deaf and hard-of-hearing (DHH) students’ awareness of phoneme repetition in Chinese sentence reading, two experiments were conducted in the self-paced, moving-window reading paradigm. The materials comprised sentences in which Chinese characters that sequentially followed each other shared similar spelling initials and finals in Experiments 1 and 2, respectively. In comparison to hearing participants, DHH participants were more likely to find it more time-consuming to read sentences with, as opposed to without, phoneme repetitions. However, their difficulty in phonological processing seemed to be linked to their weakness at syntactic skilfulness, thus having a negative influence on their reading performance. It is concluded that Chinese DHH college students have developed a phoneme repetition awareness which is different from how hearing college students are aware of phoneme repetitions in Chinese sentence reading. It is implicated that DHH students are able to develop their own skills of phonological information processing in sentence reading as a result of many practices. © The Author(s) 2020. Published by Oxford University Press. All rights reserved.",,awareness; China; comprehension; female; hearing impaired person; human; language; male; phonetics; psychology; reading; young adult; Awareness; China; Comprehension; Female; Humans; Language; Male; Persons With Hearing Impairments; Phonetics; Reading; Young Adult,Article,Final,Scopus,2-s2.0-85090881539,Peter,,
"Koerber R., Jennings M.B.",57192396640;8856456500;,Increasing telephone accessibility for workers with hearing loss: a scoping review with recommendations,2020,International Journal of Audiology,59,10,,727,736,,1.0,10.1080/14992027.2020.1753120,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086845874&doi=10.1080%2f14992027.2020.1753120&partnerID=40&md5=e61d9a1df42735689d3aedeb895db54a,"Objective: A scoping review was undertaken to identify strategies which increase telephone accessibility for workers with hearing loss. Design: The scoping review protocol outlined by the Joanna Brigg’s Institute was used. Terms relating to hearing loss, telephones, and management strategies were searched in CINAHL, MEDLINE, and Web of Science. An additional hand search was also conducted for two journals and two publications from consumer organisations as they were known to publish relevant articles. Inclusion and exclusion criteria were applied to the resulting 1086 texts. Strategies from the selected texts were organised into categories through thematic analyses. Study sample: Eighty-four texts were included in the review. Results: The effective telephone strategies fall into the following categories: amplifying the telephone signal, reducing background noise, listening to the telephone through both ears, accessing text-based supports to understanding telephone speech, using Internet-based telephony, optimising mobile phones, improving telephone skills and communication strategies, and requesting accommodation in the workplace. Conclusion: Strategies exist by which the telephone can be made more accessible to workers with hearing loss. These have the potential to benefit both workers and their employers. © 2020 British Society of Audiology, International Society of Audiology, and Nordic Audiological Society.",accessibility; accommodation; assistive device; Hearing loss; strategy; telephone,hearing impairment; human; noise; speech perception; telephone; Deafness; Hearing Loss; Humans; Noise; Speech Perception; Telephone,Review,Final,Scopus,2-s2.0-85086845874,Peter,,
Romero-Fresco P.,36675476200;,Subtitling Through Speech Recognition: Respeaking,2020,Subtitling Through Speech Recognition: Respeaking,,,,1,194,,7.0,10.4324/9781003073147,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108126227&doi=10.4324%2f9781003073147&partnerID=40&md5=3b5676b4ee39fe230a1370616932f730,"Based on sound research and first-hand experience in the field, Subtitling through Speech Recognition: Respeaking is the first book to present a comprehensive overview of the production of subtitles through speech recognition in Europe. Topics covered include the origins of subtitling for the deaf and hard of hearing, the different methods used to provide live subtitles and the training and professional practice of respeaking around the world. The core of the book is devoted to elaborating an in-depth respeaking course, including the skills required before, during and after the respeaking process. The volume also offers detailed analysis of the reception of respeaking, featuring information about viewers' preferences, comprehension and perception of respoken subtitles obtained with eye-tracking technology. An accompanying DVD features a wealth of video clips and documents designed to illustrate the material in the book and to serve as a basis for the exercises included at the end of each chapter. The working language of the book is English, but the DVD also contains sample material in Dutch, French, Galician, German, Italian and Spanish. Subtitling through Speech Recognition: Respeaking is designed for use as a coursebook for classroom practice or as a handbook for self-learning. It will be of interest to undergraduate and postgraduate students as well as freelance and in-house language professionals. It will also find a reading public among broadcasters, cinema, theatre and museum managers, as well as the deaf and members of deaf associations, who may use the volume to support future campaigns and enhance the quality of the speech-to-text accessibility they provide to their members. © Pablo Romero-Fresco 2011. All rights reserved.",,,Book,Final,Scopus,2-s2.0-85108126227,Peter,,
"Tu J., Lin G., Starner T.",57219172474;57219179781;7003469397;,Conversational greeting detection using captioning on head worn displays versus smartphones,2020,"Proceedings - International Symposium on Wearable Computers, ISWC",,,,84,86,,3.0,10.1145/3410531.3414293,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091559212&doi=10.1145%2f3410531.3414293&partnerID=40&md5=dc49f807d895b080752c993d0b6653b4,"People who are deaf and hard of hearing often have difficulty realizing when someone is attempting to get their attention, especially when mobile. Speech recognition coupled with a head-worn display (HWD) may aid in awareness of when someone calls the user's name. As our intended users are often oversubscribed with experiments, we chose to test non-deaf and hard of hearing subjects while refining our procedures. Preliminary findings from three hearing participants wearing sound masking headphones and performing a mobile task suggest that a HWD display may be faster than, and preferred to, a smartphone for displaying captions for attending to one's name being called. © 2020 Owner/Author.",accessibility; captioning; deaf; head-worn display,Smartphones; Speech recognition; Wearable computers; Hard of hearings; Head-worn displays; Sound masking; Audition,Conference Paper,Final,Scopus,2-s2.0-85091559212,Peter,,
"Schlippe T., Alessai S., El-Taweel G., Wolfel M., Zaghouani W.",42062203500;57221606968;57221596659;8870295600;55151388100;,Visualizing Voice Characteristics with Type Design in Closed Captions for Arabic,2020,"Proceedings - 2020 International Conference on Cyberworlds, CW 2020",,,9240549,196,203,,2.0,10.1109/CW49994.2020.00039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099561159&doi=10.1109%2fCW49994.2020.00039&partnerID=40&md5=c5dd1bf70dcdd9996d12e22ce04cf5e3,"Diversification of fonts in video captions based on the voice characteristics, namely loudness, speed and pauses, can affect the viewer receiving the content. This study evaluates a new method, WaveFont, which visualizes the voice characteristics for captions in an intuitive way. The study was specifically designed to test captions, which aims to add a new experience for Arabic viewers. The results indicate that our visualization is comprehensible and acceptable and provides significant added value-for hearing-impaired and non-hearing impaired participants: Significantly more participants stated that WaveFont improves their watching experience more than standard captions. © 2020 IEEE.",accessibility; Arabic; closed captions; responsive type; speech processing; subtitles; type design; typography,Added values; Hearing impaired; Type designs; Video captions; Voice characteristics; Audition,Conference Paper,Final,Scopus,2-s2.0-85099561159,Peter,,
"Agulló B., Matamala A.",57206483570;25921557700;,Subtitles in virtual reality: Guidelines for the integration of subtitles in 360° content,2020,Ikala,25,3,,643,661,,3.0,10.17533/udea.ikala.v25n03a03,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092072997&doi=10.17533%2fudea.ikala.v25n03a03&partnerID=40&md5=e94f07f6db7e53fb6811d5d993271d28,"Immersive content has become a popular medium for storytelling. This type of content is typically accessed via a head-mounted visual display within which the viewer is located at the center of the action with the freedom to look around and explore the scene. The criteria for subtitle position for immersive media still need to be defined. Guiding mechanisms are necessary for circumstances in which the speakers are not visible and viewers, lacking an audio cue, require visual information to guide them through the virtual scene. The aim of this reception study is to compare different subtitling strategies: always-visible position to fixed-position and arrows to radar. To do this, feedback on preferences, immersion (using the ipq questionnaire) and head movements was gathered from 40 participants (20 hearing and 20 hard of hearing). Results show that always-visible subtitles with arrows are the preferred option. Always-visible and arrows achieved higher scores in the ipq questionnaire than fixed-position and radar. Head-movement patterns show that participants move more freely when the subtitles are always-visible than when they are in a fixed position, meaning that with always-visible subtitles the experience is more realistic, because the viewers do not feel constrained by the implementation of subtitles. © 2020 Universidad de Antioquia.",360° content; Hearing impairment; Immersive media; Subtitles; Subtitling strategies; Virtual reality,,Article,Final,Scopus,2-s2.0-85092072997,Peter,,
"Tracy L.F., Segina R.K., Cadiz M.D., Stepp C.E.",57203655551;57210216186;57218504835;35759791500;,The impact of communication modality on voice production,2020,"Journal of Speech, Language, and Hearing Research",63,9,,2913,2920,,11.0,10.1044/2020_JSLHR-20-00161,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090937859&doi=10.1044%2f2020_JSLHR-20-00161&partnerID=40&md5=cdf66353e4a46c2c136dd0fa37844d6b,"Purpose: Communicating remotely using audio and audiovisual technology is ubiquitous in modern work and social environments. Remote communication is increasing in medicine and in voice therapy delivery, and this evolution may have an impact on speakers’ voices. This study sought to determine whether these communication modalities impact the voice production of typical speakers. Method: The speech acoustics of 12 participants with healthy voices were recorded as they held standardized conversations with a single investigator using three communication modalities: in-person, remote-audio, and remote-audiovisual. Participants rated their vocal effort on a 100-mm visual analog scale. Results: Compared to in-person communication, self-ratings of vocal effort were statistically significantly increased for remote-audiovisual communication; vocal effort during remote-audio and in-person communication were not significantly different. In comparison to in-person communication, vocal intensity and smoothed cepstral peak prominence (CPPS) were statistically significantly higher during remote-audio and remote-audiovisual communication. Effect sizes for CPPS changes were larger than for sound pressure level (SPL), and changes in CPPS and SPL between in-person and remote-audiovisual communication were not significantly correlated. Conclusions: Vocal effort and SPL were increased when using remote-audio and remote-audiovisual communication in comparison to in-person communication. Voice quality was also impacted by technology use, with changes in CPPS that were consistent with, but not fully explained by, increases in SPL. This may impact the telepractice delivery of voice therapy, and further investigation is warranted. © 2020 American Speech-Language-Hearing Association.",,adult; article; clinical article; controlled study; conversation; effect size; female; human; human experiment; male; sound pressure; speech; visual analog scale; voice,Article,Final,Scopus,2-s2.0-85090937859,Peter,,
"Grogorick S., Tauscher J.-P., Heesen N., Castillo S., Magnor M.",57063513000;57194002446;57218672916;56312658200;57191188427;,Stereo Inverse Brightness Modulation for Guidance in Dynamic Panorama Videos in Virtual Reality,2020,Computer Graphics Forum,39,6,,542,553,,1.0,10.1111/cgf.14091,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089968881&doi=10.1111%2fcgf.14091&partnerID=40&md5=b778e41b9138ffe93c2973bbb7c8ddd0,"The peak of virtual reality offers new exciting possibilities for the creation of media content but also poses new challenges. Some areas of interest might be overlooked because the visual content fills up a large portion of viewers' visual field. Moreover, this content is available in 360° around the viewer, yielding locations completely out of sight, making, for example, recall or storytelling in cinematic Virtual Reality (VR) quite difficult. In this paper, we present an evaluation of Stereo Inverse Brightness Modulation for effective and subtle guidance of participants' attention while navigating dynamic virtual environments. The used technique exploits the binocular rivalry effect from human stereo vision and was previously shown to be effective in static environments. Moreover, we propose an extension of the method for successful guidance towards target locations outside the initial visual field. We conduct three perceptual studies, using 13 distinct panorama videos and two VR systems (a VR head mounted display and a fully immersive dome projection system), to investigate (1) general applicability to dynamic environments, (2) stimulus parameter and VR system influence, and (3) effectiveness of the proposed extension for out-of-sight targets. Our results prove the applicability of the method to dynamic environments while maintaining its unobtrusive appearance. © 2020 The Authors. Computer Graphics Forum published by Eurographics - The European Association for Computer Graphics and John Wiley & Sons Ltd",assistive interfaces; immersive VR; interaction; perceptually based rendering; rendering; virtual environments,Binocular vision; Helmet mounted displays; Inverse problems; Luminance; Modulation; Stereo image processing; Stereo vision; Binocular rivalry; Dynamic environments; Dynamic virtual environment; Head mounted displays; Static environment; Target location; Visual content; Visual fields; Virtual reality,Article,Final,Scopus,2-s2.0-85089968881,Peter,,
"Jiang N., Hou F., Jiang X.",36704900900;57218171167;57205176641;,Analytic Versus Holistic Recognition of Chinese Words Among L2 Learners,2020,Modern Language Journal,104,3,,567,580,,11.0,10.1111/modl.12662,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089141982&doi=10.1111%2fmodl.12662&partnerID=40&md5=97ca974e2fd6ee33b5e6e60ca19efdb9,"In studying the relationship between word recognition and reading development, a distinction is made between analytic and holistic processing of words. These strategies are often assessed in a length effect in an alphabetic language or in a stroke-number effect in a logographic language. Analytical processing is associated with a robust length or stroke-number effect while holistic processing is reflected in smaller or a lack of such effects. Research has shown that skilled readers employ holistic processing while less skilled readers rely more on analytical processing. The present study examined analytic versus holistic word recognition among second language learners by comparing learners of Chinese as a second language (CSL) and Chinese native speakers (NSs) in a lexical decision task. Thirty Chinese NSs and 28 CSL learners were tested on 90 disyllabic Chinese words that varied in stroke number from 5 to 27. A robust stroke-number effect was found among CSL participants but not among NS controls. The findings raised a number of theoretical and pedagogical issues in relation to word recognition and reading development among CSL learners and among second language learners in general. © National Federation of Modern Language Teachers Associations",analytic processing; Chinese as a second language; holistic processing; stroke-number effect; word recognition,,Article,Final,Scopus,2-s2.0-85089141982,Peter,,
"Turton L., Souza P., Thibodeau L., Hickson L., Gifford R., Bird J., Stropahl M., Gailey L., Fulton B., Scarinci N., Ekberg K., Timmer B.",57208206957;35577095800;7006267432;7004043266;7102275184;56949957400;56769904000;15118894600;55488717300;13608802400;55641225100;56872714400;,Guidelines for best practice in the audiological management of adults with severe and profound hearing loss,2020,Seminars in Hearing,41,3,,141,245,,4.0,10.1055/s-0040-1714744,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098053349&doi=10.1055%2fs-0040-1714744&partnerID=40&md5=8c617c5465b91c54d69d7c691ac1ddd3,"Individuals with severe to profound hearing loss are likely to present with complex listening needs that require evidence-based solutions. This document is intended to inform the practice of hearing care professionals who are involved in the audiological management of adults with a severe to profound degree of hearing loss and will highlight the special considerations and practices required to optimize outcomes for these individuals. Copyright © 2020 by Thieme Medical Publishers, Inc.,",Cochlear implants; Guideline; Hearing aids; Severe to profound hearing loss,,Review,Final,Scopus,2-s2.0-85098053349,Peter,,
"Montagud M., Boronat F., Pastor J., Marfil D.",35868074700;23388192100;57194784652;57189242570;,Web-based platform for a customizable and synchronized presentation of subtitles in single- and multi-screen scenarios,2020,Multimedia Tools and Applications,79,29-30,,21889,21923,,6.0,10.1007/s11042-020-08955-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085015224&doi=10.1007%2fs11042-020-08955-x&partnerID=40&md5=550b600e79e6d762702171d888ebec1c,"This paper presents a web-based platform for a customized and synchronized presentation of subtitles in both single- and multi-screen scenarios. The platform enables a dynamic user-level customization of subtitles in terms of format (font family, size, color, transparency..) and position, according to the users’ preferences and/or needs. It also allows adjusting the number of subtitle lines to be presented, being able to skip to the corresponding playout position of the beginning of a specific line by clicking on it. Likewise, multiple languages can be simultaneously presented, and a delay offset to the presentation of subtitles can be applied. All these functionalities can also be available on companion devices, by associating them to the session on the main screen. This enables the presentation of subtitles in a synchronized manner with the content on the main screen and their independent customization. The platform provides support for different subtitle formats, as well as for HTML5 and Youtube videos. It includes a module to upload videos and their subtitle files, and to manage playlists. Overall, the platform enables personalized and more engaging consumption experiences, contributing to improve the Quality of Experience (QoE). It can additionally provide benefits in a variety of scenarios, such as language learning, crowded multi-culture and noisy environments. The results from a subjective evaluation study, with the participation of 40 users without accessibility needs, reveal that the platform can provide relevant benefits for the whole spectrum of consumers. In particular, users have been very satisfied with the usability, attractiveness, effectiveness and usefulness of all features of the platform. [Figure not available: see fulltext.]. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Accessibility; Multi-screen; Personalization; QoE; Subtitles; Synchronization,Synchronization; Websites; Customizable; Language learning; Multi screens; Multiple languages; Noisy environment; Quality of experience (QoE); Subjective evaluations; Web based platform; Quality of service,Article,Final,Scopus,2-s2.0-85085015224,Peter,,
"Zendel B.R., Alexander E.J.",6506586820;57211912605;,Autodidacticism and Music: Do Self-Taught Musicians Exhibit the Same Auditory Processing Advantages as Formally Trained Musicians?,2020,Frontiers in Neuroscience,14,,752,,,,6.0,10.3389/fnins.2020.00752,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088914298&doi=10.3389%2ffnins.2020.00752&partnerID=40&md5=c1527082243123431541f5ef7a6b7428,"Multiple studies have demonstrated that musicians have enhanced auditory processing abilities compared to non-musicians. In these studies, musicians are usually defined as having received some sort of formal music training. One issue with this definition is that there are many musicians who are self-taught. The goal of the current study was to determine if self-taught musicians exhibit different auditory enhancements as their formally trained counterparts. Three groups of participants were recruited: formally trained musicians, who received formal music training through the conservatory or private lessons; self-taught musicians, who learned to play music through informal methods, such as with books, videos, or by ear; non-musicians, who had little or no music experience. Auditory processing abilities were assessed using a speech-in-noise task, a passive pitch oddball task done while recording electrical brain activity, and a melodic tonal violation task, done both actively and passively while recording electrical brain activity. For the melodic tonal violation task, formally trained musicians were better at detecting a tonal violation compared to self-taught musicians, who were in turn better than non-musicians. The P600 evoked by a tonal violation was enhanced in formally trained musicians compared to non-musicians. The P600 evoked by an out-of-key note did not differ between formally trained and self-taught musicians, while the P600 evoked by an out-of-tune note was smaller in self-taught musicians compared to formally trained musicians. No differences were observed between the groups for the other tasks. This pattern of results suggests that music training format impacts auditory processing abilities in musical tasks; however, it is possible that these differences arose due to pre-existing factors and not due to the training itself. © Copyright © 2020 Zendel and Alexander.",auditory processing; ERAN; MMN; music training; musicianship; P600; speech-in-noise,adult; article; brain function; case report; clinical article; ear; female; human; human experiment; male; music; musician; noise; pitch; speech; videorecording,Article,Final,Scopus,2-s2.0-85088914298,Peter,,
"Oncins E., Orero P.",55746064200;24921771100;,"No audience left behind, one App fits all: an integrated approach to accessibility services",2020,Journal of Specialised Translation,,34,,192,211,,1.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100756337&partnerID=40&md5=0ef6cfc5eac058d0c7b1372bdc2d80f6,"Most academic research on Audiovisual Translation and specifically on Media Accessibility has focused on content creation. If media content today can be compared to the visible tip of the iceberg, technology represents the unknown hidden part. Accessibility services are epigonic to media content, presenting an intimate relationship between technology for delivery and technology for consumption. Technology allows not only for the production of media content and access services, but also elicits new formats and interactions. This paper focuses on accessibility services delivery. The first part provides a brief on state-of-the-art accessibility apps available in the market today, including emerging trends and market uptake. Section 2 presents audience needs away from the classical medical classification of assistive technologies and an analysis of existing accessibility apps. Section 3 presents a novel solution that is currently available in the market for transmedia storytelling. The new app has been designed to provide personalised access, away from one-size-fits-all services. © 2020 University of Roehampton. All rights reserved.",Accessibility; Distribution; Large audience; Live events; Personalisation,,Article,Final,Scopus,2-s2.0-85100756337,Peter,,
"Tu J., Lin G.E., Starner T.",57219172474;57219179781;7003469397;,Towards an Understanding of Real-time Captioning on Head-worn Displays,2020,"Extended Abstracts - 22nd International Conference on Human-Computer Interaction with Mobile Devices and Services: Expanding the Horizon of Mobile Interaction, MobileHCI 2020",,,3410543,,,,,10.1145/3406324.3410543,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102138310&doi=10.1145%2f3406324.3410543&partnerID=40&md5=a940009a743024ffc34a18ea5510f36b,"Automatic speech recognition has made live captioning possible on mobile devices for people with hearing difficulties. In what situations is it advantageous to display the transcription on a head worn display (HWD) versus a mobile phone? Based on iterative design efforts for an on-going user study, we demonstrate the use of HWDs versus a mobile phone for captioning while a participant's hearing is blocked using noise-canceling headphones. In the past, to compare the efficacy of the Vuzix Blade HWD to a mobile phone, eight participants attempted a toy block assembly task guided by captioning of a live instructor's speech. The HWD had higher mental, physical and overall workload scores than the phone, potentially due to the blurriness of the HWD's image. Re-running the experiment with Google Glass Enterprise Edition 2 HWD (above line-of-sight) with another twelve participants resulted in higher mental, effort, frustration, and overall workload scores than the phone. While Glass has a much sharper image than the Blade, the speech recognition quality was significantly worse. However, nine of the twelve participants stated that they would prefer Glass for the task if the speech recognition was better. Current efforts replace live speech recognition with a simulation of perfect recognition and improve display quality in order to more directly compare captioning on a mobile phone, a line-of-sight HWD, and an above line-of-sight HWD. © 2020 Owner/Author.",accessibility; captions; deaf; hard of hearing; head-worn display; human computer interaction,Audition; Cellular telephones; Glass; Human computer interaction; Speech; Assembly tasks; Automatic speech recognition; Display quality; Head-worn displays; Iterative design; Line of Sight; Noise canceling; Toy blocks; Speech recognition,Conference Paper,Final,Scopus,2-s2.0-85102138310,Peter,,
"Tellex S., Gopalan N., Kress-Gazit H., Matuszek C.",7801643708;55921052400;15762557900;10239806100;,Robots That Use Language,2020,"Annual Review of Control, Robotics, and Autonomous Systems",3,,,25,55,,46.0,10.1146/annurev-control-101119-071628,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117816482&doi=10.1146%2fannurev-control-101119-071628&partnerID=40&md5=6e8669fc2677287a7e30f6950fe4326d,"This article surveys the use of natural language in robotics from a robotics point of view. To use human language, robots must map words to aspects of the physical world, mediated by the robot's sensors and actuators. This problem differs from other natural language processing domains due to the need to ground the language to noisy percepts and physical actions. Here, we describe central aspects of language use by robots, including understanding natural language requests, using language to drive learning about the physical world, and engaging in collaborative dialogue with a human partner. We describe common approaches, roughly divided into learning methods, logic-based methods, and methods that focus on questions of human-robot interaction. Finally, we describe several application domains for language-using robots. Copyright © 2020 by Annual Reviews. All rights reserve.",Dialogue; Grounding; Language; Learning; Logic; Robots,,Article,Final,Scopus,2-s2.0-85117816482,Peter,,
Zdenek S.,24072575200;,Transforming access and inclusion in composition studies and technical communication,2020,College English,82,5,,536,544,,9.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094974357&partnerID=40&md5=f78f2853fe93f52d814c38a5bcf95859,[No abstract available],,,Review,Final,Scopus,2-s2.0-85094974357,Peter,,
"Cutler R., Mehran R., Johnson S., Zhang C., Kirk A., Whyte O., Kowdle A.",7202057388;14023328000;57195285279;55885032600;7102479358;36467394100;35173101000;,Multimodal Active Speaker Detection and Virtual Cinematography for Video Conferencing,2020,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings",2020-May,,9053171,4527,4531,,4.0,10.1109/ICASSP40776.2020.9053171,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089227558&doi=10.1109%2fICASSP40776.2020.9053171&partnerID=40&md5=8ef919a90f152db5392b4ed88aced041,"Active speaker detection (ASD) and virtual cinematography (VC) can significantly improve the experience of a video conference by automatically panning, tilting and zooming of a camera: subjectively users rate an expert video cinematographer significantly higher than the unedited video. We describe a new automated ASD and VC that performs within 0.3 MOS of an expert cinematographer based on subjective ratings with a 1-5 scale. This system uses a 4K wide-FOV camera, a depth camera, and a microphone array, extracts features from each modality and trains an ASD using an AdaBoost machine learning system that is very efficient and runs in real-time. A VC is similarly trained using machine learning. To avoid distracting the room participants the system has no moving parts-the VC works by cropping and zooming the 4K wide-FOV video stream. The system was tuned and evaluated using extensive crowdsourcing techniques and evaluated on a system with N=100 meetings, each 25 minutes in length. © 2020 IEEE.",Active speaker detection; computer vision; crowdsourcing; machine learning; multimodal fusion; sound source localization; video conferencing; virtual cinematography,Adaptive boosting; Audio signal processing; Cameras; Machine learning; Speech communication; Speech recognition; Depth camera; FOV cameras; Microphone arrays; Moving parts; Multi-modal; Speaker detection; Subjective rating; Virtual cinematography; Video conferencing,Conference Paper,Final,Scopus,2-s2.0-85089227558,Peter,,
"Mirzaei M., Kán P., Kaufmann H.",57215514317;55584917500;7203004662;,EarVR: Using Ear Haptics in Virtual Reality for Deaf and Hard-of-Hearing People,2020,IEEE Transactions on Visualization and Computer Graphics,26,5,8998298,2084,2093,,9.0,10.1109/TVCG.2020.2973441,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079598821&doi=10.1109%2fTVCG.2020.2973441&partnerID=40&md5=8efa09c9a0d37074947ca4206cf22433,"Virtual Reality (VR) has a great potential to improve skills of Deaf and Hard-of-Hearing (DHH) people. Most VR applications and devices are designed for persons without hearing problems. Therefore, DHH persons have many limitations when using VR. Adding special features in a VR environment, such as subtitles, or haptic devices will help them. Previously, it was necessary to design a special VR environment for DHH persons. We introduce and evaluate a new prototype called 'EarVR' that can be mounted on any desktop or mobile VR Head-Mounted Display (HMD). EarVR analyzes 3D sounds in a VR environment and locates the direction of the sound source that is closest to a user. It notifies the user about the sound direction using two vibro-motors placed on the user's ears. EarVR helps DHH persons to complete sound-based VR tasks in any VR application with 3D audio and a mute option for background music. Therefore, DHH persons can use all VR applications with 3D audio, not only those applications designed for them. Our user study shows that DHH participants were able to complete a simple VR task significantly faster with EarVR than without. The completion time of DHH participants was very close to participants without hearing problems. Also, it shows that DHH participants were able to finish a complex VR task with EarVR, while without it, they could not finish the task even once. Finally, our qualitative and quantitative evaluation among DHH participants indicates that they preferred to use EarVR and it encouraged them to use VR technology more. © 2020 IEEE.",3D audio; deaf and hard-of-hearing; haptic; sound localization; vibrotactile; Virtual reality,Audio acoustics; Computer hardware; Haptic interfaces; Helmet mounted displays; Job analysis; Sound reproduction; Three dimensional displays; Virtual reality; 3D audio; Auditory systems; haptic; Hard of hearings; Resists; Sound localization; Task analysis; Vibrotactile; Audition; adolescent; adult; computer graphics; computer interface; female; hearing impairment; human; male; middle aged; physiology; sound detection; task performance; vibration; virtual reality; young adult; Adolescent; Adult; Computer Graphics; Deafness; Female; Humans; Male; Middle Aged; Smart Glasses; Sound Localization; Task Performance and Analysis; User-Computer Interface; Vibration; Virtual Reality; Young Adult,Article,Final,Scopus,2-s2.0-85079598821,Peter,,
"Seita M., Huenerfauth M.",57192541826;12240800100;,Deaf individuals' views on speaking behaviors of hearing peers when using an automatic captioning app,2020,Conference on Human Factors in Computing Systems - Proceedings,,,3383083,,,,9.0,10.1145/3334480.3383083,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090236774&doi=10.1145%2f3334480.3383083&partnerID=40&md5=a84576c9f052f8e4ef4e367d0af94584,"As automatic speech recognition (ASR) becomes more accurate, many deaf and hard-of-hearing (DHH) individuals are interested in ASR-based mobile applications to facilitate in-person communication with hearing peers. We investigate DHH users' preferences regarding the behaviors of the hearing person in this context. Using an ASR-based captioning app, eight Deaf/deaf participants held short conversations, with a hearing actor who exhibited certain behaviors, e.g. speaking quietly/loudly or slowly/quickly. Participants indicated some of the hearing individual's behaviors were more influential as to their subjective impression of the communication efficacy. We also found that these behaviors differed in how noticeable they were to the Deaf participants. This study provides guidance, from a Deaf perspective, about the types of behaviors hearing users should ideally exhibit in this context, motivating a focus on such behaviors in future design or evaluation of ASR-based communication apps. © 2020 Owner/Author.",Accessibility; Automatic speech recognition; Deaf and hard of hearing; Speaking behavior,Human engineering; Speech communication; Speech recognition; Automatic speech recognition; Future designs; Hard of hearings; Mobile applications; Subjective impressions; Audition,Conference Paper,Final,Scopus,2-s2.0-85090236774,Peter,,
Seita M.,57192541826;,Designing automatic speech recognition technologies to improve accessibility for deaf and hard-of-hearing people in small group meetings,2020,Conference on Human Factors in Computing Systems - Proceedings,,,3375039,,,,1.0,10.1145/3334480.3375039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090205028&doi=10.1145%2f3334480.3375039&partnerID=40&md5=e2be90f79c35dd14a41037d765126f98,"Deaf and hard of hearing (DHH) individuals face several barriers to communication in the workplace, particularly in small-group meetings with their hearing peers. The impromptu nature of these meetings makes scheduling sign-language interpreting or professional captioning services difficult. Recent advances in Automatic Speech Recognition (ASR) technology could help remove some of these barriers that prevent DHH people from becoming involved in group meetings. However, ASR is still imperfect, and it contains errors in its output text in many real-world conversation settings. My research proposes to investigate whether there are benefits in using ASR technology to aid understanding and communication among DHH and hearing individuals. My dissertation research will evaluate the effectiveness of using ASR in small group meetings (through empirical studies with DHH and hearing participants), as well as develop guidelines for system design to encourage hearing participants to communicate and speak more clearly. © 2020 Owner/Author.",Accessibility; Automatic speech recognition; Communication; Deaf and hard of hearing; Small group discussions; Speaking behavior; User interfaces,Audition; Human engineering; Automatic speech recognition; Automatic Speech Recognition Technology; Empirical studies; Hard of hearings; Real-world; Sign language interpreting; Speech recognition,Conference Paper,Final,Scopus,2-s2.0-85090205028,Peter,,
"Goodman S., Kirchner S., Guttman R., Jain D., Froehlich J., Findlater L.",57219114076;57217590440;57209400148;57014317900;7101665384;10040303000;,Evaluating Smartwatch-based Sound Feedback for Deaf and Hard-of-hearing Users Across Contexts,2020,Conference on Human Factors in Computing Systems - Proceedings,,,3376406,,,,9.0,10.1145/3313831.3376406,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091291793&doi=10.1145%2f3313831.3376406&partnerID=40&md5=4d3dd3a89d5eeacfa389d57a4674949c,"We present a qualitative study with 16 deaf and hard of hearing (DHH) participants examining reactions to smartwatch-based visual + haptic sound feedback designs. In Part 1, we conducted a Wizard-of-Oz (WoZ) evaluation of three smartwatch feedback techniques (visual alone, visual + simple vibration, and visual + tacton) and investigated vibrational patterns (tactons) to portray sound loudness, direction, and identity. In Part 2, we visited three public or semi-public locations where we demonstrated sound feedback on the smartwatch in situ to examine contextual influences and explore sound filtering options. Our findings characterize uses for vibration in multimodal sound awareness, both for push notification and for immediately actionable sound information displayed through vibrational patterns (tactons). In situ experiences caused participants to request sound filtering - particularly to limit haptic feedback - as a method for managing soundscape complexity. Additional concerns arose related to learnability, possibility of distraction, and system trust. Our findings have implications for future portable sound awareness systems. © 2020 ACM.",deaf and hard of hearing; smartwatches; sound awareness,Audition; Human engineering; Awareness systems; Feedback techniques; Haptic feedbacks; Hard of hearings; Implications for futures; Learnability; Qualitative study; Sound feedback; Wearable computers,Conference Paper,Final,Scopus,2-s2.0-85091291793,Peter,,
"Alonzo O., Seita M., Glasser A., Huenerfauth M.",57215303209;57192541826;57195128152;12240800100;,Automatic Text Simplification Tools for Deaf and Hard of Hearing Adults: Benefits of Lexical Simplification and Providing Users with Autonomy,2020,Conference on Human Factors in Computing Systems - Proceedings,,,3376563,,,,11.0,10.1145/3313831.3376563,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091275073&doi=10.1145%2f3313831.3376563&partnerID=40&md5=398994e3355b88486ca99cc834ee0f61,"Automatic Text Simplification (ATS), which replaces text with simpler equivalents, is rapidly improving. While some research has examined ATS reading-assistance tools, little has examined preferences of adults who are deaf or hard-of-hearing (DHH), and none empirically evaluated lexical simplification technology (replacement of individual words) with these users. Prior research has revealed that U.S. DHH adults have lower reading literacy on average than their hearing peers, with unique characteristics to their literacy profile. We investigate whether DHH adults perceive a benefit from lexical simplification applied automatically or when users are provided with greater autonomy, with on-demand control and visibility as to which words are replaced. Formative interviews guided the design of an experimental study, in which DHH participants read English texts in their original form and with lexical simplification applied automatically or on-demand. Participants indicated that they perceived a benefit form lexical simplification, and they preferred a system with on-demand simplification. © 2020 ACM.",autonomy; lexical simplification; people who are deaf or hard of hearing; reading assistance,Human engineering; Hard of hearings; On demands; Reading literacies; Simplification technology; Audition,Conference Paper,Final,Scopus,2-s2.0-85091275073,Peter,,
"Kurzhals K., Göbel F., Angerbauer K., Sedlmair M., Raubal M.",55390097400;55303168800;57056390800;24802477200;23390593200;,A View on the Viewer: Gaze-Adaptive Captions for Videos,2020,Conference on Human Factors in Computing Systems - Proceedings,,,3376266,,,,11.0,10.1145/3313831.3376266,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091270227&doi=10.1145%2f3313831.3376266&partnerID=40&md5=a94f26cfb970b8bfd297bd1def9d12f1,"Subtitles play a crucial role in cross-lingual distribution of multimedia content and help communicate information where auditory content is not feasible (loud environments, hearing impairments, unknown languages). Established methods utilize text at the bottom of the screen, which may distract from the video. Alternative techniques place captions closer to related content (e.g., faces) but are not applicable to arbitrary videos such as documentations. Hence, we propose to leverage live gaze as indirect input method to adapt captions to individual viewing behavior. We implemented two gaze-adaptive methods and compared them in a user study (n=54) to traditional captions and audio-only videos. The results show that viewers with less experience with captions prefer our gaze-adaptive methods as they assist them in reading. Furthermore, gaze distributions resulting from our methods are closer to natural viewing behavior compared to the traditional approach. Based on these results, we provide design implications for gaze-adaptive captions. © 2020 ACM.",eye tracking; gaze input; gaze-responsive display; multimedia; subtitles; video captions,Human engineering; Adaptive methods; Cross-lingual; Design implications; Hearing impairments; Input methods; Multimedia contents; Related content; Traditional approaches; Audition,Conference Paper,Final,Scopus,2-s2.0-85091270227,Peter,,
"Berke L., Seita M., Huenerfauth M.",57200494669;57192541826;12240800100;,Deaf and hard-of-hearing users' prioritization of genres of online video content requiring accurate captions,2020,"Proceedings of the 17th International Web for All Conference, W4A 2020",,,,,,,7.0,10.1145/3371300.3383337,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086812635&doi=10.1145%2f3371300.3383337&partnerID=40&md5=ebffa44dd6618736aff734bebe9ac149,"Online video is an important information source, yet its pace of growth, including user-submitted content, is so rapid that automatic captioning technologies are needed to make content accessible for people who are Deaf or Hard-of-Hearing (DHH). To support future creation of a research dataset of online videos, we must prioritize which genres of online video content DHH users believe are of greatest importance to be accurately captioned. Our first contribution is to validate that the Best-Worst Scaling (BWS) methodology is able to accurately gather judgments on this topic by conducting an in-person study with 25 DHH users, using a card-sorting methodology to rank the importance for various YouTube genres of online video to be accurately captioned. Our second contribution is to identify video genres of highest captioning importance via an online survey with 151 DHH individuals, and those participants highly ranked: News and Politics, Education, and Technology and Science. © 2020 ACM.",captioning; deaf and hard-of-hearing; genres; video,Surveys; Video recording; Best-worst scaling; Card-sorting; Hard of hearings; Information sources; Online surveys; Online video; Prioritization; YouTube; Audition,Conference Paper,Final,Scopus,2-s2.0-85086812635,Peter,,
"Conn P., Gotfrid T., Zhao Q., Celestine R., Mande V., Shinohara K., Ludi S., Huenerfauth M.",57216911128;57192541558;57216909815;57192845777;57216908573;16239695600;57204223407;12240800100;,Understanding the Motivations of Final-year Computing Undergraduates for Considering Accessibility,2020,ACM Transactions on Computing Education,20,2,15,,,,7.0,10.1145/3381911,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085251196&doi=10.1145%2f3381911&partnerID=40&md5=241f34cdeec19a3dad0e743a09fd0162,"We investigate the degree to which undergraduate computing students in a United States university consider accessibility several years after instruction. Prior work has found that cultural and ethical norms become ingrained early in STEM professionals' careers; so, we focus on students approaching graduation and after an internship experience, who are just getting started in their career. In semi-structured interviews, a majority of these final-year computing students (14 of 16) indicated that they were not motivated to improve their skills in accessibility, attributing this to not being required to consider accessibility in subsequent work or classes, not seeing accessibility as an essential skill in their profession, and challenges due to a learn-it-on-your-own approach in computing. Participants suggested instructional methods and topics that they believed would have better prepared them for considering accessibility. A survey of 114 additional final-year students revealed similar themes, including that students did not personally view accessibility training as essential career preparation. Prior research has largely focused on evaluating short-term changes in students' knowledge after an educational intervention. Therefore, by focusing on students several years after an intervention, this work highlights lingering barriers for university programs in promoting accessibility among rising computing professionals. © 2020 ACM.",Accessibility; computing; education; pedagogy,Computer programming; Computer science; Career preparations; Educational intervention; Instructional methods; Semi structured interviews; Short term; University programs; Students,Article,Final,Scopus,2-s2.0-85085251196,Peter,,
"Hong J., Vaing C., Kacorri H., Findlater L.",56154510700;57220770068;35487798800;10040303000;,Reviewing Speech Input with Audio,2020,ACM Transactions on Accessible Computing,13,1,3382039,,,,1.0,10.1145/3382039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097583132&doi=10.1145%2f3382039&partnerID=40&md5=fbce8cc9763cd03c790c51161976547a,"Speech input is a primary method of interaction for blind mobile device users, yet the process of dictating and reviewing recognized text through audio only (i.e., without access to visual feedback) has received little attention. A recent study found that sighted users could identify only about half of automatic speech recognition (ASR) errors when listening to text-to-speech output of the ASR results. Blind screen reader users, in contrast, may be better able to identify ASR errors through audio due to their greater use of speech interaction and increased ability to comprehend synthesized speech. To compare the experiences of blind and sighted users with speech input and ASR errors, as well as to compare their ability to identify ASR errors through audio-only interaction, we conducted a lab study with 12 blind and 12 sighted participants. The study included a semi-structured interview portion to qualitatively understand experiences with ASR, followed by a controlled speech input task to quantitatively compare participants' ability to identify ASR errors in their dictated text. Findings revealed differences between blind and sighted participants in terms of how they use speech input and their level of concern for ASR errors (e.g., blind users were more highly concerned). In the speech input task, blind participants were able to identify only 40% of ASR errors, which, counter to our hypothesis, was not significantly different from sighted participants' performance. In depth analysis of speech input, ASR errors, and strategy of identifying ASR errors scrutinized how participants entered a text with speech input and reviewed it. Our findings indicate the need for future work on how to support blind users in confidently using speech input to generate accurate, error-free text. © 2020 ACM.",ASR errors; blind; dictation; Speech input; synthesized speech; text entry; visual impairment,Character recognition; Errors; Speech; Visual communication; Automatic speech recognition; In-depth analysis; Mobile device users; Semi structured interviews; Speech interaction; Synthesized speech; Text to speech; Visual feedback; Speech recognition,Review,Final,Scopus,2-s2.0-85097583132,Peter,,
"Hintz F., Meyer A.S., Huettig F.",55342275800;7401839760;24337902700;,Activating words beyond the unfolding sentence: Contributions of event simulation and word associations to discourse reading,2020,Neuropsychologia,141,,107409,,,,4.0,10.1016/j.neuropsychologia.2020.107409,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081026629&doi=10.1016%2fj.neuropsychologia.2020.107409&partnerID=40&md5=f7e61299307e5e510bbd6bdcbab5ac9c,"Previous studies have shown that during comprehension readers activate words beyond the unfolding sentence. An open question concerns the mechanisms underlying this behavior. One proposal is that readers mentally simulate the described event and activate related words that might be referred to as the discourse further unfolds. Another proposal is that activation between words spreads in an automatic, associative fashion. The empirical support for these proposals is mixed. Therefore, theoretical accounts differ with regard to how much weight they place on the contributions of these sources to sentence comprehension. In the present study, we attempted to assess the contributions of event simulation and lexical associations to discourse reading, using event-related brain potentials (ERPs). Participants read target words, which were preceded by associatively related words either appearing in a coherent discourse event (Experiment 1) or in sentences that did not form a coherent discourse event (Experiment 2). Contextually unexpected target words that were associatively related to the described events elicited a reduced N400 amplitude compared to contextually unexpected target words that were unrelated to the events (Experiment 1). In Experiment 2, a similar but reduced effect was observed. These findings support the notion that during discourse reading event simulation and simple word associations jointly contribute to language comprehension by activating words that are beyond contextually congruent sentence continuations. © 2020 Elsevier Ltd",Associations; Discourse reading; ERPs; Event simulation,adult; article; comprehension; controlled study; evoked response; female; human; human experiment; language; male; reading; simulation; theoretical study; electroencephalography; evoked response; semantics; Comprehension; Electroencephalography; Evoked Potentials; Female; Humans; Language; Male; Reading; Semantics,Article,Final,Scopus,2-s2.0-85081026629,Peter,,
"Dinis M.G., Eusébio C., Breda Z.",57467304400;47961796400;36631560700;,Assessing social media accessibility: the case of the Rock in Rio Lisboa music festival,2020,International Journal of Event and Festival Management,11,1,,26,46,,5.0,10.1108/IJEFM-02-2019-0012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079458641&doi=10.1108%2fIJEFM-02-2019-0012&partnerID=40&md5=5d0307c1893f6122a8592042ceec5ed6,"Purpose: This paper aims to present a framework to analyse whether information published on social media is accessible for people with disabilities (PwD), namely, visual and hearing disabilities, with an application to a music festival. Design/methodology/approach: The methodology used in this exploratory study consists of establishing a recommended framework to assess social media accessibility for PwD, especially for people with visual and hearing disabilities (PwVHD), and analyse, through an observation grid, if the information published on the official pages of the “Rock in Rio Lisboa” music festival on Facebook, Instagram, Twitter and YouTube is accessible for this target audience. Findings: The results indicate that, although the Rock in Rio Lisboa music festival is promoted as a festival for all, posts on social media are not accessible for people with visual and/or hearing disabilities and do not meet most of the defined parameters established in the proposed assessment framework. Originality/value: Social media accessibility has not been analysed in previous research in the tourism context. This paper aims to fill in the void by establishing criteria and parameters that can serve as a basis for a framework for accessibility assessment in social media for PwVHD. © 2019, Emerald Publishing Limited.",Accessibility; Accessible tourism; Information; Music festivals; People with disabilities; Rock in Rio Lisboa; Social media,,Article,Final,Scopus,2-s2.0-85079458641,Peter,,
Brinkley J.,55604856000;,Participation at what cost? teaching accessibility using participatory design: An experience report,2020,SIGCSE 2020 - Proceedings of the 51st ACM Technical Symposium on Computer Science Education,,,,114,120,,1.0,10.1145/3328778.3366931,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081596009&doi=10.1145%2f3328778.3366931&partnerID=40&md5=652969ad76f5535aacb99a5045c0b6a2,"As institutions respond to market demand in their training of the next generation of technology designers, there is an increasing awareness of the need to add accessibility to computer science and informatics curricula. Advocates have suggested three strategies for including accessibility and discussions of disability in courses: changing a lecture, adding a lecture or adding a new course. In this paper we report on our experiences with the latter; incorporating accessibility within two new graduate and undergraduate inclusive design courses taught concurrently. We found that while the use of participatory design was decidedly effective in supporting student learning and ameliorating ableist attitudes, creating and managing teams comprised of students and visually impaired co-designers proved challenging. Despite these challenges, overall, students demonstrated steady growth in their grasp of inclusive design concepts as they tackled accessibility challenges through a series of mobility-related group projects. Efficiencies were also realized through the concurrent teaching of both courses though the pace of course deliverables proved challenging at times for undergraduates. We argue that a review of our experience may help others interested in teaching accessibility related courses, specifically in course design and execution. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Accessibility; Assistive technology; Education; Participatory design,Education; Education computing; Engineering education; Students; Teaching; Accessibility; Assistive technology; Experience report; Group projects; Inclusive design; Participatory design; Student learning; Visually impaired; Curricula,Conference Paper,Final,Scopus,2-s2.0-85081596009,Peter,,
"Shinohara K., Jacobo N., Pratt W., Wobbrock J.O.",16239695600;57214126998;7201578006;6603152369;,Design for social accessibility method cards: Engaging users and reflecting on social scenarios for accessible design,2020,ACM Transactions on Accessible Computing,12,4,17,,,,10.0,10.1145/3369903,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078233632&doi=10.1145%2f3369903&partnerID=40&md5=ecd8a8f3c86601bcd077935a98bfa404,"This article is an extended version of our 2018 ASSETS paper entitled, “Incorporating Social Factors in Accessible Design.” In our ASSETS paper, we demonstrated the viability of the Design for Social Accessibility perspective through a series of user-centered workshops with professional designers. With this expanded article, we conducted a follow-up research study with a user-centered design course that examined the use of Design for Social Accessibility Method Cards over a longer design cycle, specifically as the method and cards contributed to a term-long project, rather than just a workshop. We also offer a new analysis on work leading to the development of Design for Social Accessibility, with a focus on how practical considerations in the design process influence how designers engage accessible design. We found that the concrete and real-life scenarios in the Design for Social Accessibility Method Cards helped mediate useful interactions between student designers and deaf and hard-of-hearing users. In addition, we identified how practical choices in investigating strategies for socially accessible design enabled designers to center disabled perspectives. The contributions of this work'when added to the findings of our ASSETS 2018 paper on incorporating social factors'demonstrate the viability of Design for Social Accessibility by providing: (1) empirical data showing that designers can use the Design for Social Accessibility perspective and method cards to generate accessible designs and appropriately engage deaf and hard-of-hearing users to incorporate social considerations; and (2) a summative analysis highlighting practical steps for how designers can use the Design for Social Accessibility perspective and methods cards to create accessible designs. © 2019 Association for Computing Machinery.",Accessibility; Design workshops; User-centered design,Audition; Curricula; Accessibility; Design workshops; Extended versions; Hard of hearings; Professional designers; Research studies; Social scenarios; Summative analysis; User centered design,Article,Final,Scopus,2-s2.0-85078233632,Peter,,
Aleksandrowicz P.,57204371920;,Can subtitles for the deaf and hard-of-hearing convey the emotions of film music? A reception study,2020,Perspectives: Studies in Translation Theory and Practice,28,1,,58,72,,4.0,10.1080/0907676X.2019.1631362,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067868222&doi=10.1080%2f0907676X.2019.1631362&partnerID=40&md5=fda72ba177142335507ab98197cb724f,"The present paper describes a reception study into subtitling music in subtitles for the d/Deaf and hard-of-hearing. The study makes use of a database of film fragments that were selected and tested for eliciting a specific emotion. The research involves two highly emotional clips from the database as well as one control clip. The three excerpts were shown to a control group of 26 hearing people. Then, three versions of subtitles were added to each clip–one with music description, another with the composition title and the name of the performer, and another one with no information on the music. Subsequently, the fragments were shown to a test group of 60 d/Deaf and hard-of-hearing viewers divided into three groups. The participants reported on the emotions they felt by filling a Differential Emotions Scale questionnaire. For every emotion the clips were supposed to elicit, the results for each subtitling method are very similar to each other, suggesting that either method of subtitling music is equally effective in an emotional scene. Moreover, the emotion intensity the d/Deaf felt is also very close to the intensity experienced by the hard-of-hearing and the hearing control group. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.",deaf; emotion; hard-of-hearing; media accessibility; music; SDH; Subtitles for the deaf and hard-of-hearing,,Article,Final,Scopus,2-s2.0-85067868222,Peter,,
"Bosch-Baliarda M., Orero P., Soler-Vilageliu O.",57204854558;24921771100;57222314882;,Towards recommendations for TV sign language interpretation1,2020,SKASE Journal of Translation and Interpretation,13,2,,38,57,,1.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117444381&partnerID=40&md5=a214b1e087c9f77a758008435cd5af93,"Sign language interpreting (SLI) on TV is still in need of basic research to support video production guidelines, a complex matter given the variety of sign language styles and screen layouts adopted by international broadcasters. The current paper aims to draft recommendations regarding the formal parameters for displaying SLI on TV. First, it offers an overview of current SLI access services. Second, it proposes a set of variables to be further studied. Third, it reports on feedback gathered from stakeholders. The article concludes with a list of recommendations that may be applied by broadcasters offering SLI access services. © 2020 Slovak Association for the Study of English. All rights reserved.",Accessibility; Audiovisual translation; Deaf TV service users; Media interpreting; Sign language interpreting,,Article,Final,Scopus,2-s2.0-85117444381,Peter,,
"Bosch-Baliarda M., soler-Vilageliu O., orero P.",57204854558;57222314882;24921771100;,SIGN LANGUAGE INTERPRETING ON TV: A RECEPTION STUDY OF VISUAL SCREEN EXPLORATION IN DEAF SIGNING USERS1,2020,Monografias de Traduccion e Interpretacion (MonTI),,12,,108,143,,8.0,10.6035/MonTI.2020.12.04,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105312906&doi=10.6035%2fMonTI.2020.12.04&partnerID=40&md5=af9a4b83377c25ff4306f780165198ff,"We studied how sign language users responded to a screen composition including a larger screen for the content and a smaller screen for the sign language interpreter. 32 deaf users participated in this experiment, watching four similar clips with four different screen compositions. We registered the pattern of screen exploration with Eye Tracker, and we assessed content recall with two questionnaires. Our results show that sign language users mainly look at the sign language interpreter screen. Participants tend to look more often and for longer time at the SLI side closer to the main screen. Results are interpreted in terms of perceptual strategies developed by Sign Language users. © 2020. All Rights Reserved.",Access in HBBTV; Accessibilitat en la televisió connectada; Accessibilitat per a sords; Accessibility for the deaf; Eye-tracking; Interpretació en llengua de signes; Moviments oculars; Qualitat dels serveis; Service quality; Sign language interpreting,,Article,Final,Scopus,2-s2.0-85105312906,Peter,,
"Luciá M.J., Revuelta P., Garciá Á., Ruiz B., Vergaz R., Cerdán V., Ortiz T.",57215432646;57222475257;57222483512;6507597856;6602783927;57220204764;7005899324;,Vibrotactile Captioning of Musical Effects in Audio-Visual Media as an Alternative for Deaf and Hard of Hearing People: An EEG Study,2020,IEEE Access,8,,,190873,190881,,3.0,10.1109/ACCESS.2020.3032229,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102830708&doi=10.1109%2fACCESS.2020.3032229&partnerID=40&md5=afcf664176731fb7ed32fc995f859609,"Standard captioning for the deaf and hard of hearing people cannot transmit the emotional information that music provides in support of the narrative in audio-visual media. We explore an alternative method using vibrotactile stimulation as a possible channel to transmit the emotional information contained in an audio-visual soundtrack and, thus, elicit a greater emotional reaction in hearing-impaired people. To achieve this objective, we applied two one-minute videos that were based on image sequences that were unassociated with dramatic action, maximizing the effect of the music and vibrotactile stimuli. While viewing the video, using EEG we recorded the brain activity of 9 female participants with normal hearing, and 7 female participants with very severe and profound hearing loss. The results show that the same brain areas are activated in participants with normal hearing watching the video with the soundtrack, and in participants with hearing loss watching the same video with a soft and rhythmic vibrotactile stimulation on the palm and fingertips, although in different hemispheres. These brain areas (auditory cortex, superior temporal cortex, medial frontal cortex, inferior frontal gyrus, superior temporal pole and insula) have been consistently reported as areas involved in the emotional perception of music. We conclude that vibrotactile stimuli can generate cortex activation while watching audio-visual media in a similar way to sound. Thus, a further in-depth study of the possibilities of these stimuli can contribute to an alternative subtitling channel for enriching the audiovisual experience of hearing-impaired people. © 2021 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.",Accessibility; Audio-visual; Captions; Electroencephalography; Hearing impairment; Music emotion recognition; Vibrotactile,Audio acoustics; Behavioral research; Brain; Sound recording; Audio-visual media; Auditory cortex; Cortex activation; Emotional information; Emotional reactions; Hard of hearings; Hearing impaired; Inferior frontal gyrus; Audition,Article,Final,Scopus,2-s2.0-85102830708,Peter,,
Abdel Latif M.M.M.,56075027700;,Translation and interpreting assessment research,2020,New Frontiers in Translation Studies,,,,61,84,,1.0,10.1007/978-981-15-8550-0_4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100833368&doi=10.1007%2f978-981-15-8550-0_4&partnerID=40&md5=839bcd9278b333639813ba4e32142e4d,"This chapter highlights translation and interpreting assessment research. The chapter covers seven key areas of this translator and interpreter education research type. These areas are: surveying translation and interpreting assessment practices, validating translation/interpreting tests, identifying the difficulty level of the source text, developing performance assessment rubrics, examining rating practices and testing conditions, developing translation/interpreting motivational scales, and investigating user evaluation/reception. In the sections covering these areas, the author discusses the main research issues addressed and highlights exemplary studies representing them. Based on the overview given in this chapter, it is generally concluded that more issues have been researched on assessing interpreting than translation. © Springer Nature Singapore Pte Ltd. 2020.",Interpreting assessment research; Interpreting testing; Translation assessment research; Translation research; Translation scales; Translation testing,,Book Chapter,Final,Scopus,2-s2.0-85100833368,Peter,,
Attardo S.,6701698312;,The linguistics of humor an introduction,2020,The Linguistics of Humor: An Introduction,,,,1,465,,19.0,10.1093/oso/9780198791270.001.0001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097786859&doi=10.1093%2foso%2f9780198791270.001.0001&partnerID=40&md5=eef3dfd3f5be9081eb1742af644a2ed7,"This book is the first comprehensive systematic introduction to the linguistics of humor. Assuming no background in humor studies at all, and an elementary knowledge of linguistics, all the terminology and conceptual apparatus of humor studies are introduced, as well as all the linguistic concepts necessary to understand the most up-to-date formulations in the linguistics and applied linguistics of humor. The book is not limited to the theoretical linguistic analyses of humor (for example the General Theory of Verbal humor or the Isotopy Disjunction Model), but has a broad approach encompassing pragmatics, conversation and discourse analysis, ethnomethodology, interactionist and variationist sociolinguistics. Chapters on puns, on the main theories of humor, the semiotics of humor, and on the incongruity-resolution model elucidate the foundations of humor studies, while chapters on the performance of humor, on humor in conversation and discourse, provide the first-ever in-depth discussion and synthesis of the field of the applied linguistics of humor. Chapters on the translation of humor, and on humor in the classroom and in literature broaden the discussion to applications in fields other than linguistics. For the first time ever in a discussion of the linguistics of humor all the fields of linguistics, theoretical and applied alike are given equal treatment and theoretical importance. Thus this book is both a summary of the acquired knowledge about humor and linguistics and a proposal to unify most of the strands of research in a coherent vision. © Salvatore Attardo 2020.",Conversation analysis; Discourse analysis; Ethnomethodology; Humor; Interactional sociolinguistics; Pragmatics; Semantics; Sociolinguistics; Teaching; Translation; Variationism,,Book,Final,Scopus,2-s2.0-85097786859,Peter,,
"Shahin N., Watfa M.",57220200388;14632574200;,"Deaf and hard of hearing in the United Arab Emirates interacting with Alexa, an intelligent personal assistant",2020,Technology and Disability,32,4,,255,269,,1.0,10.3233/TAD-200286,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097214506&doi=10.3233%2fTAD-200286&partnerID=40&md5=da7500a701900af1765c9d5ef3ca46f5,"BACKGROUND: Intelligent Personal Assistants have been booming around the world since 2014, allowing millions of users to interact with different cloud-based software via speech. Unfortunately, the Deaf and Hard of Hearing individuals have been left out without recognizable accessibility to such technologies, although it might be used to make their daily life routine easier. OBJECTIVE: In this research, the researcher studies the interaction and perception of Amazon's Alexa among the Deaf and Hard of Hearing in the United Arab Emirates in its current set up (Tap-to-Alexa accessibility option) in addition to Sign Language as an input method. The researcher expands on the Technology Acceptance Model to study the acceptance of Alexa as an assistive technology for the Deaf and Hard of Hearing. Additionally, the researcher discusses more suitable input methods and solutions to allow Alexa, and other Intelligent Personal Assistants, be more accessible for the Deaf and Hard of Hearing. METHODS: The mixed method is used in this research in terms of collecting primary data through hands-on experiments, surveys, and interviews with the Deaf and Hard of Hearing participants. RESULTS: The researcher found that the Deaf and Hard of Hearing in the United Arab Emirates perceive that Sign Language combined with a Live interpreter is better than the accessibility option 'Tap-to-Alexa', which is a solution provided by Amazon. The researcher also found that Sign Language combined with a Live interpreter is the most suitable input method to make the device accessible for the Deaf and Hard of Hearing, in addition to translating the 'Tap-to-Alexa' to different languages. Finally, the researcher proposes a modification to the Technology Acceptance Model to suit the research study of the Deaf and Hard of Hearing perception of Alexa. CONCLUSIONS: The researcher concludes that the ideal scenario for the Deaf and Hard of Hearing to interact and benefit the most from Amazon's Alexa, and IPAs in general, is to include Sign Language as an embedded input method in the device and provide live interpreters; this sheds light on the importance of the interpreters' jobs around the world. Additionally, 'Tap-to-Alexa' must be translated into different languages for a better perception of the input method. © 2020 - IOS Press and the authors. All rights reserved.",Alexa; Deaf; Hard of Hearing; intelligent personal assistants; interaction,adult; Article; artificial intelligence; assistive technology; clinical article; cloud computing; controlled study; facial expression; gesture; grammar; hard of hearing; hearing; hearing impairment; human; interpersonal communication; perception; sign language; speech discrimination; translating (language); United Arab Emirates; vocabulary,Article,Final,Scopus,2-s2.0-85097214506,Peter,,
"Naylor G., Burke L.A., Holman J.A.",12761085700;57219770414;55979497800;,Covid-19 lockdown affects hearing disability and handicap in diverse ways: A rapid online survey study,2020,Ear and Hearing,,,,1442,1449,,37.0,10.1097/AUD.0000000000000948,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095403719&doi=10.1097%2fAUD.0000000000000948&partnerID=40&md5=1af370cff94cd7afbb213c1fe18ca41e,"Objectives: The aim of this study was to explore the perceived effects of coronavirus disease 2019 (Covid-19) social distancing restrictions and safety measures on people with hearing loss. Design: Participants were 129 adults (48.1% female, mean age 64.4 years) with an audiometric hearing loss, living in Glasgow, Scotland. A rapidly deployed 24-item online questionnaire asked about the effects of certain aspects of lockdown, including face masks, social distancing, and video calling, on participants' behavior, emotions, hearing performance, practical issues, and tinnitus. Data were analyzed descriptively across the entire sample, and with Chi-squared tests for differences between subgroups self-reporting relatively good and relatively poor unaided hearing, respectively. Additional free-text responses provided further perspectives. Results: Behavior: Video calls are used more frequently than prelockdown. The better-hearing group use their hearing aids less. Emotions: There is increased anxiety (especially among the worse hearing group) concerning verbal communication situations and access to audiology services, and greater rumination about one's own hearing loss. Enjoyment of group video calls is mixed. The worse hearing group shows substantial relief at not being obliged to attend challenging social gatherings. Across both groups, a majority would like to see all key workers equipped with transparent face masks. Hearing performance: A large majority finds it hard to converse with people in face masks due to muffled sound and lack of speechreading cues, but conversing at a safe distance is not universally problematic. In the worse hearing group, performance in video calls is generally inferior to face-to-face, but similar to telephone calls. Those who use live subtitling in video calls appreciate their value. TV and radio updates about Covid-19 are easy to follow for most respondents. There is only weak evidence of face mask fixtures interfering with hearing aids on the ear, and of tinnitus having worsened during lockdown. Conclusions: With due regard for the limitations of this rapid study, we find that there are many negative - and a few positive - effects of Covid-19 restrictions and safety measures on people with hearing loss. From a societal perspective, the widespread adoption of clear face masks may alleviate some of the difficulties and anxieties this population experience. From an individual perspective, one may consider using live subtitles on video calls. Manufacturers of hearing devices should consider developing processing modes and accessories specifically designed for video calls. Finally, repair and maintenance services should be resumed as soon as it is safe to do so. © 2020 World Scientific Publishing Co. Pte Ltd. All rights reserved.",Covid-19; Hearing disability; Hearing handicap; Social distancing,"aged; anxiety; attitude to health; auditory threshold; complication; coping behavior; disability; female; health survey; hearing aid; hearing impaired person; human; male; mask; middle aged; psychology; questionnaire; Scotland; tinnitus; videoconferencing; Adaptation, Psychological; Aged; Anxiety; Attitude to Health; Auditory Threshold; COVID-19; Disability Evaluation; Female; Health Surveys; Hearing Aids; Humans; Male; Masks; Middle Aged; Persons With Hearing Impairments; Physical Distancing; Scotland; Surveys and Questionnaires; Tinnitus; Videoconferencing",Review,Article in Press,Scopus,2-s2.0-85095403719,Peter,,
"Datta P., Jakubowicz P., Vogler C., Kushalnagar R.",57219157047;57219166203;7005789370;36142036500;,Readability of punctuation in automatic subtitles,2020,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),12377 LNCS,,,195,201,,1.0,10.1007/978-3-030-58805-2_23,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091496923&doi=10.1007%2f978-3-030-58805-2_23&partnerID=40&md5=930942596f36e832c1b7699c5c737260,"Automatic subtitles are widely used for subtitling television and online videos. Some include punctuation while others do not. Our study with 21 participants watching subtitled videos found that viewers reported that punctuation improves the “readability” experience for deaf, hard of hearing, and hearing viewers, regardless of whether it was generated via ASR or humans. Given that automatic subtitles have become widely integrated into online video and television programs, and that nearly 20% of television viewers in US or UK use subtitles, there is evidence that supports punctuation in subtitles has the potential to improve the viewing experience for a significant percentage of the all television viewers, including people who are deaf, hard of hearing, and hearing. © Springer Nature Switzerland AG 2020.",Deaf; Hard of hearing; Subtitles,Artificial intelligence; Computer science; Computers; Hard of hearings; Online video; Television programs; Audition,Conference Paper,Final,Scopus,2-s2.0-85091496923,Peter,,
"Akmal S., Mulia I.D.",57215426400;57218903689;,"Investigating students’ interest on reading journal articles: Materials, reasons and strategies",2020,Studies in English Language and Education,7,1,,194,208,,3.0,10.24815/siele.v7i1.15358,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090644587&doi=10.24815%2fsiele.v7i1.15358&partnerID=40&md5=be034d2a422644fbf7144f3227df610c,"This article investigates students’ reading materials, reasons for reading journal articles, and strategies in handling its difficulties. The data was collected by the use of qualitative method with structured interview. A number of eight students were purposely selected as the participants of this study, each representing eight different units studying in the seventh semester at a university in Banda Aceh, Indonesia. In analyzing the data, this research employed the qualitative descriptive analysis of data organization, data examination and data explanation. The findings showed that the favorite reading materials for students are website articles and social media captions, followed by non-fiction readings and newspapers. It is also found that preparing assignment is the utmost popular reason for reading journal articles for the students. Students also said that looking up in dictionary, internet surfing, consulting friends and lecturers, more practices, predicting the meaning of the words, and partial reading were some strategies they used to tackle the problems of reading journal articles. The implication of this study can be of actual practice to the academic reading course and curriculum and material development, especially for future improvement on students’ reading performance and proficiency. © Universitas Syiah Kuala.",Journal articles; Materials; Reasons; Strategies; Students’ interest,,Article,Final,Scopus,2-s2.0-85090644587,Peter,,
"Acosta T., Acosta-Vargas P., Zambrano-Miranda J., Lujan-Mora S.",57200384739;57192678428;57216205739;6603381780;,Web Accessibility Evaluation of Videos Published on YouTube by Worldwide Top-Ranking Universities,2020,IEEE Access,8,,9115660,110994,111011,,11.0,10.1109/ACCESS.2020.3002175,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087414020&doi=10.1109%2fACCESS.2020.3002175&partnerID=40&md5=e80ce9750519a6e2832ea40cbcd2cadf,"Video consumption on the web has increased markedly in recent years. Universities use videos in different teaching-learning modalities, as well as on their websites, to publish information aimed at their stakeholders. Access to education and information has been recognized as a human right in several international conventions and the constitutions of most countries. Therefore, it is essential to ensure that videos published on the web can be accessed by people with disabilities. The universality of the web is so important that some organizations worldwide have contributed to the development of standards and recommendations focused on web accessibility. Despite these efforts, the rights of millions of people are currently violated, as they are excluded from access to both education and information published on the web. Regarding videos, the reasons are a lack of captions, sign language, audio descriptions, and transcriptions, among others. The objective of this study is to evaluate the accessibility of videos published on YouTube by the best universities in the world based on compliance with the Web Content Accessibility Guidelines (WCAG) 2.1 of the World Wide Web Consortium. We carry out a manual evaluation of 91,421 videos, which were all published on YouTube by 113 universities taken from the Shanghai Ranking. Our purpose is to highlight the urgent need to change the current low level of accessibility that their educational videos show. Consequently, statistical results are presented regarding the compliance with video accessibility according to the regions and positions of the universities in the ranking. © 2013 IEEE.",Accessibility; audio description; captions; disabilities; education; subtitles; videos; web Content accessibility guidelines (WCAG),Websites; Audio description; Educational videos; International conventions; Level of accessibility; People with disabilities; Teaching-learning; Web content accessibility guidelines; World wide web consortiums; Publishing,Article,Final,Scopus,2-s2.0-85087414020,Peter,,
Pegrum M.,10642047100;,Mobile lenses on learning: Languages and literacies on the move,2020,Mobile Lenses on Learning: Languages and Literacies on the Move,,,,1,309,,11.0,10.1007/978-981-15-1240-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085453668&doi=10.1007%2f978-981-15-1240-7&partnerID=40&md5=5eb9de352e0d7fbe1e03387ed533a165,"This book explores mobile learning as a form of learning particularly suited to our ever more mobile world, presenting a new conceptualisation of the value of mobile devices in education through the metaphor of lenses on learning. With a principal focus on mobile-assisted language learning (MALL), it draws on insights derived from MALL language, literacy and cultural projects to illustrate the possibilities inherent in all mobile learning. In its broad sweep the book takes in new and emerging technologies and tools from robots to holograms, virtual reality to augmented reality, and smart glasses to embeddable chips, considering their potential impact on education and, indeed, on human society and the planet as a whole. While not shying away from discussing the risks, it demonstrates that, handled appropriately, mobile, context-aware technologies allow educators to build on the personalised and collaborative learning facilitated by web 2.0 and social media, but simultaneously to go much further in promoting authentic learning experiences grounded in real-world encounters. In this way, teachers can better prepare students to face a global, mobile future, with all of its evolving possibilities and challenges. © Springer Nature Singapore Pte Ltd. 2019.",Digital literacy/literacies; Educational learning trails; Educational treasure hunts; Mobile augmented reality learning; Mobile big data; Mobile gamification; Mobile learning analytics; Mobile literacy/literacies; Mobile personal learning environments; Mobile personal learning networks; Mobile professional development for teachers; Mobile virtual reality learning; Mobile-assisted language learning; Networking literacy; Overview of m-learning; Overview of mobile learning; Wearable devices in education,,Book,Final,Scopus,2-s2.0-85085453668,Peter,,
"Şılbır L., Coşar A.M., Kartal Y., Altun T., Atasoy M., Özçamkan-Ayaz G.",36992756500;36454496300;57216437151;36991997900;57216431221;57216441747;,Graphic symbol based interactive animation development process for deaf or hard of hearing students,2020,International Electronic Journal of Elementary Education,12,4,,371,382,,2.0,10.26822/iejee.2020459466,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083517593&doi=10.26822%2fiejee.2020459466&partnerID=40&md5=d5aa2667b9410c0d132cafbb4acc2ca8,"This study examined the development process of graphic symbol-based animations for enhancing literacy skills of deaf or hard of hearing students (D/HH). Participants of the study consisted of two teachers and seven students studying in the third and fourth grade in a primary school for hard of hearing. As a result of the studies conducted throughout the fall and spring semesters, animation environments based on graphic symbols were developed. Within the framework of the design-based research methodology, development studies were followed by revisions. Revision studies were conducted in line with the data obtained from the interviews and observation notes that continued throughout the practices. A content analysis was utilized to examine the data obtained through interview and observation. It has been determined that; in the animations each sentence should be presented clearly and it may be paused if necessary, minor details and the dormant objects in the background should be removed, animations of actions should be standardized, and fonts in animations should match the writing style used by the students in the animations. As a result of the study, a design guide for graphic symbols-based animations for D/HH were developed for the researchers in line with the acquired results. © 2020 Published by T& K Academic.",Deaf or Hard of Hearing (D/HH); Education; Symbols-Based Animation,,Article,Final,Scopus,2-s2.0-85083517593,Peter,,
"Ohene-Djan J., Sushko V.",6508300037;57216416866;,A real-time spatial measurement interface for emotional evaluation of temporal media,2020,"Proceedings of the 20th BCS HCI Group Conference: Engage, HCI 2006",,,,187,191,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083421886&partnerID=40&md5=83881d4565be632b6f0d3569b92d990e,"In this paper we present a real-time advanced spatial measurement interface (RASMI), designed to allow users to engage with temporal media presentations, such as party political television broadcasts, in order to imprecise intuitive responses or emotions. The interface is based on an equilateral triangle that records user input as three sets of data points in real time over a two-dimensional space. This information is then used to calculate the levels of emotional uncertainty relative to other choices using a triangle-based spatial measurement technique. Levels of emotional uncertainty are represented using a set of 2D Interactive graphs that link user responses to points in media playback timelines. 2D Interactive graphs not only provide a visual representation of the data flow but also act as an extremely engaging backtracking tool enabling easy identification of when within a media presentation and with what degree of uncertainty a user wished to register an emotion. Information captured is also visualized by means of an Equilateral Triangle Scatter Graph and line graphs. The novelty of RASMI is its ability to allow groups of users to experience temporal media presentations while concurrently providing real-time data on their emotional experience. The RASMI is software runs on any PC, that's supports digital video, and a mouse. We are currently using RASMI to explore user responses to party political television broadcasts and envisage future use in the area of online market research. © BCS HCI Group Conference: Engage, HCI 2006.All right reserved.",Assessment; Information visualization; Temporal media; Uncertainty,Computer graphics; Data flow analysis; Flow graphs; Human computer interaction; Multimedia systems; Social computing; Spatial variables measurement; Television broadcasting; Uncertainty analysis; User experience; Degree of uncertainty; Emotional experiences; Equilateral triangles; Online markets; Spatial measurements; Temporal media; Two dimensional spaces; Visual representations; Mammals,Conference Paper,Final,Scopus,2-s2.0-85083421886,Peter,,
"Matamala A., Soler-Vilageliu O., Iturregui-Gallardo G., Jankowska A., Méndez-Ulrich J.-L., Ratera A.S.",25921557700;12778196600;57203986821;57188545956;55695715400;57216345440;,Electrodermal activity as a measure of emotions in media accessibility research: Methodological considerations,2020,Journal of Specialised Translation,,33,,129,151,,4.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083257460&partnerID=40&md5=8072ccbc6d03b37658019c9e564a2171,"This article proposes electrodermal activity (EDA) as a new objective measure for experimental studies in media accessibility. It first presents a theoretical framework in which the concept of emotion and its categorisation are presented. It then explains how EDA can be used to measure emotional reaction: the article reports on experimental design, participant selection, stimuli preparation, data collection devices, experimental procedure and data analysis. It also discusses briefly how EDA can be combined with other measures. Overall, the article provides a general methodological framework for the implementation of electrodermal activity as a measure of emotions in media accessibility research. © 2020 University of Roehampton. All rights reserved.",Electrodermal activity; Emotions; Media accessibility; Psychophysiology; Reception studies,,Article,Final,Scopus,2-s2.0-85083257460,Peter,,
de los Reyes Lozano J.,57196152792;,Straight from the horse’s mouth: Children’s reception of dubbed animated films in Spain,2020,Journal of Specialised Translation,,33,,233,258,,2.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083175457&partnerID=40&md5=8dd0b0f004235ba4336812f5429ad2ac,"Reception studies in the field of audiovisual translation (AVT) have increased considerably in the last two decades, including the target viewer in the picture. This paper presents the results of a study exploring young children’s reactions to some of the translation strategies regularly adopted in dubbed animated films. A total of 163 participants were shown nine animated film clips dubbed from English into Spanish, which included cultural references, colloquial language, educational content and songs. Data were then collected through a questionnaire adapted to the participants’ level of cognitive development and the analysis was based on two independent variables: the participants’ year in school and the number of previous viewings of the films. The results show that children do not seem to have much trouble understanding cultural, educational and musical content that is specific to the source culture and is kept in the target text. Interestingly, having previously watched the films does not appear to be a determining factor in children’s ability to identify these elements. © 2020 University of Roehampton. All rights reserved.",Audiovisual translation; Children’s films; Dual audience; Dubbing; Experimental study; Reception,,Article,Final,Scopus,2-s2.0-85083175457,Peter,,
"Acosta T., Zambrano-Miranda J., Lujan-Mora S.",57200384739;57216205739;6603381780;,Techniques for the Publication of Accessible Multimedia Content on the Web,2020,IEEE Access,8,,9039611,55300,55322,,3.0,10.1109/ACCESS.2020.2981326,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082798697&doi=10.1109%2fACCESS.2020.2981326&partnerID=40&md5=969cfe53c57b440fb79316e627966a6b,"Multimedia has become one of the most important sources of information and communication on the web. However, despite recent technological progress, people with disabilities and the elderly face difficulties accessing multimedia on the web. In some cases, these difficulties are impossible to overcome and are a fundamental cause of digital exclusion. Given the importance of this topic, several investigations on the problems of accessing multimedia resources have been carried out. Some organizations have also proposed certain standards to guide the creation and publication of accessible web content. Nevertheless, the authoring tools used in the process of publishing multimedia on the web do not offer all the accessibility features required. Authoring tools can also be used by people who do not have knowledge about web accessibility or programming, resulting in web publications lacking accessibility. This research proposes 278 novel techniques to guide authors, designers, programmers, and testers in the publication of accessible and inclusive multimedia on the web. These techniques are designed to guarantee the compliance with the recommended success criteria of Authoring Tools Accessibility Guidelines (ATAG) 2.0 of the World Wide Web Consortium. Moreover, these techniques can be used to evaluate the accessibility of the existing authoring tools used to create multimedia for the web. Additionally, we present 80 possible failures that can cause the non-fulfillment of ATAG 2.0. These failures can help authors discern what to avoid and help evaluators check whether particular multimedia is accessible. © 2013 IEEE.",Accessibility; accessibility content; Authoring Tools Accessibility Guidelines (ATAG) 2.0; disabilities; e-learning; multimedia; techniques; World Wide Web Consortium (W3C),E-learning; Publishing; World Wide Web; Accessibility; accessibility content; Accessibility guidelines; disabilities; multimedia; techniques; World wide web consortiums; Multimedia systems,Article,Final,Scopus,2-s2.0-85082798697,Peter,,
"Wisniewska N., Mora J.C.",57215862601;55505235100;,CAN CAPTIONED VIDEO BENEFIT SECOND LANGUAGE PRONUNCIATION?,2020,Studies in Second Language Acquisition,42,3,,599,624,,22.0,10.1017/S0272263120000029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082106254&doi=10.1017%2fS0272263120000029&partnerID=40&md5=6813cf67e1fb4c4e086835e3e10fdfcb,"The present study investigated the potential benefits of extended exposure to captioned videos for second language pronunciation. We tested 90 L2 adult learners of English on speech processing skills (segmentation, speed of lexical access, and sentence processing) and phonological accuracy in perception (ABX discrimination) and production (accentedness ratings) before and after an 8-week treatment consisting of regular exposure to audiovisual materials. Participants were randomly assigned to four experimental conditions involving two viewing modes (captioned or uncaptioned) and two task focus conditions (focus on phonetic form or focus on meaning). Results showed benefits in speech segmentation and speech processing skills irrespective of viewing mode. No significant benefits were found for phonological accuracy in perception. In production, a focus on phonetic form improved pronunciation only in the absence of captions, whereas captioned viewing led to pronunciation gains as long as there was no focus on phonetic form. These findings suggest that pronunciation improvement can take place with the help of captions or, in the absence of captions, when learners' attention is directed to pronunciation. Cognitive overload might explain why no benefits were obtained when attention was directed to pronunciation in a captioned viewing mode. ©",,,Article,Final,Scopus,2-s2.0-85082106254,Peter,,
"Nagels L., Bastiaanse R., Başkent D., Wagner A.",57209053708;7003821546;6602445586;37022155400;,Individual differences in lexical access among cochlear implant users,2020,"Journal of Speech, Language, and Hearing Research",63,1,,286,304,,8.0,10.1044/2019_JSLHR-19-00192,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078552008&doi=10.1044%2f2019_JSLHR-19-00192&partnerID=40&md5=6737345334df7d2746c54b7afbd0a95e,"Purpose: The current study investigates how individual differences in cochlear implant (CI) users’ sensitivity to word–nonword differences, reflecting lexical uncertainty, relate to their reliance on sentential context for lexical access in processing continuous speech. Method: Fifteen CI users and 14 normal-hearing (NH) controls participated in an auditory lexical decision task (Experiment 1) and a visual-world paradigm task (Experiment 2). Experiment 1 tested participants’ reliance on lexical statistics, and Experiment 2 studied how sentential context affects the time course and patterns of lexical competition leading to lexical access. Results: In Experiment 1, CI users had lower accuracy scores and longer reaction times than NH listeners, particularly for nonwords. In Experiment 2, CI users’ lexical competition patterns were, on average, similar to those of NH listeners, but the patterns of individual CI users varied greatly. Individual CI users’ word–nonword sensitivity (Experiment 1) explained differences in the reliance on sentential context to resolve lexical competition, whereas clinical speech perception scores explained competition with phonologically related words. Conclusions: The general analysis of CI users’ lexical competition patterns showed merely quantitative differences with NH listeners in the time course of lexical competition, but our additional analysis revealed more qualitative differences in CI users’ strategies to process speech. Individuals’ word–nonword sensitivity explained different parts of individual variability than clinical speech perception scores. These results stress, particularly for heterogeneous clinical populations such as CI users, the importance of investigating individual differences in addition to group averages, as they can be informative for clinical rehabilitation. © 2019 The Authors.",,adult; aged; auditory stimulation; cochlea prosthesis; cochlear implantation; female; hearing impairment; human; individuality; male; middle aged; phonetics; procedures; psychology; speech perception; task performance; Acoustic Stimulation; Adult; Aged; Cochlear Implantation; Cochlear Implants; Deafness; Female; Humans; Individuality; Male; Middle Aged; Phonetics; Speech Perception; Task Performance and Analysis,Article,Final,Scopus,2-s2.0-85078552008,Peter,,
"Seale J., Carrizosa H.G., Rix J., Sheehy K., Hayhoe S.",57198322307;57202913459;23006045500;9636302500;55681880100;,In Search of a Decision-Making Framework for Involving Users Who Have Learning Disabilities or Sensory Impairments in the Process of Designing Future Technologies,2020,Advances in Intelligent Systems and Computing,1069,,,844,861,,2.0,10.1007/978-3-030-32520-6_61,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075649239&doi=10.1007%2f978-3-030-32520-6_61&partnerID=40&md5=8f534df8c40c83626f7eeb535acfd4db,"A comprehensive literature review was undertaken in order to identify design approaches that have been employed with users who have learning disabilities or sensory impairment; the factors that influenced their choices and the extent to which the approaches and techniques adopted were successful. There was a huge variation across the corpus regarding whether a justification was offered for the choice of approach and the extent to which those justifications were supported by evidence. In addition there was a lack of comprehensive evaluation of the design approaches. Technology designers who intend working with users with learning disabilities or sensory impairments therefore currently have little to help them decide which design approach might be the most appropriate or effective. © 2020, Springer Nature Switzerland AG.",Decision-making; Design process; Disability; User participation,Computer programming; Computer science; Comprehensive evaluation; Decision-making frameworks; Design process; Disability; Future technologies; Learning disabilities; Literature reviews; User participation; Decision making,Conference Paper,Final,Scopus,2-s2.0-85075649239,Peter,,
Romero-Fresco P.,36675476200;,The dubbing effect: An eye-tracking study on how viewers make dubbing work,2020,Journal of Specialised Translation,,33,,17,40,,9.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070615609&partnerID=40&md5=c3e0a3f95a7b5dc1a7089c4c3295a7b0,"Although dubbing is regularly criticised for its artifice and its manipulation of film sound, it has proved to be the preferred modality of audiovisual translation for millions of viewers. Research in this area has explored at length the way in which the professionals involved in dubbing make it work. What has been overlooked so far is the cognitive process undergone by the viewers to make it work. In order to explore this issue, this paper starts with a discussion of several aspects that may be relevant to the perception and overall reception of dubbing, including cultural arguments on habituation, psychological and cognitive notions of suspension of disbelief and perceptual phenomena such as the McGurk effect. It then goes on to compare, with the help of eye-tracking technology, the eye movements of a group of native Spanish participants watching a clip dubbed into Spanish featuring closeups with (a) the eye movements of a group of native English participants watching the same clip in English and (b) the eye movements of the Spanish participants watching an original (and comparable) clip in Spanish. This analysis is complemented by data on the participants’ comprehension, sense of presence and self-perception of their eye movements when watching these clips. The findings obtained point to the potential existence of a dubbing effect, an unconscious eye movement strategy performed by dubbing viewers to avoid looking at mouths in dubbing, which prevails over the natural way in which they watch original films and real-life scenes, and which allows them to suspend disbelief and be transported into the fictional world. © 2020 University of Roehampton. All rights reserved.",Dubbing; Dubbing effect; Engagement; Eye tracking; Habituation; McGurk effect; Suspension of disbelief,,Article,Final,Scopus,2-s2.0-85070615609,Peter,,
Robinson D.,53564594700;,"Becoming a translator: An introduction to the theory and practice of translation, fourth edition",2019,"Becoming a Translator: An Introduction to the Theory and Practice of Translation, Fourth Edition",,,,1,304,,2.0,10.4324/9780429276606,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104804070&doi=10.4324%2f9780429276606&partnerID=40&md5=dbbe593753d0b7350307f29f81a4cb4d,"Fusing theory with advice and information about the practicalities of translating, Becoming a Translator is the essential resource for novice and practicing translators. The book explains how the market works, helps translators learn how to translate faster and more accurately, as well as providing invaluable advice and tips about how to deal with potential problems, such as stress. The fourth edition has been revised and updated throughout, offering: a whole new chapter on multimedia translation, with a discussion of the move from ""intersemiotic translation"" to ""audiovisual translation,"" ""media access"" and ""accessibility studies"" new sections on cognitive translation studies, translation technology, online translator communities, crowd-sourced translation, and online ethnography ""tweetstorms"" capturing the best advice from top industry professionals on Twitter student voices, especially from Greater China Including suggestions for discussion, activities, and hints for the teaching of translation, and drawing on detailed advice from top translation professionals, the fourth edition of Becoming a Translator remains invaluable for students and teachers of Translation Studies, as well as those working in the field of translation. © 2020 Douglas Robinson and for selected content, Mark Shuttleworth and Chuan Yu.",,,Book,Final,Scopus,2-s2.0-85104804070,Peter,,
"Sanchez-Gordon S., Luján-Mora S.",56027184100;6603381780;,"Design, implementation and evaluation of MOOCs to improve inclusion of diverse learners",2019,Accessibility and Diversity in Education: Breakthroughs in Research and Practice,,,,52,79,,7.0,10.4018/978-1-7998-1213-5.ch004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085501295&doi=10.4018%2f978-1-7998-1213-5.ch004&partnerID=40&md5=04c23c8d7c17faed08438806271d3fdb,"This chapter presents accessibility requirements that need to be considered in the design, implementation and evaluation of Massive Open Online Courses (MOOCs) to ensure they are inclusive. Accessibility requirements take in account particular needs, preferences, skills and situations of diverse learners, e.g. people with disabilities, elderly people and foreign students. The accessibility needs have to be considered in the design and implementation of MOOCs' interfaces, contents and learning/assessment activities. Due to its open and massive nature, with an adequate implementation, MOOCs can overcome inclusion barriers for the benefit of potential learners worldwide, both able and disabled. For evaluation, there are accessibility evaluation tools that identify accessibility problems in the content, semantic and structural elements of a website that can be used to evaluate the level of accessibility of MOOCs. Additional expert-based and user-based evaluations are always recommended in order to achieve valid results. © 2020, IGI Global.",,,Book Chapter,Final,Scopus,2-s2.0-85085501295,Peter,,
"Kaba A., Ellala Z.K.",36951393200;57205338683;,Digital information resources: use and perceptions of deaf and hearing students,2019,Digital Library Perspectives,35,3-4,,227,243,,5.0,10.1108/DLP-05-2019-0020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074922874&doi=10.1108%2fDLP-05-2019-0020&partnerID=40&md5=86a27ddca91fc938046424c4a4d78f05,"Purpose: The purpose of this study is to investigate the use of the internet among deaf and hearing students. The study also explores the perceptions of students toward the internet, online databases and digital libraries. Design/methodology/approach: The study uses a survey questionnaire to collect data from the sample. The sample consists of 59 hearing and 53 deaf students studying at Al Ain University of Science and Technology Al Ain University (AAU). Descriptive statistics and t-tests are used to analyze data. Findings: Findings of the study show that at least 90 per cent of participants are using the internet for intellectual activities and social communications. In addition, the majority of deaf and hearing students have positive perceptions toward digital information resources. For gender differences, the study found no significant difference between men and women in the use of internet or in their perceptions toward the importance of online databases and digital libraries. However, results of the study confirmed significant difference between deaf and hearing students in relation to internet use, perception toward the importance of online databases and digital libraries. Research limitations/implications: The sample of the study are students studying at AAU. Future studies may expend this study by including deaf participants from various institutions in UAE. Originality/value: The results of the study could be used in planning and providing digital information resources and services for deaf students. © 2019, Emerald Publishing Limited.",Deaf students; Digital libraries; Hearing students; Internet use; Online database; Perception,Audition; Database systems; Sensory perception; Students; Surveys; Deaf students; Descriptive statistics; Design/methodology/approach; Digital information resources; Intellectual activities; Internet use; Online database; Science and Technology; Digital libraries,Article,Final,Scopus,2-s2.0-85074922874,Peter,,
"Solberg Økland H., Todorović A., Lüttke C.S., McQueen J.M., de Lange F.P.",57209003279;55521272400;56986501000;55760271100;8567604600;,Combined predictive effects of sentential and visual constraints in early audiovisual speech processing,2019,Scientific Reports,9,1,7870,,,,2.0,10.1038/s41598-019-44311-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066239027&doi=10.1038%2fs41598-019-44311-2&partnerID=40&md5=e8aa9229469ce938279d6410a59e9e3b,"In language comprehension, a variety of contextual cues act in unison to render upcoming words more or less predictable. As a sentence unfolds, we use prior context (sentential constraints) to predict what the next words might be. Additionally, in a conversation, we can predict upcoming sounds through observing the mouth movements of a speaker (visual constraints). In electrophysiological studies, effects of visual constraints have typically been observed early in language processing, while effects of sentential constraints have typically been observed later. We hypothesized that the visual and the sentential constraints might feed into the same predictive process such that effects of sentential constraints might also be detectable early in language processing through modulations of the early effects of visual salience. We presented participants with audiovisual speech while recording their brain activity with magnetoencephalography. Participants saw videos of a person saying sentences where the last word was either sententially constrained or not, and began with a salient or non-salient mouth movement. We found that sentential constraints indeed exerted an early (N1) influence on language processing. Sentential modulations of the N1 visual predictability effect were visible in brain areas associated with semantic processing, and were differently expressed in the two hemispheres. In the left hemisphere, visual and sentential constraints jointly suppressed the auditory evoked field, while the right hemisphere was sensitive to visual constraints only in the absence of strong sentential constraints. These results suggest that sentential and visual constraints can jointly influence even very early stages of audiovisual speech comprehension. © 2019, The Author(s).",,adult; article; brain function; controlled study; female; human; human experiment; language processing; left hemisphere; magnetoencephalography; male; modulation; mouth; right hemisphere; speech discrimination; videorecording; brain; comprehension; hearing; physiology; semantics; speech; speech perception; vision; young adult; Adult; Auditory Perception; Brain; Comprehension; Female; Humans; Magnetoencephalography; Male; Semantics; Speech; Speech Perception; Visual Perception; Young Adult,Article,Final,Scopus,2-s2.0-85066239027,Peter,,
"Glasser A., Riley E., Weeks K., Kushalnagar R.",57195128152;57212199224;57529618400;36142036500;,Mixed reality speaker identification as an accessibility tool for deaf and hard of hearing users,2019,"Proceedings of the ACM Symposium on Virtual Reality Software and Technology, VRST",,,3364720,,,,3.0,10.1145/3359996.3364720,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076138246&doi=10.1145%2f3359996.3364720&partnerID=40&md5=e13f935f5a26f871a4aede8e327ebbff,"People who are Deaf or Hard of Hearing (DHH) benefit from text captioning to understand audio, yet captions alone are often insufficient for the complex environment of a panel presentation, with rapid and unpredictable turn-taking among multiple speakers. It is challenging and tiring for DHH individuals to view captioned panel presentations, leading to feelings of misunderstanding and exclusion. In this work, we investigate the potential of Mixed Reality (MR) head-mounted displays for providing captioning with visual cues to indicate which person on the panel is speaking. For consistency in our experimental study, we simulate a panel presentation in virtual reality (VR) with various types of MR visual cues; in a study with 18 DHH participants, visual cues made it easier to identify speakers. © 2019 Copyright held by the owner/author(s).",Deaf and Hard of Hearing; Mixed Reality; Speaker Identification,Audition; Helmet mounted displays; Loudspeakers; Speech recognition; Complex environments; Hard of hearings; Head mounted displays; Speaker identification; Turn-taking; Visual cues; Mixed reality,Conference Paper,Final,Scopus,2-s2.0-85076138246,Peter,,
"Mocanu B., Tapu R., Zaharia T.",24822846400;26424843800;6601999900;,Enhancing the accessibility of hearing impaired to video content through fully automatic dynamic captioning,2019,"2019 7th E-Health and Bioengineering Conference, EHB 2019",,,8970038,,,,2.0,10.1109/EHB47216.2019.8970038,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079356501&doi=10.1109%2fEHB47216.2019.8970038&partnerID=40&md5=e8c25007617994c8f32beb05536e1213,"In this paper we introduce an automatic subtitle positioning approach designed to enhance the video accessibility of deaf and hearing impaired people to multimedia documents. By using a dynamic subtitle and captioning approach, which exploits various computer vision techniques, including face detection, tracking and recognition, video temporal segmentation into shots and scenes and active speaker recognition, we are able to position each video subtitle segment in the near vicinity of the active speaker. The experimental evaluation performed on 30 video elements validates our approach with average F1-scores superior to 92%. © 2019 IEEE.",Active speaker recognition; Deaf and hearing impaired people; Dynamic subtitle,Face recognition; Speech recognition; Computer vision techniques; Deaf and hearing impaired; Dynamic captioning; Experimental evaluation; Hearing impaired; Multimedia documents; Speaker recognition; Video temporal segmentation; Audition,Conference Paper,Final,Scopus,2-s2.0-85079356501,Peter,,
"Kameswaran V., Muralidhar S.H.",57190061358;57194548563;,"Cash, digital payments and accessibility - A case study from India",2019,Proceedings of the ACM on Human-Computer Interaction,3,CSCW,97,,,,12.0,10.1145/3359199,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075054387&doi=10.1145%2f3359199&partnerID=40&md5=666e2f1cb3e82ea8bba920e6070f7d25,"Despite the growing interest in digitization and money in HCI and CSCW, the use of cash and digital payments by people with disabilities has received scant attention. We present findings from a qualitative study of people with visual impairments’ use of cash and digital payments in metropolitan India. Using ride-hailing services as an exemplar, we find that both cash and digital payments were inaccessible to participants. We use Perry and Ferreira’s ""moneywork"" as a theoretical framework to highlight the ""added"" work necessitated by this inaccessibility; that is, the work done in addition to the interactional work necessary to complete financial transactions. We argue that this ""added"" work is instrumental in ""making"" payments accessible. We discuss how ride-hailing platforms mediated collaborations between drivers and riders in relation to payments, while still making ""moneywork"" essential. We provide recommendations to improve the accessibility of digital payments to facilitate greater economic inclusion. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Accessibility; Blind users; Cash; Digital money; Digital payments; Mobile money; Mobile payments; Ola; Ridesharing; Social accessibility; Social interactions; Uber,,Editorial,Final,Scopus,2-s2.0-85075054387,Peter,,
"Hirskyj-Douglas I., Kytö M., McGookin D.",57188718395;36975727300;14035860500;,"Head-mounted displays, smartphones, or smartwatches? – Augmenting conversations with digital representation of self",2019,Proceedings of the ACM on Human-Computer Interaction,3,CSCW,179,,,,7.0,10.1145/3359281,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075040532&doi=10.1145%2f3359281&partnerID=40&md5=2fe2c3de5d5dd3dfa8f46d39f4cf2894,"Technologies that augment face-to-face interactions with a digital sense of self have been used to support conversations. That work has employed one homogenous technology, either 'off-the-shelf' or with a bespoke prototype, across all participants. Beyond speculative instances, it is unclear what technology individuals themselves would choose, if any, to augment their social interactions; what influence it may exert; or how use of heterogeneous devices may affect the value of this augmentation. This is important, as the devices that we use directly affect our behaviour, influencing affordances and how we engage in social interactions. Through a study of 28 participants, we compared head-mounted display, smartphones, and smartwatches to support digital augmentation of self during face-to-face interactions within a group. We identified a preference among participants for head-mounted displays to support privacy, while smartwatches and smartphones better supported conversational events (such as grounding and repair), along with group use through screen-sharing. Accordingly, we present software and hardware design recommendations and user interface guidelines for integrating a digital form of self into face-to-face conversations. Copyright is held by the owner/author(s). Publication rights licensed to ACM.",Conversation; Face-to-face interactions; Head-mounted displays; Smartphone; Smartwatch,Smartphones; Street traffic control; User interfaces; Wearable computers; Conversation; Digital representations; Face-to-face conversation; Face-to-face interaction; Head mounted displays; Smartwatch; Software and hardwares; User interface guidelines; Helmet mounted displays,Article,Final,Scopus,2-s2.0-85075040532,Peter,,
"Jones M.D., Jeannette Lawler M.",57211332837;13607054000;,Delivering sign language in a live planetarium show using head-mounted displays and infrared light,2019,ASSETS 2019 - 21st International ACM SIGACCESS Conference on Computers and Accessibility,,,,396,401,,1.0,10.1145/3308561.3353809,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074950762&doi=10.1145%2f3308561.3353809&partnerID=40&md5=b9dde67120253e73344a5e26305e9496,Sign language narration is difficult to view at live planetarium shows because the room is dark and the signer is not located near images projected onto the planetarium dome. We have designed and implemented a system using head-mounted displays (HMDs) and infrared light to support viewing real-time sign language narration of live planetarium shows. Results from a series of 3 studies involving 29 students who are deaf or hard-of-hearing suggest that viewing properly configured video of sign language narration in an HMD may increase learning in this setting. Participants expressed no single preference regarding signer position in the video feed but did indicate that the relative brightness of the HMD must be tuned to match the apparent brightness of images broadcast on the planetarium dome. We also identified other issues related to HMD fit and the appearance of the signer in the video. © 2019 Copyright is held by the owner/author(s). Publication rights licensed to ACM.,Head mounted display; Planetarium; Sign language,Audition; Domes; Luminance; Planetariums; Street traffic control; Hard of hearings; Head mounted displays; Infrared light; Real time; Relative brightness; Sign language; Helmet mounted displays,Conference Paper,Final,Scopus,2-s2.0-85074950762,Peter,,
"Kafle S., Yeung P., Huenerfauth M.",57200500342;57204729169;12240800100;,Evaluating the benefit of highlighting key words in captions for people who are deaf or hard of hearing,2019,ASSETS 2019 - 21st International ACM SIGACCESS Conference on Computers and Accessibility,,,,43,55,,11.0,10.1145/3308561.3353781,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074941386&doi=10.1145%2f3308561.3353781&partnerID=40&md5=2caa8a5ca2a8336c0d71c60606082bc1,"Recent research has investigated automatic methods for identifying how important each word in a text is for the overall message, in the context of people who are Deaf and Hard of Hearing (DHH) viewing video with captions. We examine whether DHH users report benefits from visual highlighting of important words in video captions. In formative interview and prototype studies, users indicated a preference for underlining of 5%-15% of words in a caption text to indicate that they are important, and they expressed an interest for such text markup in the context of educational lecture videos. In a subsequent user study, 30 DHH participants viewed lecture videos in two forms: with and without such visual markup. Users indicated that the videos with captions containing highlighted words were easier to read and follow, with lower perceived task-load ratings, compared to the videos without highlighting. This study motivates future research on caption highlighting in online educational videos, and it provides a foundation for how to evaluate the efficacy of such systems with users. © 2019 Association for Computing Machinery.",Caption Highlighting; Captioning System; Deaf and Hard of Hearing; Feedback; Text Highlighting; User Study,Feedback; Online systems; Caption Highlighting; Captioning System; Hard of hearings; Text Highlighting; User study; Audition,Conference Paper,Final,Scopus,2-s2.0-85074941386,Peter,,
"Jain D., Desjardins A., Findlater L., Froehlich J.E.",57014317900;39061273100;10040303000;7101665384;,Autoethnography of a hard of hearing traveler,2019,ASSETS 2019 - 21st International ACM SIGACCESS Conference on Computers and Accessibility,,,,236,248,,27.0,10.1145/3308561.3353800,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074938622&doi=10.1145%2f3308561.3353800&partnerID=40&md5=24188fda05645541d91e0594e7fabe10,"Travel experiences offer a diverse view into an individual's interactions with different cultures, societies, and places. In this paper, we present a 2.5-year autoethnographic travel account of a hard of hearing individual-Jain. Through retrospective journals and field notes, we reveal the tensions and nuances in his travel, including the magnified difficulty of social conversations, issues with navigating unfamiliar environments and cultural contexts, and changes in the relationship to personal assistive technologies. By exploring the longitudinal travel experiences of a single individual, we uncover evocative and personal insights rarely available through participant-based research methods. Based on these lived experiences and post hoc reflections, we present two design explorations of personalized technology the autoethnographer created for aiding his travel. Finally, we offer reflections for customized travel technologies for deaf and hard of hearing users, and methodological guidelines for performing first-person research in the context of disability. © 2019 Association for Computing Machinery.",Accessibility; Autobiographical design; Autoethnography; Deaf; Hard of hearing; Personalized technology; Travel,Accessibility; Autoethnography; Deaf; Hard of hearings; Travel; Audition,Conference Paper,Final,Scopus,2-s2.0-85074938622,Peter,,
"Butler J., Trager B., Behm B.",57202196333;56884439300;57211760102;,Exploration of automatic speech recognition for deaf and hard of hearing students in higher education classes,2019,ASSETS 2019 - 21st International ACM SIGACCESS Conference on Computers and Accessibility,,,,32,42,,3.0,10.1145/3308561.3353772,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074898684&doi=10.1145%2f3308561.3353772&partnerID=40&md5=ad82f47cb435930631b02121e8e4bf66,"Automatic speech recognition (ASR) programs that generate real-time speech-to-text captions can be provided as supplemental access technologies for deaf and hard of hearing (DHH) students in higher education classes. As part of a pilot program, we implemented ASR as a supplemental access service in biology, statistics, and other courses at our university. To identify the benefits and limitations of ASR as an access technology, we surveyed 26 DHH students and interviewed 8 of these students about their experiences with ASR in their mainstream classes. Participants believed that ASR was beneficial despite the errors that ASR continued to generate; however, the accuracy and readability of ASR need to improve so that students can better access spoken information through ASR. This paper reviews points for researchers to consider when designing and providing ASR as a supplemental access service in educational settings. © 2019 Association for Computing Machinery.",Human-centered computing-Accessibility design and evaluation methods; Human-centered computing-Accessibility technologies; Human-centered computing-Empirical studies in accessibility,Audition; Character recognition; Students; Access technology; Automatic speech recognition; Design and evaluation methods; Educational settings; Empirical studies; Hard of hearings; Higher education; Human-centered computing; Speech recognition,Conference Paper,Final,Scopus,2-s2.0-85074898684,Peter,,
"Carmo G.M.D., Paiva D.M.B., Cagnin M.I.",57212474351;19638910400;6601927082;,How to develop accessible web interfaces for deaf people?,2019,IHC 2019 - Proceedings of the 18th Brazilian Symposium on Human Factors in Computing Systems,,,3358437,,,,3.0,10.1145/3357155.3358437,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076772210&doi=10.1145%2f3357155.3358437&partnerID=40&md5=287653dae273f9ecefe370c23fea7c84,"There are several entity initiatives, such as World Wide Web Consortium (W3C), which develop usability and accessibility standards for the Web. In general, the available technologies comply with these standards, but do not attend particularities and challenges of each disability, such as hard of hearing. Despite the initiatives, there are still some obstacles on the part of developers, such as the lack of interest, motivation and necessary knowledge about how to properly develop accessible web application. This paper presents results of a systematic mapping to obtain how Web interfaces are being designed and implemented for deaf people. The results showed few studies about development of tools and technologies to help developers; lack of requirements and recommendations that complement those existing to improve Web accessibility for the hard of hearing; and little use of assistive technology resources that can directly contribute to improving Web accessibility for this audience. © 2019 ACM.",Accessibility; Hard of hearing; Systematic mapping; Web user interface,Audition; Human engineering; Mapping; Websites; Accessibility; Assistive technology; Hard of hearings; Systematic mapping; Tools and technologies; Web accessibility; Web user interface; World wide web consortiums; User interfaces,Conference Paper,Final,Scopus,2-s2.0-85076772210,Peter,,
Oswal S.K.,55481178500;,Breaking the exclusionary boundary between user experience and access: Steps toward making UX inclusive of users with disabilities,2019,SIGDOC 2019 - Proceedings of the 37th ACM International Conference on the Design of Communication,,,12,,,,20.0,10.1145/3328020.3353957,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074759841&doi=10.1145%2f3328020.3353957&partnerID=40&md5=9cb68fc88f01f2f81e9b6bc2c133cf77,"This research paper points out that we as Designers have failed to come up with a model of UX that would proximate a satisfying user experience for users with disabilities. It underscores the gaps in designer knowledge about disabled bodies. The research paper also draws the attention of the designer community to the limited understanding we presently possess of the disabled people’s notions of, and expectations from, satisfying user experiences. It proposes a multi-step process for shifting the focus of design activity from a “medical model of accessibility design” that retrofits normative designs to the needs of users with disabilities to developing an “accessible user experience model (AUX)” of design that counts these users as design collaborators, possessors of special knowledge about disabled bodies, and untapped sources of innovative designs that might offer additional design features for all users. © 2019 Copyright held by the owner/author(s). Publication rights licensed to",Accessible design; Accessible UX (AUX); Disabled co-designers; Haptic technology; Medical model of accessibility design; Participatory design; Retrofitting accessibility,Accessible UX (AUX); Disabled co-designers; Haptic technology; Medical modeling; Participatory design; Retrofitting,Conference Paper,Final,Scopus,2-s2.0-85074759841,Peter,,
"Bartolotti J., Marian V.",16315037800;6602268362;,Learning and processing of orthography-to-phonology mappings in a third language,2019,International Journal of Multilingualism,16,4,,377,397,,7.0,10.1080/14790718.2017.1423073,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041007194&doi=10.1080%2f14790718.2017.1423073&partnerID=40&md5=5a1615cd0b03b03be3f7ff558c80dc98,"Bilinguals’ two languages are both active in parallel, and controlling co-activation is one of bilinguals’ principle challenges. Trilingualism multiplies this challenge. To investigate how third language (L3) learners manage interference between languages, Spanish-English bilinguals were taught an artificial language that conflicted with English and Spanish letter-sound mappings. Interference from existing languages was higher for L3 words that were similar to L1 or L2 words, but this interference decreased over time. After mastering the L3, learners continued to experience competition from their other languages. Notably, spoken L3 words activated orthography in all three languages, causing participants to experience cross-linguistic orthographic competition in the absence of phonological overlap. Results indicate that L3 learners are able to control between-language interference from the L1 and L2. We conclude that while the transition from two languages to three presents additional challenges, bilinguals are able to successfully manage competition between languages in this new context. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.",bilingualism; eye-tracking; language learning; Multilingualism,,Article,Final,Scopus,2-s2.0-85041007194,Peter,,
"Tapu R., Mocanu B., Zaharia T.",26424843800;24822846400;6601999900;,Dynamic subtitles: A multimodal video accessibility enhancement dedicated to deaf and hearing impaired users,2019,"Proceedings - 2019 International Conference on Computer Vision Workshop, ICCVW 2019",,,9022026,2558,2566,,6.0,10.1109/ICCVW.2019.00313,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082486712&doi=10.1109%2fICCVW.2019.00313&partnerID=40&md5=ed41ad985e140fdc1da865addd98418a,"In this paper, we introduce a novel dynamic subtitle positioning system designed to increase the accessibility of the deaf and hearing impaired people to video documents. Our framework places the subtitle in the near vicinity of the active speaker in order to allow the viewer to follow the visual content while regarding the textual information. The proposed system is based on a multimodal fusion of text, audio and visual information in order to detect and recognize the identity of the active speaker. The experimental evaluation, performed on a large dataset of more than 30 videos, validates the methodology with average accuracy and recognition rates superior to 92%. The subjective evaluation demonstrates the effectiveness of our approach outperforming both conventional (static) subtitling and other state of the art techniques in terms of enhancement of the overall viewing experience and eyestrain reduction. © 2019 IEEE.",Active speaker detection; Deaf and hearing impaired people; Dynamic subtitle positioning,Character recognition; Computer vision; Large dataset; Speech recognition; Audio and visual information; Deaf and hearing impaired; Experimental evaluation; Multi-modal fusion; Speaker detection; State-of-the-art techniques; Subjective evaluations; Textual information; Audition,Conference Paper,Final,Scopus,2-s2.0-85082486712,Peter,,
"Kim H.-S., Kim K.",56542018100;56019032600;,The Effects of Open Captions in a Medical Drama on the Acquisition of Medical Terminology about Chronic Health Conditions Related to Physical Injury,2019,American Journal of Health Education,50,5,,318,329,,2.0,10.1080/19325037.2019.1642267,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070861410&doi=10.1080%2f19325037.2019.1642267&partnerID=40&md5=c28845ecd449c2bf5b7d0458fcb97b7a,"Background: Despite previous efforts to improve health literacy through entertainment media, current practice seems to address only a few public health topics. Purpose: We examined the impact of supplementary open captions about medical terminology related to physical injuries that might lead to chronic health conditions on the acquisition and retention of relevant information presented in a medical drama. Methods: We conducted a two-group, between-subjects experiment (no open captions vs. open captions) with 150 adult participants to measure how open captions might help viewers retain medical information without disrupting their enjoyment of the storyline. Results: The open captions helped viewers retain the terms and their definitions without disrupting narrative transportation to the events in the episode. Discussion: As long as the open-captioned medical information was seamlessly woven into the storyline of the episode, it did not prevent the viewers from appreciating the dramatic content of the show. Translation to Health Education Practice: Health educators are encouraged to collaborate with media producers to implement open captions for health and medical information rather than simply monitoring the accuracy of health and medical topics featured in television shows. © 2019, © 2019 SHAPE America.",,,Article,Final,Scopus,2-s2.0-85070861410,Peter,,
Teng F.,57190169191;,Maximizing the potential of captions for primary school ESL students’ comprehension of English-language videos,2019,Computer Assisted Language Learning,32,7,,665,691,,20.0,10.1080/09588221.2018.1532912,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059957846&doi=10.1080%2f09588221.2018.1532912&partnerID=40&md5=d68709a8a5747e40a767c5619fd5b559,"This study investigated the effects of captioned videos on ESL primary school students’ comprehension of video content. A total of 182 primary school students watched two short English story videos in one of three conditions: fully captioned videos (N = 62), keyword captioned videos (N = 63), and uncaptioned videos (N = 57). Each group included learners with higher and lower levels of English proficiency. Two videos were selected, and the second video was watched twice. After each video, all participants took a comprehension test, including global comprehension and detailed questions. Findings revealed that fully captioned group achieved the best results on the global comprehension questions. Significant differences between the fully captioned and keyword captioned videos on the detailed comprehension questions were not detected. Learners with a higher level of English proficiency and those who watched the video for a second time achieved better comprehension scores. These findings suggest that full captioning videos, rather than keyword captioning videos, should be considered when using video-based comprehension activities for ESL primary school learners. However, learners’ English level and the frequency of video viewing should also be considered. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.",English proficiency level; frequency; Full captions; KEYWORD captions; video comprehension,,Article,Final,Scopus,2-s2.0-85059957846,Peter,,
"Tao J., Qin Z., Meng Z., Zhang L., Liu L., Yan G., Benson V.",57210986561;57210987435;57203951571;57210988494;57210987847;13607271700;24330546500;,Reading skill modulates the effect of parafoveal distractors on foveal lexical decision in deaf students,2019,PLoS ONE,14,9,e0221891,,,,,10.1371/journal.pone.0221891,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072145790&doi=10.1371%2fjournal.pone.0221891&partnerID=40&md5=40a1a71dc749eb21ad122aefaa597b6a,"In low-level perceptual tasks and reading tasks, deaf individuals show a redistribution of spatial visual attention toward the parafoveal and peripheral visual fields. In the present study, the experiment adopted the modified flanker paradigm and utilized a lexical decision task to investigate how these unique visual skills may influence foveal lexical access in deaf individuals. It was predicted that irrelevant linguistic stimuli presented in parafoveal vision, during a lexical decision task, would produce a larger interference effect for deaf college student readers if the stimuli acted as distractors during the task. The results showed there was a larger interference effect in deaf college student readers compared to the interference effect observed in participants with typical levels of hearing. Furthermore, deaf college student readers with low-skilled reading levels showed a larger interference effect than those with high-skilled reading levels. The current study demonstrates that the redistribution of spatial visual attention toward the parafoveal visual regions in deaf students impacts foveal lexical processing, and this effect is modulated by reading skill. The findings are discussed in relation to the potential effect that enhanced parafoveal attention may have on everyday reading for deaf individuals. © 2019 Tao et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"adult; article; college student; controlled study; female; hearing impaired person; human; human experiment; literacy; male; retina fovea; vision; visual attention; adolescent; attention; case control study; deaf education; hearing impairment; language test; pathophysiology; psychology; reading; retina fovea; semantics; spatial orientation; student; young adult; Adolescent; Attention; Case-Control Studies; Deafness; Education of Hearing Disabled; Female; Fovea Centralis; Humans; Language Tests; Male; Persons With Hearing Impairments; Reading; Recognition, Psychology; Semantics; Spatial Navigation; Students; Young Adult",Article,Final,Scopus,2-s2.0-85072145790,Peter,,
"Martinez D., Singleton J.L.",57214037127;35962027700;,"The effect of bilingualism on lexical learning and memory across two language modalities: some evidence for a domain-specific, but not general, advantage",2019,Journal of Cognitive Psychology,31,5-6,,559,581,,1.0,10.1080/20445911.2019.1634080,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068036995&doi=10.1080%2f20445911.2019.1634080&partnerID=40&md5=bfa7061662386e3ed5cf5d3d21883da8,"The present study was conducted to replicate bilingual advantages in short-term memory for language-like material and word learning in young adults and extend this research to the sign domain, ultimately with the goal of investigating the domain specificity of bilingual advantages in cognition. Data from 112 monolingual hearing non-signers and 78 bilingual hearing non-signers were analysed for this study. Participants completed a battery of tasks assessing sign and word learning, short-term memory, working memory capacity, intelligence, and a language and demographic questionnaire. Overall, the results of this study suggested a bilingual advantage in memory for speech-like material–no other advantage (or disadvantage) was found. Results are discussed within the context of recent large-scale experimental and meta-analytic studies that have failed to find bilingual advantages in domain-general abilities such as attention control and working memory capacity in young adults. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.",Bilingual advantage; lexical learning; short-term memory; sign language; working memory capacity,adult; article; attention; bilingualism; female; hearing; human; human experiment; intelligence; learning; male; questionnaire; short term memory; sign language; speech; working memory; young adult,Article,Final,Scopus,2-s2.0-85068036995,Peter,,
"Suárez M.D.M., Gesa F.",57209227207;57209225977;,Learning vocabulary with the support of sustained exposure to captioned video: do proficiency and aptitude make a difference?,2019,Language Learning Journal,47,4,,497,517,,15.0,10.1080/09571736.2019.1617768,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066883365&doi=10.1080%2f09571736.2019.1617768&partnerID=40&md5=a5cbbfde047dbe78f206e6912b9071bc,"Video viewing can be a valuable resource to expose students to large quantities of input so they can improve their vocabulary and content comprehension. Most studies so far have used short clips and have not explored in much detail the effects of individual differences (IDs) such as aptitude, listening skills and vocabulary size. This paper aims to address this gap by exposing 57 Grade-10 EFL learners and 60 university students to captioned video. On a weekly basis over an academic term, all learners were pre-taught a set of target words (TWs); half of them (the experimental group) were additionally shown captioned episodes from a TV series containing the TWs. All learners were pre- and post-tested on the TW forms and meanings. Results revealed significant differences between experimental and control groups in the learning of TWs in the high school population, but not among university participants. A main effect for proficiency was observed on the learning scores for both TW forms and meanings. However, language aptitude was only a significant factor for TW meanings. Results are discussed regarding how video viewing and these IDs mediate vocabulary learning. © 2019, © 2019 Association for Language Learning.",Captioned video viewing; language aptitude; listening skills; vocabulary learning; vocabulary size,,Article,Final,Scopus,2-s2.0-85066883365,Peter,,
Vanderplank R.,24475848800;,"‘Gist watching can only take you so far’: attitudes, strategies and changes in behaviour in watching films with captions",2019,Language Learning Journal,47,4,,407,423,,9.0,10.1080/09571736.2019.1610033,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065320449&doi=10.1080%2f09571736.2019.1610033&partnerID=40&md5=5f1d1fcdeb62d485330abd2220b7fe1c,"Watching foreign language programmes and films with captions (same-language subtitles intended for the deaf and hard-of-hearing) has been shown to assist learners in phonetic retuning, vocabulary acquisition and listening comprehension [e.g. Mitterer and McQueen, 2009. Foreign subtitles help but native-language subtitles harm foreign speech perception. PLOS ONE 4, no. 1: e7785. doi:10.1371/journal.pone.0007785; Montero Perez, Van Den Noortgate and Desmet, 2013. captioned video for L2 listening and vocabulary learning: A meta-analysis. System 41, no. 3: 720–739; Rodgers and Webb, 2017. The effects of captions on EFL learners’ comprehension of English-language television programs. CALICO Journal 34, no. 1: 20–38]. However, relatively little is known about the outcomes and changes in viewing behaviour when learners watch with captions over an extended period if choice and control of viewing material are provided. This article reports qualitative findings of the EURECAP Project in which 36 learners of French, German, Italian and Spanish at intermediate level and above in a large UK university chose from a wide range of films on DVD with optional captions and watched them under their control in their own time. Their experiences as noted in viewing diaries revealed wide differences in viewing behaviour, attitudes to watching with captions and caption-guided viewing strategies over the period of the study. Participants fell into three broad strategic categories: Minimal users who were focused throughout on enjoying films as they would in their L1, evolving users who showed marked changes in their viewing behaviour over time, and maximal users who tended to be experienced at using films to enhance their language learning. We also compare participant behaviour with models of multimedia learning and suggest implications for future practice and research, especially in the light of global video streaming services with captions. © 2019, © 2019 Association for Language Learning.",audio-visual; captioned films; captions; multimedia; strategies; Subtitles; television,,Article,Final,Scopus,2-s2.0-85065320449,Peter,,
"Pulsiri N., Vatananan-Thesenvitz R., Tantipisitkul K., Aung T.H., Schaller A.-A., Schaller A.-M., Methananthakul K., Shannon R.",57204636695;57193733416;57211998922;57211998227;57213325405;57204637271;57211998630;21740377500;,Achieving sustainable development goals for people with disabilities through digital technologies,2019,"PICMET 2019 - Portland International Conference on Management of Engineering and Technology: Technology Management in the World of Intelligent Systems, Proceedings",,,8893725,,,,4.0,10.23919/PICMET.2019.8893725,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075638942&doi=10.23919%2fPICMET.2019.8893725&partnerID=40&md5=4bda2d4f6a062fbc3d059638825f9016,"The world is moving towards an aging society, which causes a rise in the population of people with disabilities. A lot of attention is given to the use of digital technologies that can facilitate activities and solve issues in the daily life of a disabled person. The Sustainable Development Goals (SDGs) initiative launched by the United Nations focuses on the development of a blueprint to achieve a better and more sustainable future for all. In this view, digital technologies are promising to support the development and to help achieve the SDGs. This paper outlines a systematic literature review with bibliometric analysis of scientific publications to provide a connection between digital technologies for persons with disabilities and the SDGs. The results show the clusters of digital technologies and their research collaboration networks. The review intends to provide a knowledge base and evaluation of applications for these technologies. It is the intention of this paper to facilitate the understanding of scientists and policy makers in how digital technologies can address the SDGs and which ones will be most relevant in 2030. © 2019 PICMET.",,Intelligent systems; Knowledge based systems; Planning; Sustainable development; Aging societies; Bibliometric analysis; Digital technologies; People with disabilities; Persons with disabilities; Research collaborations; Scientific publications; Systematic literature review; Disabled persons,Conference Paper,Final,Scopus,2-s2.0-85075638942,Peter,,
"Jiang J.-Y., Guo F., Chen J.-H., Tian X.-H., Lv W.",57193115996;55202044200;57209024290;57209024347;57204544133;,Applying eye-tracking technology to measure interactive experience toward the navigation interface of mobile games considering different visual attention mechanisms,2019,Applied Sciences (Switzerland),9,16,3242,,,,10.0,10.3390/app9163242,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070991243&doi=10.3390%2fapp9163242&partnerID=40&md5=207fb30d3b9de8fbe3acde7ff9c82155,"As an initial channel for users learning about a mobile game, the interactive experience of the navigation interface will directly affect the first impression of the users on the game and their subsequent behaviors and willingness to use. This study aims to investigate players' visual attention mechanisms of various interactive levels of mobile games' interfaces under free-browsing and task-oriented conditions. Eye-tracking glasses and a questionnaire were used to measure the interactive experience of mobile games. The results show that in the free-browsing condition, the fixation count, saccade count and average saccade amplitude can be used to reflect and predict the interactive experiences of mobile games' navigation interface; while in the task-oriented condition, the fixation count, first fixation duration, dwell time ratio and saccade count can be used to reflect and predict the interactive experience of mobile games' navigation interface. These findings suggest that apart from the different eye movement indicators, players' motivations should also be considered during the process of the games' navigation interface design. © 2019 by the authors.",Bottom-up; Eye tracking; Fixation count; Interactive experience; Mobile game; Navigation interface; Top-down,,Article,Final,Scopus,2-s2.0-85070991243,Peter,,
Lyrigkou C.,57200407916;,Not to be overlooked: agency in informal language contact,2019,Innovation in Language Learning and Teaching,13,3,,237,252,,11.0,10.1080/17501229.2018.1433182,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041237355&doi=10.1080%2f17501229.2018.1433182&partnerID=40&md5=766911450d497e04d6f413a3b6c005a2,"Easy and rapid access to the Internet, the affordances of Web 2.0 tools and technological advances, such as smartphone applications, have rendered informal contact with English an undisputed reality. This out-of-class contact without the primary or conscious objective of language learning has been the focus of recent research into the field of second language acquisition (e.g. Jarvis and Krashen 2014. “Is CALL Obsolete? Language Acquisition and Language Learning Revisited in a Digital age.” TESL-EJ 17 (4): 1–6; Sockett 2014. The Online Informal Learning of English. UK: Palgrave Macmillan). However, being in its infancy, the phenomenon is under-explored, while the learner, as a complex individual, is not examined in depth. Focusing on the skill of speaking, which is not sufficiently studied in research into informal learning, the present study investigated 76 Greek adolescent learners of English between the ages of 13 and 16, and used a speaking test, a questionnaire and a semi-structured interview to examine participants’: speaking performance, frequency of interaction with English through a range of media, and their agency and exerted effort during informal contact with the language. Findings showed that students’ lack of agency and their attitudes towards informal learning appeared to hinder their active engagement with informal sources, impeding further linguistic benefits. The present study underlines the importance of examining different individual and contextual factors before drawing conclusions regarding the effect of informal, out-of-class interaction with the target language. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.",adolescents; informal language contact; informal learning; Learner agency; speaking,,Article,Final,Scopus,2-s2.0-85041237355,Peter,,
"Wang X., Li D.",57194544292;35215276400;,Processing of Phonological and Orthographic Information in Word Recognition in Discourse Reading,2019,SAGE Open,9,3,,,,,,10.1177/2158244019861502,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068828693&doi=10.1177%2f2158244019861502&partnerID=40&md5=8dd651c42c06c5e7ee21234aef1e2dd0,"To examine the processing of phonological and configurational information in word recognition in discourse reading, we conducted two experiments using the self-paced reading paradigm. The materials were three-sentence discourses, in each of which the last word of the second sentence and the third word from the end of the last sentence formed a prime–target pair. The discourse in which the target word (T) was semantically congruent or incongruent with the prime word was converted into a new version by replacing the T with its homophone or with the control word (con-T) in Experiment 1. Similarly, the Ts were replaced by words that were similar to them in configuration or by the con-Ts in Experiment 2. We adopted mixed-effects modeling to analyze the participants’ reading times to the targets, the first words after the targets, and the second words after the targets. It is concluded that the processing of phonological information begins earlier than that of configurational information in activating the semantic representations for the upcoming words that fit the context in discourse reading. © The Author(s) 2019.",Chinese words; discourse; orthography; phonology; recognition,,Article,Final,Scopus,2-s2.0-85068828693,Peter,,
"Li D., Wang S., Zhang F., Zhu L., Wang T., Wang X.",35215276400;57209284889;57209285708;57209281125;57211656066;57077301900;,DHH students' comprehension of irony in self-paced reading,2019,Journal of Deaf Studies and Deaf Education,24,3,,270,279,,3.0,10.1093/deafed/enz009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067170236&doi=10.1093%2fdeafed%2fenz009&partnerID=40&md5=f2783fc244e91b6f54dc1cbddbc20c75,"Irony comprehension can be a kind of challenge to those who are relatively less skillful in reading. To examine how DHH college students (DCSs) were different from hearing college students (HCSs) in the reading of ironic discourses, we conducted two experiments in the self-paced reading task. In Experiment 1, the statement was either literally congruent with the preceding context or had to be understood in an ironic way in each trial; In Experiment 2, the statement was the same but the context was not across the two levels of discourse type. The DCSs generally had a poorer performance than the hearing participants. Although able to comprehend ironies, they had a significantly lower efficiency than their hearing counterparts. The results were consistently in agreement with the prediction of the graded salience hypothesis (Giora, R. (1997). Understanding figurative and literal language: The graded salience hypothesis. Cognitive Linguistics, 7, 183-206. doi:10.1515/ cogl.1997.8.3.183) and the parallel-constraint-satisfaction framework (Pexman, P. M. (2008). It's fascinating research. Current Directions in Psychological Science, 17(4)286-290. doi:10.1111/j.1467-8721.2008.00591.x), and the DCSs' performance appears to indicate an amplified version of this support. It is implied that educational environments should be created in which deaf and DHH students are encouraged to do as much reading as possible. Exercises should be designed in helping them to improve vocabulary and syntactic skills in general and to improve skills of inference-making in particular. © The Author(s) 2019.",,adult; article; college student; comprehension; controlled study; exercise; female; hearing; human; human experiment; linguistics; male; prediction; reading test; satisfaction; skill; vocabulary; comprehension; emotion; hearing disorder; hearing impaired person; language; physiology; psychology; reading; student; young adult; Comprehension; Emotions; Female; Hearing Disorders; Humans; Language; Male; Persons With Hearing Impairments; Reading; Students; Young Adult,Article,Final,Scopus,2-s2.0-85067170236,Peter,,
"Minatani K., Yokota K.",24725081700;57209979199;,Launching Experiment-driven Research and maintaining Research Community for Assistive Technology: A look at Experiments in Presentations given to the Technical Committee on Well-being Information Technology,2019,ACM International Conference Proceeding Series,,,,172,175,,,10.1145/3316782.3321540,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069158853&doi=10.1145%2f3316782.3321540&partnerID=40&md5=0a3fc85ab843fd4b8a1458b0d1ba2aed,"The Technical Committee on Well-being Information Technology is a research group that has been steadily holding meetings on assistive technology in Japan for twenty years. This study looked at the way experiments presented to the committee were conducted between 2013 and 2015 in order to collect basic data that would contribute to alleviating the difficulty of conducting experiments having persons with disabilities as participants. A total of 330 presentations were given during the study period. The median number of participants in experiments having persons with disabilities as participants in was four, and the mode was one. The median number of participants in experiments having able-bodied persons as participants in was six, and the mode was six. © 2019 Association for Computing Machinery.",Assistive technology; Experiment; Persons with disabilities; Well-being information technology,Computer programming; Experiments; Assistive technology; Persons with disabilities; Research communities; Research groups; Technical committees; Well being; Computer applications,Conference Paper,Final,Scopus,2-s2.0-85069158853,Peter,,
"Szarkowska A., Dutka Ł., Pilipczuk O., Krejtz K.",54416458200;57190034357;56018914300;55258716700;,Respeaking crisis points. An exploratory study into critical moments in the respeaking process,2019,Audiovisual Translation - Research and Use: 2nd Expanded Edition,,,,193,215,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117450252&partnerID=40&md5=f342dd210504df0df7a65620b55ae785,"In this paper we introduce respeaking crisis points (RCPs), understood as potentially problematic moments in the respeaking process, resulting from the difficulty of the source material and/or cognitive overload on the part of the respeakers. We present results of the respeaking study on Polish participants who respoke four videos intralingually (Polish to Polish) and one interlingually (from English to Polish). We measured the participants' cognitive load with EEG (Emotiv) using two measures: concentration and frustration. By analysing peaks in both EEG measures, we show where respeaking crisis points occurred. Features that triggered RCPs include very slow and very fast speech rate, visual complexity of the material, overlapping speech, numbers and proper names, speaker changes, word play, syntactic complexity, and implied meaning. The results of this study are directly applicable to respeaker training and provide valuable insights into the respeaking process from the cognitive perspective. © Peter Lang GmbH Internationaler Verlag der Wissenschaften Berlin 2017. All rights reserved.",Audiovisual translation; Cognitive load; Live subtitling; Respeaking; Respeaking crisis points,,Book Chapter,Final,Scopus,2-s2.0-85117450252,Peter,,
"Berke L., Huenerfauth M., Patel K.",57200494669;12240800100;57188741594;,Design and psychometric evaluation of American sign language translations of usability questionnaires,2019,ACM Transactions on Accessible Computing,12,2,a6,,,,7.0,10.1145/3314205,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067240514&doi=10.1145%2f3314205&partnerID=40&md5=69fb759ad188215095242f29c4e86835,"To promote greater inclusion of people who are Deaf and Hard of Hearing (DHH) in studies conducted by Human-Computer Interaction (HCI) researchers or professionals, we have undertaken a project to formally translate several standardized usability questionnaires from English to ASL. Many deaf adults in the U.S. have lower levels of English reading literacy, but there are currently no standardized usability questionnaires available in American Sign Language (ASL) for these users. A critical concern in conducting such a translation is to ensure that the meaning of the original question items has been preserved during translation, as well as other key psychometric properties of the instrument, including internal reliability, criterion validity, and construct validity. After identifying best-practices for such a translation and evaluation project, a bilingual team of domain experts (including native ASL signers who are members of the Deaf community) translated the System Usability Scale (SUS) and Net Promoter Score (NPS) instruments into ASL and then conducted back-translation evaluations to assess the faithfulness of the translation. The new ASL instruments were employed in usability tests with DHH participants, to assemble a dataset of response scores, in support of the psychometric validation.We are disseminating these translated instruments, as well as collected response values from DHH participants, to encourage greater participation in HCI studies among DHH users. © 2019 Association for Computing Machinery.",American Sign Language; ASL-NPS; ASL-SUS; Construct validity; Criterion validity; Factor analysis; Internal reliability; Net promoter score; NPS; SUS; System usability scale; Translation,Audition; Factor analysis; Human computer interaction; Neptunium; Reliability analysis; Statistical tests; Surveys; Usability engineering; American sign language; ASL-SUS; Construct validity; Criterion validity; Internal reliabilities; Net promoter score; System usability; Translation (languages),Article,Final,Scopus,2-s2.0-85067240514,Peter,,
"Findlater L., Froehlich J., Chinh B., Kushalnagar R., Jain D., Lin A.C.",10040303000;7101665384;57197731463;36142036500;57014317900;57209399065;,Deaf and Hard-of-hearing Individuals’ Preferences for Wearable and Mobile Sound Awareness Technologies,2019,Conference on Human Factors in Computing Systems - Proceedings,,,,,,,24.0,10.1145/3290605.3300276,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067608915&doi=10.1145%2f3290605.3300276&partnerID=40&md5=3877a39b041658c74eecbf0abc54d4b8,"To investigate preferences for mobile and wearable sound awareness systems, we conducted an online survey with 201 DHH participants. The survey explores how demographic factors affect perceptions of sound awareness technologies, gauges interest in specific sounds and sound characteristics, solicits reactions to three design scenarios (smartphone, smartwatch, head-mounted display) and two output modalities (visual, haptic), and probes issues related to social context of use. While most participants were highly interested in being aware of sounds, this interest was modulated by communication preference–that is, for sign or oral communication or both. Almost all participants wanted both visual and haptic feedback and 75% preferred to have that feedback on separate devices (e.g., haptic on smartwatch, visual on head-mounted display). Other findings related to sound type, full captions vs. keywords, sound filtering, notification styles, and social context provide direct guidance for the design of future mobile and wearable sound awareness systems. © 2019 Association for Computing Machinery.",Deaf; Hard of hearing; Hearing loss; Mobile; Online survey; Sound awareness; User study; Wearable,Audition; Bioinformatics; Helmet mounted displays; Human computer interaction; Human engineering; Online systems; Wearable computers; Deaf; Hard of hearings; Hearing loss; Mobile; Online surveys; User study; Wearable; Surveys,Conference Paper,Final,Scopus,2-s2.0-85067608915,Peter,,
"Lee S., Hubert-Wallander B., Stevens M., Carroll J.M.",57189050289;46161220400;57209519918;7402034833;,Understanding and designing for deaf or hard of hearing drivers on uber,2019,Conference on Human Factors in Computing Systems - Proceedings,,,,,,,4.0,10.1145/3290605.3300759,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067606761&doi=10.1145%2f3290605.3300759&partnerID=40&md5=97e54472825ea978357e4b5de3ca476e,"We used content analysis of in-app driver survey responses, customer support tickets, and tweets, and face-to-face interviews of DHH Uber drivers to better understand the DHH driver experience. Here we describe challenges DHH drivers experience and how they address those difficulties via Uber’s accessibility features and their own workarounds. We also identify and discuss design and product opportunities to improve the DHH driver experience on Uber. © 2019 Copyright held by the owner/author(s).",Accessibility; Communication; Deaf or hard of hearing drivers; Uber,Communication; Human computer interaction; Human engineering; Product design; Accessibility; Content analysis; Customer support; Driver experience; Face-to-face interview; Hard of hearings; Uber; Audition,Conference Paper,Final,Scopus,2-s2.0-85067606761,Peter,,
"Vinayagamoorthy V., Glancy M., Ziegler C., Schäffer R.",8124651300;16309279800;57195130003;57203214007;,Personalising the TV Experience using Augmented Reality An Exploratory Study on Delivering Synchronised Sign Language Interpretation,2019,Conference on Human Factors in Computing Systems - Proceedings,,,,,,,17.0,10.1145/3290605.3300762,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067603946&doi=10.1145%2f3290605.3300762&partnerID=40&md5=7a666d8824d400cc5f883ff2388a58f1,"Augmented Reality (AR) technology has the potential to extend the screen area beyond the rigid frames of televisions. The additional display area can be used to augment televisions (TVs) with extra information tailored to individuals, for instance, the provision of access services like sign language interpretations. We invited 23 (11 in the UK, 12 in Germany) users of signed content to evaluate three methods of watching a sign language interpreted programme – one traditional in-vision method with signed programme content on TV and two AR-enabled methods in which an AR sign language interpreter (a ‘half-body’ version and a ‘full-body’ version) is projected just outside the frame of the TV presenting the programme. In the UK, participants were split 3-ways in their preferences while in Germany, half the participants preferred the traditional method followed closely by the ‘half-body’ version. We discuss our participants reasoning behind their preferences and implications for future research. © 2019 Copyright held by the owner/author(s).",Accessibility; Augmented reality; BSL; Companion screen; Connected experiences; DGS; HbbTV 2.0; HoloLens; Interaction techniques; Personalisation; Second screen; Sign language; SSE; Synchronisation; Television,Binary alloys; Human computer interaction; Human engineering; Synchronization; Television; Accessibility; Connected experiences; HbbTV 2.0; HoloLens; Interaction techniques; Personalisation; Second screens; Sign language; Augmented reality,Conference Paper,Final,Scopus,2-s2.0-85067603946,Peter,,
"Berke L., Seita M., Albusays K., Huenerfauth M.",57200494669;57192541826;57189681874;12240800100;,Preferred appearance of captions generated by automatic speech recognition for deaf and hard-of-hearing viewers,2019,Conference on Human Factors in Computing Systems - Proceedings,,,3312921,,,,12.0,10.1145/3290607.3312921,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067306388&doi=10.1145%2f3290607.3312921&partnerID=40&md5=04c438171a18fe37f63283a2073049bd,"As the accuracy of Automatic Speech Recognition (ASR) nears human-level quality, it might become feasible as an accessibility tool for people who are Deaf and Hard of Hearing (DHH) to transcribe spoken language to text. We conducted a study using in-person laboratory methodologies, to investigate requirements and preferences for new ASR-based captioning services when used in a small group meeting context. The open-ended comments reveal an interesting dynamic between: caption readability (visibility of text) and occlusion (captions blocking the video contents). Our 105 DHH participants provided valuable feedback on a variety of caption-appearance parameters (strongly preferring familiar styles such as closed captions), and in this paper we start a discussion on how ASR captioning could be visually styled to improve text readability for DHH viewers. © 2019 Copyright held by the owner/author(s).",Appearance; Automatic Speech Recognition; Captioning; Deaf and Hard-of-Hearing; User Interface,Audition; Character recognition; Human computer interaction; Human engineering; User interfaces; Appearance; Automatic speech recognition; Captioning; Hard of hearings; Human levels; Spoken languages; Video contents; Speech recognition,Conference Paper,Final,Scopus,2-s2.0-85067306388,Peter,,
"Engelberg M., Nakaji M.C., Harry K.M., Wang R.M., Kennedy A., Pan T.M., Sanchez T., Sadler G.R.",56931470000;25522295300;48861419000;57226734655;57198448465;57189222112;55588227000;7005152183;,Promotion of Healthy Humor Cancer Education Messages for the Deaf Community,2019,Journal of Cancer Education,34,2,,323,328,,1.0,10.1007/s13187-017-1305-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036528054&doi=10.1007%2fs13187-017-1305-5&partnerID=40&md5=0bed3d172fb9e4a47643d5840faadbe4,"The Deaf community members of this community-campus partnership identified the lack of health information in American Sign Language (ASL) as a significant barrier to increasing the Deaf community’s health knowledge. Studies have shown that the delivery of health messages in ASL increased Deaf study participants’ cancer knowledge. Once health messages are available on the Internet, strategies are needed to attract viewers to the website and to make repeat visits in order to promote widespread knowledge gains. This feasibility study used the entertainment-education strategy of coupling cancer information with jokes in ASL to increase the appeal and impact of the health messages. ASL-delivered cancer control messages coupled with Deaf-friendly jokes were shown to 62 Deaf participants. Participants completed knowledge questionnaires before, immediately after, and 1 week after viewing the paired videos. Participants’ health knowledge statistically significantly increased after viewing the paired videos and the gain was retained 1 week later. Participants also reported sharing the newly acquired information with others. Statistically significant results were demonstrated across nearly all measures, including a sustained increase in cancer-information-seeking behavior and intent to improve health habits. Most participants reported that they would be motivated to return to such a website and refer others to it, provided that it was regularly updated with new jokes. © 2017, American Association for Cancer Education.",Cancer; Deaf community; Dissemination; Education; Edutainment; Entertainment-education; Health disparities; Humor; Prevention,"adolescent; adult; attitude to health; feasibility study; female; health education; hearing impaired person; human; humor; male; middle aged; neoplasm; procedures; sign language; young adult; Adolescent; Adult; Feasibility Studies; Female; Health Education; Health Knowledge, Attitudes, Practice; Humans; Male; Middle Aged; Neoplasms; Persons With Hearing Impairments; Sign Language; Wit and Humor as Topic; Young Adult",Article,Final,Scopus,2-s2.0-85036528054,Peter,,
Teng F.,57190169191;,Incidental vocabulary learning for primary school students: the effects of L2 caption type and word exposure frequency,2019,Australian Educational Researcher,46,1,,113,136,,22.0,10.1007/s13384-018-0279-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059939447&doi=10.1007%2fs13384-018-0279-6&partnerID=40&md5=6b04bafe0c57a4adba5920b97b27838f,"Within instructed second language research, there is growing interest in research focusing on primary school vocabulary learning. Research has emphasized classroom-based learning of vocabulary knowledge, with growing focus on the potential for using captioned videos and increased word encounters. The present study investigated the effects of various captioning conditions (i.e. full captioning, keyword captioning, and no captions), the number of word encounters (one and three), and the combinations of these two variables on incidental learning of new words while viewing a video. Six possible conditions were explored. A total of 257 primary school students learning English as a second language (ESL) were divided into six groups and randomly assigned to a condition in which 15 target lexical items were included. A post-test, measuring the recognition of word form/meaning and recall of word meaning, was administered immediately after participants viewed the video. The post-test was not disclosed to the learners in advance. The group viewing the full captioning video scored significantly higher than the keyword captioning group and the no-captioning group. Repeated encounters with the targeted lexical items led to more successful learning. The combination of full captioning and three encounters was most effective for incidental learning of lexical items. This quasi-experimental study contributes to the literature by providing evidence which suggests that captioned videos coordinate two domains (i.e. auditory and visual components) and help ESL learners to obtain greater depth of word form processing, identify meaning by unpacking language chunks, and reinforce the form-meaning link. © 2018, The Australian Association for Research in Education, Inc.",Frequency; Full captions; Incidental vocabulary learning; Keyword captions,,Article,Final,Scopus,2-s2.0-85059939447,Peter,,
"Szarkowska A., Bogucka L.",54416458200;57226316543;,Six-second rule revisited An eye-tracking study on the impact of speech rate and language proficiency on subtitle reading,2019,"Translation, Cognition and Behavior",2,1,,101,124,,15.0,10.1075/tcb.00022.sza,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086006221&doi=10.1075%2ftcb.00022.sza&partnerID=40&md5=dd753dc0013a2a5e3b1038e30f57babd,"The most famous rule on the speed of subtitles is the six-second rule. In this study we investigate if the six-second rule is too slow for contemporary viewers. We also address the question of whether subtitle processing depends on the speech rate of film dialogues and on viewer's proficiency in the language of the film soundtrack. With these questions in mind, we tested 53 Polish viewers watching English videos at two different speech rates (slow and fast), subtitled into Polish in accordance with the six-second rule. We examined participants' reading patterns and comprehension and asked them to assess subtitle speed and the congruity of subtitles with the dialogue. Analysing people's eye movements enabled us to measure that viewers were looking at subtitles for only about 30% of the subtitle display time. We found that the film speech rate affected comprehension: faster dialogues, implying more text condensation in subtitles, resulted in lower comprehension compared to slow speech rates. Viewers more proficient in the language of the film soundtrack spent less time gazing at subtitles than those who had only elementary knowledge of the language. © 2021 Translation, Cognition and Behavior. All rights reserved.",Audiovisual translation; Eyetracking; Reading speed; Six-second rule; Speech rate; Subtitling,,Article,Final,Scopus,2-s2.0-85086006221,Peter,,
"Anderson K.L., Balandin S.",55326030600;57203104315;,Kicking a goal for inclusion in sports clubs and stadia,2019,"Inclusion, Equity and Access for Individuals with Disabilities: Insights from Educators across World",,,,297,316,,2.0,10.1007/978-981-13-5962-0_15,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078642740&doi=10.1007%2f978-981-13-5962-0_15&partnerID=40&md5=7f7319b6a927fbb4bc05315e3534b5b8,[No abstract available],,,Book Chapter,Final,Scopus,2-s2.0-85078642740,Peter,,
"Benson T.L., Brugger P., Park S.",55538055400;57191210382;7501834137;,Bodily self-disturbance in schizophrenia-spectrum populations: Introducing the Benson et al. Body Disturbances Inventory (B-BODI),2019,PsyCh Journal,8,1,,110,121,,13.0,10.1002/pchj.280,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063402246&doi=10.1002%2fpchj.280&partnerID=40&md5=54934a90bea54da7001cb9e95367f8fc,"Anomalous or weakened sense of self was central to early theories of schizophrenia. Recent studies have also documented disturbances in body ownership and increased susceptibility for dissociative experiences, such as the out-of-body experience in individuals with schizophrenia, but further research is necessary to clarify components of bodily self-disturbances in the schizophrenia spectrum, and the stability of these experiences over time. With respect to methodology, self-disturbances research in schizophrenia tends to rely exclusively on verbal self-report questionnaires and interviews. Given that individuals with schizophrenia suffer from language and communication difficulties, verbal self-report measures may be insufficient. To bridge this gap, we have developed a new picture-based instrument, the Benson et al. Body Disturbances Inventory (B-BODI), designed to quantify bodily self-disturbances with respect to the frequency and vividness of these experiences, as well as the degree of distress associated with them. Drawings that depicted different aspects of aberrant bodily self-experiences were presented with accompanying captions. Participants were asked to indicate the frequency, vividness, and distressfulness of the experience captured by the picture using a 5-point scale. Individuals with schizophrenia, older healthy controls, and college students participated in two alternative versions of the B-BODI. Participants were also asked to complete a battery of established questionnaires that probed psychosis proneness and a range of self, body, and perceptual aberrations. The results suggest that the B-BODI is a useful tool that accurately captures bodily self-disturbances and has the potential to predict psychosis risk in healthy young individuals. Furthermore, anomalous self-disturbances seem to be relatively stable across time in individuals with chronic schizophrenia. © 2019 The Institute of Psychology, Chinese Academy of Sciences and John Wiley & Sons Australia, Ltd",bodily self; dissociation; phenomenology; schizophrenia; self disturbance,adult; body image; female; human; male; middle aged; neuropsychological test; pathophysiology; psychosis; schizophrenia; self concept; young adult; Adult; Body Image; Female; Humans; Male; Middle Aged; Neuropsychological Tests; Psychotic Disorders; Schizophrenia; Self Concept; Young Adult,Article,Final,Scopus,2-s2.0-85063402246,Peter,,
"Brooks P.J., Kempe V.",7202275220;8589051200;,More Is More in Language Learning: Reconsidering the Less-Is-More Hypothesis,2019,Language Learning,69,,,13,41,,8.0,10.1111/lang.12320,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053502355&doi=10.1111%2flang.12320&partnerID=40&md5=2b69d1763c5e5f707316b31db908e47f,"The Less-Is-More hypothesis was proposed to explain age-of-acquisition effects in first language (L1) and second language (L2) learning. We scrutinize different renditions of the hypothesis by examining how learning outcomes are affected by (a) limited cognitive capacity, (b) reduced interference resulting from less prior knowledge, and (c) simplified language input. While there is little to no evidence of benefits of limited cognitive capacity, there is ample support for a More-Is-More account linking enhanced capacity with better L1 and L2 learning outcomes and reduced capacity with childhood language disorders. Instead, reduced prior knowledge (relative to adults) may afford children greater flexibility in inductive inference; this contradicts the idea that children benefit from a more constrained hypothesis space. Finally, studies of child-directed speech confirm benefits from less complex input at early stages but also emphasize how greater lexical and syntactic complexity of the input confers benefits in L1 attainment. © 2018 Language Learning Research Club, University of Michigan",cognitive control; development; individual differences; less is more; more is more; processing capacity; second language; working memory,,Article,Final,Scopus,2-s2.0-85053502355,Peter,,
"Lauro J., Schwartz A.I.",57190284038;14036588900;,Cognate effects on anaphor processing,2019,Journal of Experimental Psychology: Learning Memory and Cognition,45,3,,381,396,,3.0,10.1037/xlm0000601,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049779946&doi=10.1037%2fxlm0000601&partnerID=40&md5=7e605e2d003b48ab4f3ebc850e08e288,"There are numerous studies demonstrating facilitated processing of cognates relative to noncognates for bilinguals, providing evidence that bilingual lexical access is language nonselective. We tested whether cross-language activation affects comprehension of larger units of meaning, focusing specifically on comprehension of anaphoric references. Highly proficient, Spanish-English bilinguals read sentences either in English (Experiment 1) or Spanish (Experiment 2) while their eye movements were recorded. Sentences consisted of an initial clause with 2 nouns that were either cognates or noncognates, and a later clause with an anaphor that either referred to the first or second noun. In the English experiment, cognate status facilitated selection of the sentence's foundational noun, reflected by shorter reading times for cognate nouns in the first position. Processing of pronouns was facilitated when they referred to cognates, reflected by higher skipping rates and shorter reading times. Final selection of cognate referents was also facilitated, reflected by total reading shorter total reading times, but only when the pronoun referred to the first noun. In the Spanish experiment, total reading times for cognate nouns were shorter, irrespective of their order of mention, reflecting a general cognate facilitation effect that was not affected by which noun was selected as the foundational structure. Spillover fixations from anaphors referring to cognates were shorter than noncognates, but only when they were the second-mentioned noun, suggesting that cognate status affected coreferencing for the more recently encountered noun. Implications for theories of cross-language activation and anaphoric reference are discussed. © 2018 American Psychological Association.",Anaphor processing; Bilingualism; Cognate facilitation; Lexical access; Sentence processing,adult; comprehension; human; multilingualism; oculography; physiology; psycholinguistics; reading; young adult; Adult; Comprehension; Eye Movement Measurements; Humans; Multilingualism; Psycholinguistics; Reading; Young Adult,Article,Final,Scopus,2-s2.0-85049779946,Peter,,
"Szarkowska A., Gerber-Morón O.",54416458200;57202588774;,Two or three lines: a mixed-methods study on subtitle processing and preferences,2019,Perspectives: Studies in Translation Theory and Practice,27,1,,144,164,,13.0,10.1080/0907676X.2018.1520267,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053735160&doi=10.1080%2f0907676X.2018.1520267&partnerID=40&md5=e797a51e5e3ca4c97e93e114d1e38f73,"The typically recommended maximum number of lines in a subtitle is two. Yet, three-line subtitles are often used in intralingual English-to-English subtitling on television programmes with high information density and fast speech rates. To the best of our knowledge, no prior empirical work has contrasted the processing of three-line with two-line subtitles. In this study, we showed participants one video with two-line subtitles and one with three-line subtitles. We measured the impact of the number of lines on subtitle processing using eye tracking as well as comprehension, cognitive load, enjoyment and preferences. We conducted two experiments with different types of viewers: hearing native speakers of English, Polish and Spanish as well as British hard of hearing and deaf viewers. Three-line subtitles induced higher cognitive load than two-line subtitles. The number of lines did not affect comprehension. Viewers generally preferred two-line over three-line subtitles. The results provide empirical evidence on the processing of two- and three-line subtitles and can be used to inform current subtitling practices. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.",audiovisual translation; cognitive load; eye tracking; hearing loss; number of lines; preferences; Subtitling,,Article,Final,Scopus,2-s2.0-85053735160,Peter,,
"Tor-Carroggio I., Orero P.",57211458241;24921771100;,User profiling in audio description reception studies: questionnaires for all,2019,inTRAlinea,21,,,,,,1.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100810278&partnerID=40&md5=cec6046e454f1addf95ddc6b6babc601,"Defining disability is not an easy task due to its multidimensionality. This paper begins with a revision of some of the most common models to define disability. The second part of the article examines end user profiling in articles, European funded projects and PhD thesis' related to one of the media accessibility modalities: audio description. The objective is to understand the approach taken by researchers. The final part of the article will propose a new approach in the study of end users in experimental research in Translation Studies, Audiovisual Translation, and Media Accessibility. This new approach gives a response to the International Telecommunication Union's suggestion of leaving the biomedical approaches behind. Our suggestion is based on Amartya Sen's capabilities approach, which has not yet been applied to user profiling in media accessibility studies. The article finishes by illustrating how this approach can be applied when profiling users in media accessibility questionnaires. © in TRAlinea & Irene Tor-Carroggio & Pilar Orero (2019)",audio description; capabilities; media accessibility; models of disability,,Article,Final,Scopus,2-s2.0-85100810278,Peter,,
"Halvorsen K.E., Schelly C., Handler R.M., Pischke E.C., Knowlton J.L.",7004691109;6506572098;55003492400;57192402479;57204340898;,A research agenda for environmental management,2019,A Research Agenda for Environmental Management,,,,1,222,,3.0,10.4337/9781788115193,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087729004&doi=10.4337%2f9781788115193&partnerID=40&md5=e8a71e6653d3c85ede1a0e98a1895753,"The understanding of global environmental management problems is best achieved through transdisciplinary research lenses that combine scientific and other sector (industry, government, etc.) tools and perspectives. However, developing effective research teams that cross such boundaries is difficult. This book demonstrates the importance of transdisciplinarity, describes challenges to such teamwork, and provides solutions for overcoming these challenges. It includes case studies of transdisciplinary teamwork, showing how these solutions have helped groups to develop better understandings of environmental problems and potential responses. © Kathleen E. Halvorsen, Chelsea Schelly, Robert M. Handler, Erin C. Pischke and Jessie L. Knowlton 2019.",,,Book,Final,Scopus,2-s2.0-85087729004,Peter,,
"Iturregui-Gallardo G., Matamala A.",57203986821;25921557700;,Audio subtitling: dubbing and voice-over effects and their impact on user experience,2019,Perspectives: Studies in Translation Theory and Practice,,,,,,,2.0,10.1080/0907676X.2019.1702065,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077878526&doi=10.1080%2f0907676X.2019.1702065&partnerID=40&md5=0fb8414c200429628cab5855ca1b99e1,"This article reports the outcome of an experiment in which 42 Spanish blind and partially sighted participants were exposed to two diverging audio subtitling strategies: audio subtitles with a voice-over effect and audio subtitles with a dubbing effect. Data on the users’ emotional responses were collected through a tactile and simplified version of the SAM questionnaire and psychophysiological recordings of electrodermal activity and heart rate. The results obtained from both methods do not show statistically significant differences between the two effects. However, results from the questionnaire proved that emotions were induced in the participants calling for more research on the topic and with the application of such methods. © 2020, © 2020 Informa UK Limited, trading as Taylor & Francis Group.",audio subtitling; Audiovisual translation; emotions; media accessibility; psychophysiology,,Article,Article in Press,Scopus,2-s2.0-85077878526,Peter,,
"Caro R.B., Clavero P.O.",57212877775;57188670426;,Easy to read as multimode accessibility service,2019,Hermeneus,2019,21,,53,74,,12.0,10.24197/her.21.2019.53-74,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077474162&doi=10.24197%2fher.21.2019.53-74&partnerID=40&md5=d6d3a287a083958e5d2c1baf2af9e92f,"Media accessibility is becoming mainstream. While it cannot compete for popularity with the two original fields -architecture and design accessibility- it is slowly gaining acknowledgment. Subtitling was and still is the most popular media access service. In recent years, more services have been joining the alternative possibilities to access information. New technologies have also increased the number of services, and Easy to Read is proposed in this article as a new candidate to join the list of services. This article will start by describing Easy to Read, and understand its approach as: A translation modality, a linguistic variation or as a service. The second part of the article presents many accessibility services and Easy to Read features. In the third part, new hybrid services are proposed. These are the result of adding to existing access services a layer of Easy to Read creating a higher degree of accessibility. Any accessibility service aiming to facilitate comprehension will improve and optimize its function by leaning on Easy to Read. The article finishes offering many examples to secure a rapid uptake of the service across the different accessibility fields, from design to web accessibility or transport. © 2019 Universidad de Valladolid. All rights reserved.",Audio description; Easy to read; Media accessibility; Media accessibility modalities; Media accessibility services; Sign Language interpreting; Subtitling,,Article,Final,Scopus,2-s2.0-85077474162,Peter,,
"Fidyka A., Matamala A.",57204918577;25921557700;,Production of access services in immersive content: Understanding the needs of audio describers,2019,Hikma,18,2,,277,300,,1.0,10.21071/hikma.v18i2.11683,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077312022&doi=10.21071%2fhikma.v18i2.11683&partnerID=40&md5=b38e53f6826a4af1b5e7972977de24e2,"360° content, offered more and more frequently by various broadcasters and culture institutions, should cater for the needs of all members of our society, including persons with sight loss. So far, however, the question of providing audio description (AD) in such content has been under researched. This study aims to report the results of the usability study of the prototype AD editor developed within the Immersive Accessibility (ImAc) project, which allowed us to gain insights into the needs of professional audio describers when working with 360° content. The editor is an online tool which allows describers to choose an appropriate sound type for AD, and place AD segments in the 360° sphere. The study was conducted online and data was collected by means of a demographic pre-questionnaire and a postquestionnaire, consisting of a System Usability Scale and additional preference questions. The results obtained provide valuable feedback on how to improve the functionality of the tool to meet the needs of its users. They also indicate the need for guidance when selecting content to be described in this media format, which suggests that AD in immersive content could be integrated into AD guidelines or specific courses offered by training institutions. The results of this study are just a starting point in the field of immersive accessibility, hence the recommendation for further research on the subject of accessibility in this media format. © 2019 Universidad de Cordoba,Servicio de Publicaciones. All rights reserved.",360° videos; Audio description; Audiovisual translation; Media accessibility; Usability,,Article,Final,Scopus,2-s2.0-85077312022,Peter,,
"Agulló B., Montagud M., Fraile I.",57206483570;35868074700;57201735130;,Making interaction with virtual reality accessible: Rendering and guiding methods for subtitles,2019,"Artificial Intelligence for Engineering Design, Analysis and Manufacturing: AIEDAM",33,4,,416,428,,14.0,10.1017/S0890060419000362,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077308603&doi=10.1017%2fS0890060419000362&partnerID=40&md5=1f4be997ef243c226daa6eed98a02096,"Accessibility in immersive media is a relevant research topic, still in its infancy. This article explores the appropriateness of two rendering modes (fixed-positioned and always-visible) and two guiding methods (arrows and auto-positioning) for subtitles in 360° video. All considered conditions have been implemented and integrated in an end-to-end platform (from production to consumption) for their validation and evaluation. A pilot study with end users has been conducted with the goals of determining the preferred options by users, the options that result in a higher presence, and of gathering extra valuable feedback from the end users. The obtained results reflect that, for the considered 360° content types, always-visible subtitles were more preferred by end users and received better results in the presence questionnaire than the fixed-positioned subtitles. Regarding guiding methods, participants preferred arrows over auto-positioning because arrows were considered more intuitive and easier to follow and reported better results in the presence questionnaire. © Cambridge University Press 2019.",360° Videos; accessibility; human-computer interaction; subtitles; virtual reality,Human computer interaction; Surveys; Virtual reality; accessibility; End to end; End users; Immersive media; Pilot studies; Research topics; subtitles; Rendering (computer graphics),Article,Final,Scopus,2-s2.0-85077308603,Peter,,
"Wang X., Tragant E.",57211857761;28167996400;,The effect of written text on comprehension of spoken English as a foreign language: A replication study,2019,IRAL - International Review of Applied Linguistics in Language Teaching,,,,,,,3.0,10.1515/iral-2018-0350,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075139283&doi=10.1515%2firal-2018-0350&partnerID=40&md5=a2611c297f4d8bf454af5689bc6239db,"The use of written text has been acclaimed to enhance L2 listening comprehension, yet some argue that using written text does not effectively prepare learners to listen in real situations. Thus, the study was conducted to explore the effect of written text on learners' perceived difficulty, listening comprehension and learning to listen through replicating the research by Diao, Chandler & Sweller (2007. The effect of written text on comprehension of spoken English as a foreign language. The American Journal of Psychology 237-261). Participants were 101 low-proficient English learners who were divided into three groups: Listening with subtitles, listening with a full script and listening only. Each group first listened to a passage in their respective mode, then all three groups listened to another passage in the listening-only mode. Participants rated their perceived difficulty and completed a free recall task after each listening. Results suggest that the difficulty of written text should be tuned with learners' proficiency level so that they can benefit from the presence of written text in listening. © 2019 Walter de Gruyter GmbH, Berlin/Boston.",cognitive load theory; dual-coding theory; replication studies; second language listening; second language teaching,,Article,Article in Press,Scopus,2-s2.0-85075139283,Peter,,
"Angerbauer K., Adel H., Vu N.T.",57056390800;55839314700;35744252000;,Automatic compression of subtitles with neural networks and its effect on user experience,2019,"Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH",2019-September,,,594,598,,3.0,10.21437/Interspeech.2019-1750,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074722004&doi=10.21437%2fInterspeech.2019-1750&partnerID=40&md5=c406e46154c7f6b0e07727f3a40983f8,"Understanding spoken language can be impeded through factors like noisy environments, hearing impairments or lack of proficiency. Subtitles can help in those cases. However, for fast speech or limited screen size, it might be advantageous to compress the subtitles to their most relevant content. Therefore, we address automatic sentence compression in this paper. We propose a neural network model based on an encoder-decoder approach with the possibility of integrating the desired compression ratio. Using this model, we conduct a user study to investigate the effects of compressed subtitles on user experience. Our results show that compressed subtitles can suffice for comprehension but may pose additional cognitive load. Copyright © 2019 ISCA",Automatic sentence compression; Recurrent neural networks; Subtitles; User study,,Conference Paper,Final,Scopus,2-s2.0-85074722004,Peter,,
"Pei T., Suwanthep J.",57211598358;56111302100;,Effects of web-based metacognitive listening on Chinese university EFL learners' listening comprehension and metacognitive awareness,2019,Indonesian Journal of Applied Linguistics,9,2,,480,492,,1.0,10.17509/ijal.v9i2.20246,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074563419&doi=10.17509%2fijal.v9i2.20246&partnerID=40&md5=ee1cc5d0af9ce3d8596b55e958124759,"The present study examined the effects of web-based metacognitive listening practice on L2 learners' listening comprehension over 14 weeks. Participants (N = 67) came from two intact classes of intermediate EFL university learners in China. The experimental group was involved in the web-based metacognitive listening practice built on the metacognitive listening principles. The control group undertook a traditional web-based listening practice with the same listening materials texts, yet without training on their metacognitive awareness. TOEFL tests and MALQ were used to track the development of listening achievements and metacognitive awareness. ANCOVA was employed to detect the differences between the two groups regarding listening achievements and metacognitive development. The results showed that the experimental group made significantly greater gains than the control group in listening achievements. However, the development of metacognitive awareness remained inconclusive. The study concludes that metacognitive listening practice under a web-based environment could outperform the traditional web-based listening practice in improving listening achievements among Chinese intermediate EFL learners. Besides, some recommendations for further study are discussed. © 2018, IJAL.",Bottom-up listening; Chinese EFL context; Language listening; Metacognitive awareness; Web-based learning,,Article,Final,Scopus,2-s2.0-85074563419,Peter,,
"Ward L.A., Shirley B.G.",57190667190;12797467200;,Personalization in object-based audio for accessibility: A review of advancements for hearing impaired listeners,2019,AES: Journal of the Audio Engineering Society,67,7-8,,584,597,,11.0,10.17743/jaes.2019.0021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070806654&doi=10.17743%2fjaes.2019.0021&partnerID=40&md5=576c45a68d70b8a2e90b601cadef364d,"Hearing loss is widespread and significantly impacts an individual's ability to engage with broadcast media. Access can be improved through new object-based audio personalization methods. Utilizing the literature on hearing loss and intelligibility this paper develops three dimensions that are evidenced to improve intelligibility: spatial separation, speech to noise ratio, and redundancy. These can be personalized, individually or concurrently, using object-based audio. A systematic review of all work in object-based audio personalization is then undertaken. These dimensions are utilized to evaluate each project's approach to personalization, identifying successful approaches, commercial challenges, and the next steps required to ensure continuing improvements to broadcast audio for hard of hearing individuals. © 2019 Audio Engineering Society. All rights reserved.",,Speech intelligibility; Commercial challenges; Hard of hearings; Hearing-impaired listeners; Personalizations; Review of advancements; Spatial separation; Systematic Review; Three dimensions; Audition,Article,Final,Scopus,2-s2.0-85070806654,Peter,,
"Tapu R., Mocanu B., Zaharia T.",26424843800;24822846400;6601999900;,DEEP-HEAR: A Multimodal Subtitle Positioning System Dedicated to Deaf and Hearing-Impaired People,2019,IEEE Access,7,,8751956,88150,88162,,7.0,10.1109/ACCESS.2019.2925806,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069767849&doi=10.1109%2fACCESS.2019.2925806&partnerID=40&md5=c867ee3663cb8ebd55a110116685c188,"In this paper, we introduce the DEEP-HEAR framework, a multimodal dynamic subtitle positioning system designed to increase the accessibility of deaf and hearing impaired people (HIP) to multimedia documents. The proposed system exploits both computer vision algorithms and deep convolutional neural networks specifically designed and tuned in order to detect and recognize the identity of the active speaker. The main contributions of the paper concern: a novel method dedicated to recognizing various characters existent in the video stream. A video temporal segmentation algorithm that divides the video sequence into semantic units, based on face tracks and visual consistency. Finally, the core of our approach concerns a novel active speaker recognition method relying on the multimodal information fusion from the text, audio, and video streams. The experimental results carried out on a large scale dataset of more than 30 videos, validate the proposed methodology with average accuracy and recognition rates superior to 90%. Moreover, the method shows robustness to important object/camera motion and face pose variation, yielding gains of more than 8% in precision and recall rates when compared with state-of-the-art techniques. The subjective evaluation of the proposed dynamic subtitle positioning system demonstrates the effectiveness of our approach. © 2013 IEEE.",Active speaker recognition; assistive framework for deaf and hearing impaired people; convolutional neural networks; dynamic subtitle positioning; face recognition,Character recognition; Convolution; Deep neural networks; Face recognition; Large dataset; Neural networks; Semantics; Speech recognition; Video streaming; Computer vision algorithms; Convolutional neural network; Deaf and hearing impaired; Multimodal information fusion; Speaker recognition; State-of-the-art techniques; Subjective evaluations; Video temporal segmentation; Audition,Article,Final,Scopus,2-s2.0-85069767849,Peter,,
Al-Ibrahim A.,52163277900;,Deaf and hard of hearing students’ perceptions of the flipped classroom strategy in an undergraduate education course,2019,European Journal of Educational Research,8,1,,325,336,,8.0,10.12973/eu-jer.8.1.325,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060859207&doi=10.12973%2feu-jer.8.1.325&partnerID=40&md5=a8f415833d5d91b125458554c98749ce,"This study aims to evaluate the academic outcomes of the flipped classroom approach in the teaching of students who are deaf or hard of hearing (DHH). Furthermore, it aims to activate the role of the teacher through encouraging both teachers and students to engage in active learning styles, while acknowledging individual differences. Participants consisted of 12 female undergraduates with hearing disabilities in a 251 CI course (applications of ICT in teaching and learning) at the College of Education, King Saud University. The study was applied throughout a semester on the contents of the course. The content material and pre-class assigned work (e.g. instructional videos and tasks) were delivered through Blackboard (learning management system), while active learning activities were carried out in class. Using mixed methods, students’ perceptions of their new learning environment were explored through a post-term questionnaire distributed at the end of the semester, in addition to writing a reflective report. Furthermore, participants were requested to write a reflective journal at the end of each lecture. Results indicated the effectiveness of the flipped classroom strategy for students. Moreover, the data indicate a positive impact on students' content learning and improved skills (e.g. collaboration and interaction). The content material which was developed for the specific course (251 CI) could be utilized for the remaining students enrolled in this course. The researcher recommends using the flipped classroom teaching strategy for courses in higher education, as the methodology can be extended and implemented through following a similar framework applied in this study. © 2018 Eurasian Society of Educational Research. All Rights Reserved.",Active learning; DHH; Flipped classroom; Special education needs; Technology integration,,Article,Final,Scopus,2-s2.0-85060859207,Peter,,
"Gago J.J., Victores J.G., Balaguer C.",57205486919;36716896800;6701864168;,"Sign language representation by TEO humanoid robots: End-user interest, comprehension and satisfaction",2019,Electronics (Switzerland),8,1,57,,,,9.0,10.3390/electronics8010057,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060255690&doi=10.3390%2felectronics8010057&partnerID=40&md5=74a2349fa933c71d086efbffd86a0206,"In this paper, we illustrate our work on improving the accessibility of Cyber–Physical Systems (CPS), presenting a study on human–robot interaction where the end-users are either deaf or hearing-impaired people. Current trends in robotic designs include devices with robotic arms and hands capable of performing manipulation and grasping tasks. This paper focuses on how these devices can be used for a different purpose, which is that of enabling robotic communication via sign language. For the study, several tests and questionnaires are run to check and measure how end-users feel about interpreting sign language represented by a humanoid robotic assistant as opposed to subtitles on a screen. Stemming from this dichotomy, dactylology, basic vocabulary representation and end-user satisfaction are the main topics covered by a delivered form, in which additional commentaries are valued and taken into consideration for further decision taking regarding robot-human interaction. The experiments were performed using TEO, a household companion humanoid robot developed at the University Carlos III de Madrid (UC3M), via representations in Spanish Sign Language (LSE), and a total of 16 deaf and hearing-impaired participants. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.",Accessibility; Anthropomorphic robotic hands; Assistive robotics; Cyber; Dactylology; Household companion; Human; Humanoid; Physical systems; Robot interaction; Robotics; Sign language; Statistics; Survey; Vocabulary,,Article,Final,Scopus,2-s2.0-85060255690,Peter,,
Kim H.,57210863776;,Understanding the life and communication of the deaf and hard of hearing for designing soundalert [청각장애인의 삶과 커뮤니케이션 방식의 이해 및 사운드얼럿 디자인],2018,Archives of Design Research,31,4,,137,153,,,10.15187/adr.2018.11.31.4.137,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059357038&doi=10.15187%2fadr.2018.11.31.4.137&partnerID=40&md5=4e84494e5178d7b281bee2d56e10781a,"Background Technology has the potential for connecting people and providing various educational or social opportunities. For people who are deaf and hard of hearing (DHH), however, there are social barriers on the internet, in software, and in hardware that can limit access to information and opportunities. The aim of the study is to understand the life and communication of DHH and to identify design spaces for DHH in order to support their independent lives and to design SoundAlert. Methods Non-intrusive, in-depth interviews, each of which lasted two hours and forty-five minutes, were conducted with four participants (three DHH and one sign language interpreter). The data were analysed for illustrating schematic view using open coding and selective coding systematically. Workshops were conducted with cross-disciplinary experts for reinterpreting the results into design spaces. Arduino was used for the SoundAlert prototype. Results The analysis of 197 units of a section resulted in 60 codes including ""life without sound"" and ""communication difficulties."" After the second analysis, 12 themes emerged (""Life and culture of DHH"", ""Different way of communication, Sign language,"" etc.) and were placed into four key categories including ""Educational environment of linguistics"" and ""Diverse service and technology"". The results were reinterpreted into design themes involving ""Emergency communication system"" and the ""Visualisation of sound information."" As part of the former, the SoundAlert prototype was developed. Conclusions The interview was successful in triggering diverse episodic, present and future stories told. The systematic analysis results were reinterpreted for designing. This process exemplifies the role of users and designers as ""co-creators"" in developing design strategic guidelines. Further field study on the SoundAlert will be carried out with DHH. © 2018, Korean Society of Design Science.연구배경 기술의 발전은 보다 많은 사람들을 연결시켜주고 다양한 교육이나 사회적 기회를 제공해준다. 그러 나, 청각장애인처럼 특별한 니즈를 필요로 하는 사람들에게는 주변 사람들의 인식과 시선 뿐 아니라 정보와 기 회에 대한 접근성을 구현할 수 있는 하드웨어나 소프트웨어 분야에서의 사회적 장벽이 여전히 존재한다. 본 연 구는 국내 청각장애인들의 실제 경험과 이야기를 통해 그들의 관점에서 삶과 소통방식을 이해하고, 문제점과 디자인 가능성을 파악하고자 하였다. 그리고 그들의 독립적인 삶을 지원하기 위한 새로운 기기인 사운드얼럿 (SoundAlert)의 디자인을 제안하였다. 연구방법 청각장애인 세 명과 수화통역사 등 총 네 명의 참가자와 비구조적 일대일 심층 인터뷰 방법을 이용 하였다(2hr45min). 수집된 데이터는 오픈 코딩(open coding)과 선택 코딩(selective coding)의 두 단계를 거쳐 체계적으로 분석하였다. 그리고, 이를 재해석하여 디자인 주제를 도출한 뒤 아두이노를 이용하여 사운드얼럿 프 로토타입을 제작하였다. 연구결과 오픈 코딩 분석 결과는 총 197개의 텍스트 유닛으로 구분되어, '소리환경', '소통의 어려움' 등 60 개의 코드로 나타났다. 선택 코딩을 통한 두 번째 분석 결과는 'T1. 소리가 없는 사람들의 삶과 문화', 'T4. 다른 방식의 소통언어, 수화' 등 12개의 주제로 나타났다. 이 주제들은 네 개의 핵심 범주에 도식적 관점(schematic view)으로 구성되었다; (a)소통의 어려움과 한계, (b)언어학적 교육환경, (c)다양한 기술과 서비스, 그리고 (d)새 로운 가능성과 희망이다. 이러한 체계적 분석결과는 디자인 주제(design themes)로 재해석되어, (i)응급상황에 서의 즉각적 시스템 요구, (ii)수화교육의 체계적인 기술적 접근, 그리고 (iii)정확한 소통을 위한 소리정보의 시각 화로 정리되었다. 그리고 첫 번째 주제의 한 부분으로서 사운드얼럿(SoundAlert) 컨셉과 작동원리를 이용한 프 로토타입을 개발하였다. 결론 청각장애인 심층인터뷰 방법과 분석 방법은 성공적이었다. 다학문적 접근으로 재해석된, 세 가지 디 자인 주제 내용은 유저와 디자이너의 코-크리에이터(co-creator) 및 향후 디자인의 전략적 가이드 역할을 할 수 있을 것이다. 사운드얼럿(SoundAlert)은 IoT와 연계하여 실제 청각장애인들과의 현장 조사를 진행할 계획이다.",DHH; Ethnography; GTA; Open coding; Prototyping; Qualitative research; Selective coding; 근거이론분석; 에쓰노그래피; 정성연구; 청각장애인; 프로토타입,,Article,Final,Scopus,2-s2.0-85059357038,Peter,,
"Naraine M.D., Fels D.I., Whitfield M.",37665797700;57218572233;55672525700;,Impacts on quality: Enjoyment factors in blind and low vision audience entertainment ratings: A qualitative study,2018,PLoS ONE,13,12,e0208165,,,,2.0,10.1371/journal.pone.0208165,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057587965&doi=10.1371%2fjournal.pone.0208165&partnerID=40&md5=cd2cb054eac5d14945b17a9455b70360,"Audio description (AD) is one of the main methods that people who are blind or low vision (B/LV) use to access film, television, and theatre content. AD is a second audio track inserted into the space(s) where speech is absent, which tends to be only a few seconds. Contained in that second track is an audio description of the important visual information contained within a specific scene. However, as there is insufficient time to describe all visual information, decisions about what is important to describe and how to present that information (style) to optimize a B/LV viewer’s entertainment experience are required. Most research to date has considered only short-term, single-episode experiences to gauge viewers’ reactions to the AD content. In addition, this research typically has used a monotone, single style of audio description, which is defined as “the conventional style” in this paper. We use an integrative style instead, that is defined as ‘AD designed to fit a specific show”, and differed between shows. We carried out a within-subjects longitudinal study with eight episodes of a dark comedy, using different description styles and describers in order to assess viewer engagement and preferences for AD describer style, language use, timing, and fit to the show. Twenty-four blind participants viewed and rated all eight episodes. Major findings included that most participants found the integrative style entertaining, a fit with the specific episodes, and enjoyable. Some participants, however, preferred the conventional style and struggled with the language and topic of a dark comedy and its associated descriptions. © 2018 Naraine et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"adult; Article; audio recording; blindness; clinical article; clinical research; controlled study; decision making; experience; female; hearing; human; language; leisure; longitudinal study; low vision; male; patient preference; visual information; emotion; low vision; Ontario; pleasure; psychology; qualitative research; questionnaire; television; visually impaired person; Emotions; Humans; Leisure Activities; Longitudinal Studies; Ontario; Pleasure; Qualitative Research; Surveys and Questionnaires; Television; Vision, Low; Visually Impaired Persons",Article,Final,Scopus,2-s2.0-85057587965,Peter,,
"Kameswaran V., Gupta J., Pal J., O’Modhrain S., Veinot T.C., Brewer R.N., Parameshwar A., Vidhya Y., O’Neill J.",57190061358;7202539915;26436360800;57190193117;8659055200;36616888500;57202047669;57202049037;8412486300;,‘We can go anywhere’: Understanding independence through a case study of ride-hailing use by people with visual impairments in metropolitan India,2018,Proceedings of the ACM on Human-Computer Interaction,2,CSCW,85,,,,26.0,10.1145/3274354,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066419427&doi=10.1145%2f3274354&partnerID=40&md5=a2a174ff758ecaf2736398917a94115c,"Ride-hailing services have received attention as part of the growing work around the sharing economy, but the focus of these studies has largely been on drivers. In this paper, we examine how ride-hailing is transforming the transportation practices of one group of passengers - people with visual impairments in metropolitan India. Through a qualitative study consisting of interviews and observations, we examined the use and impact of these services on our target population, who otherwise contend with chaotic, unreliable, and largely inaccessible modes of transportation. We found that ride-hailing services positively affects participants’ notions of independence, and we tease out how independence for our participants is not just about ‘doing things alone, without help’ but is also situated, social and relative. Furthermore, we show how accessibility, in the case of ride-hailing in India, is a socio-technical and collaborative achievement, involving interactions between the passenger, the driver, and the technology. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Accessibility; Blind users; Collaborative accessibility; Independence; Ola; Ridesharing; Social accessibility; Social interactions; Stigma; Uber,User interfaces; Accessibility; Blind users; Collaborative accessibility; Independence; Ride-sharing; Social accessibility; Social interactions; Stigma; Uber; Human computer interaction,Article,Final,Scopus,2-s2.0-85066419427,Peter,,
Walczak A.,55258034800;,Audio description on smartphones: making cinema accessible for visually impaired audiences,2018,Universal Access in the Information Society,17,4,,833,840,,8.0,10.1007/s10209-017-0568-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026522908&doi=10.1007%2fs10209-017-0568-2&partnerID=40&md5=432b9aba84e317aade15b37596fed3fc,"This article presents a study aimed at assessing an application offering audio description for mobile devices. By means of questionnaires, three features of the application were evaluated: usability, utility and quality. Fifteen blind and visually impaired volunteers participated in the study, which took place in real-life conditions—during a film festival screening. The results indicated positive ratings of all three assessed features, but also pointed to specific elements that could be subject to improvement. Overall, the application was considered as having good potential. Modifications were introduced, and the application is currently fully operational. The application can be used as a tool for providing improved access to cinema content, making cinema accessible for all. © 2017, Springer-Verlag GmbH Germany.",Accessibility; Application; Audio description; Blind and visually impaired; Cinema; Secondary devices,Applications; Information science; Information systems; Software engineering; Accessibility; Audio description; Blind and visually impaired; Cinema; Secondary devices; Surveys,Article,Final,Scopus,2-s2.0-85026522908,Peter,,
"Guimarães R.L., Brito J.O., Santos C.A.S.",35183179100;57192250837;55430678200;,Investigating the influence of subtitles synchronization in the viewer’s quality of experience,2018,ACM International Conference Proceeding Series,,,30,,,,2.0,10.1145/3274192.3274222,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056706067&doi=10.1145%2f3274192.3274222&partnerID=40&md5=c5c3631b1c81a20b7ef133de5a3186d9,"Watching a movie in which audio and video are out of sync is certainly an unpleasant experience, even with a big bucket of fresh popcorn. In a similar way, viewers can experience a negative experience when subtitles are not well synchronized with the media content. The point is that subtitles synchronization (or the lack thereof) plays a key role, positively or negatively, in the perception of quality that viewers have about the content. In this context, this work focuses on investigating how slightly more “relaxed” synchronization directives may affect significantly the perception of Quality of Experience (QoE) that users have when watching audiovisual content in English with subtitles in Brazilian Portuguese. Our observations are based on the analysis of data collected in an experiment with 24 users and the results suggest that subtitles beginning with delay have most influence in the viewers’ QoE. © 2018 Copyright is held by the owner/author(s).",Captions; QoE; SRT; Subtitles; Synchronization,Human engineering; Synchronization; Analysis of data; Audio and video; Audio-visual content; Captions; Media content; Negative experiences; Quality of experience (QoE); Subtitles; Quality of service,Conference Paper,Final,Scopus,2-s2.0-85056706067,Peter,,
"Seita M., Albusays K., Kafle S., Stinson M., Huenerfauth M.",57192541826;57189681874;57200500342;7005000200;12240800100;,Behavioral changes in speakers who are automatically captioned in meetings with deaf or hard-of-hearing peers,2018,ASSETS 2018 - Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility,,,,68,80,,15.0,10.1145/3234695.3236355,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056904288&doi=10.1145%2f3234695.3236355&partnerID=40&md5=7c6d6b48843e2422e412ca5a117e2854,"Deaf and hard of hearing (DHH) individuals face barriers to communication in small-group meetings with hearing peers; we examine generation of captions on mobile devices by automatic speech recognition (ASR). While ASR output displays errors, we study whether such tools benefit users and influence conversational behaviors. An experiment was conducted where DHH and hearing individuals collaborated in discussions in three conditions (without an ASR-based application, with the application, and with a version indicating words for which the ASR has low confidence). An analysis of audio recordings, from each participant across conditions, revealed significant differences in speech features. When using the ASR-based automatic captioning application, hearing individuals spoke more loudly, with improved voice quality (harmonics-to-noise ratio), with a non-standard articulation (changes in F1 and F2 formants), and at a faster rate. Identifying non-standard speech in this setting has implications on the composition of data used for ASR training/testing, which should be representative of its usage context. Understanding these behavioral influences may also enable designers of ASR captioning systems to leverage these effects, to promote communication success. © 2018 Association for Computing Machinery.",Accessibility; Automatic Speech Recognition; Communication; Deaf and Hard of Hearing; Speaking Behavior,Audio recordings; Audition; Communication; Speech communication; Accessibility; Automatic speech recognition; Behavioral changes; Hard of hearings; Harmonics-to-noise ratios; Speaking Behavior; Speech features; Voice quality; Speech recognition,Conference Paper,Final,Scopus,2-s2.0-85056904288,Peter,,
"Jain D., Franz R., Findlater L., Cannon J., Kushalnagar R., Froehlich J.",57014317900;57014657800;10040303000;57204725762;36142036500;7101665384;,Towards accessible conversations in a mobile context for people who are deaf and hard of hearing,2018,ASSETS 2018 - Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility,,,,81,92,,26.0,10.1145/3234695.3236362,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056847096&doi=10.1145%2f3234695.3236362&partnerID=40&md5=3fdb7d040deedac96f6a7f2053323e06,"Prior work has explored communication challenges faced by people who are deaf and hard of hearing (DHH) and the potential role of new captioning and support technologies to address these challenges; however, the focus has been on stationary contexts such as group meetings and lectures. In this paper, we present two studies examining the needs of DHH people in moving contexts (e.g., walking) and the potential for mobile captions on head-mounted displays (HMDs) to support those needs. Our formative study with 12 DHH participants identifies social and environmental challenges unique to or exacerbated by moving contexts. Informed by these findings, we introduce and evaluate a proof-of-concept HMD prototype with 10 DHH participants. Results show that, while walking, HMD captions can support communication access and improve attentional balance between the speakers(s) and navigating the environment. We close by describing open questions in the mobile context space and design guidelines for future technology. © 2018 Association for Computing Machinery.",Augmented reality; Conversations; Deaf and hard of hearing; Head-mounted display; Moving; Real-time captioning,Audition; Augmented reality; Street traffic control; Conversations; Hard of hearings; Head mounted displays; Moving; Real time; Helmet mounted displays,Conference Paper,Final,Scopus,2-s2.0-85056847096,Peter,,
"Fok R., Kaur H., Palani S., Mott M.E., Lasecki W.S.",57200532330;57213128830;57204735888;36768870600;54383728900;,Towards more robust speech interactions for deaf and hardof hearing users,2018,ASSETS 2018 - Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility,,,,57,67,,10.0,10.1145/3234695.3236343,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056831977&doi=10.1145%2f3234695.3236343&partnerID=40&md5=75a9bc1b61983abc523176a6563dc837,"Mobile, wearable, and other ubiquitous computing devices are increasingly creatinga contextin which conventionalkey-board and screen-based inputs are being replacedinfavorof more natural speech-based interactions. Digital personal as-sistants use speech to control a wide range of functionality, from environmental controls to information access. However, manydeaf and hard-of-hearing users have speech patterns that vary from those of hearing users due to incomplete acoustic feedback from their own voices. Because automatic speech recognition (ASR) systems are largely trained using speech from hearing individuals, speech-controlled technologies are typically inaccessible to deaf users. Prior work has focused on providing deaf users access to aural output via real-time captioning or signing, but little has been done to improve users' ability to provide input to these systems' speech-based interfaces. Further, the vocalization patterns of deaf speech often makeaccurate recognition intractable for both automated systems and human listeners, making traditional approaches to mitigate ASR limitations, such as human captionists, less effective.To bridge this accessibilitygap, weinvestigate the limitations of common speech recognition approaches and techniques-both automatic and human-powered-when ap-plied to deaf speech. We then explore the effectiveness of an iterative crowdsourcing workfow, and characterize the po-tential for groups to collectively exceed the performance of individuals. This paper contributes a better understanding of the challenges of deaf speech recognition and provides insights for future system development in this space. © 2018Copyrightisheldbytheowner/author(s).",Accessibility; Automatic Speech Recognition; Deaf and Hard-of-Hearing; Deaf Speech; Human Computation,Audition; Automation; Iterative methods; Pattern recognition systems; Real time systems; Speech; Ubiquitous computing; Accessibility; Automatic speech recognition; Automatic speech recognition system; Environmental control; Hard of hearings; Human computation; Speech-based interfaces; Traditional approaches; Speech recognition,Conference Paper,Final,Scopus,2-s2.0-85056831977,Peter,,
"Al-Khazraji S., Berke L., Kafle S., Yeung P., Huenerfauth M.",57203131212;57200494669;57200500342;57204729169;12240800100;,Modeling the speed and timing of American sign language to generate realistic animations,2018,ASSETS 2018 - Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility,,,,259,270,,20.0,10.1145/3234695.3236356,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056637655&doi=10.1145%2f3234695.3236356&partnerID=40&md5=f6addf588269a35df8fb2b072ae4e8e9,"To enable more websites to provide content in the form of sign language, we investigate software to partially automate the synthesis of animations of American Sign Language (ASL), based on a human-authored message specification. We automatically select: where prosodic pauses should be inserted (based on the syntax or other features), the time-duration of these pauses, and the variations of the speed at which individual words are performed (e.g. slower at the end of phrases). Based on an analysis of a corpus of multi-sentence ASL recordings with motion-capture data, we trained machine-learning models, which were evaluated in a cross-validation study. The best model out-performed a prior state-of-the-art ASL timing model. In a study with native ASL signers evaluating animations generated from either our new model or from a simple baseline (uniform speed and no pauses), participants indicated a preference for speed and pausing in ASL animations from our model. © 2018 Association for Computing Machinery.",American Sign Language; Animation; ASL; Modeling; Pauses; Prosodic Breaks; Timing,Animation; Learning systems; Models; Timing circuits; American sign language; Cross validation; Machine learning models; Motion capture data; Pauses; Prosodic breaks; State of the art; Timing; Modeling languages,Conference Paper,Final,Scopus,2-s2.0-85056637655,Peter,,
"Zelic G., Varlet M., Wishart J., Kim J., Davis C.",55039088900;36961417300;57203181609;35740855000;57220638395;,The dual influence of pacer continuity and pacer pattern for visuomotor synchronisation,2018,Neuroscience Letters,683,,,150,159,,4.0,10.1016/j.neulet.2018.07.044,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050869231&doi=10.1016%2fj.neulet.2018.07.044&partnerID=40&md5=c00668a0e6385e26ed7f2a29085e26f2,"There is growing evidence that movement synchronisation to an external rhythm is affected by the type of pacers involved. Specifically, visuomotor synchronisation (VMS) is facilitated when the pacer is continuous (continuity condition) and moves in the same direction as the movement produced (pattern matching condition). However, the extent to which the continuity and pattern matching conditions each contribute to facilitation of VMS remains unclear. The present study aimed to disentangle the potential dual influence of pacer continuity and pacer pattern on VMS. Twenty participants were asked to synchronise continuous left-right forearm movements of tracking (continuous VMS – Experiment 1) or discrete up-down finger movements of tapping (discontinuous VMS – Experiment 2) with four visual pacers. The pacers consisted of a red dot target that either oscillated (continuous pacers) or flashed (discrete pacers) periodically. The target also exhibited a left-right (left-right pacers) or a centred (centred pacers) movement pattern. Results showed lower variability in synchrony with the continuous visual pacer that respectively matched the left-right and the centred movement pattern of forearm and finger tapping. Both the continuity condition and the pattern matching condition facilitated VMS when synchronising continuous forearm tracking movements (Experiment 1) whereas both conditions were required to facilitate VMS when synchronising discrete finger tapping movements (Experiment 2). These results provide new insights into the mechanisms underlying VMS and how they are modulated by variations in pacer pattern and pacer continuity. © 2018 Elsevier B.V.",Finger tapping; Forearm tracking; Pacer-movement matching; Visuomotor synchronisation,"adult; analysis of variance; arm movement; Article; correlation analysis; cortical synchronization; female; finger; forearm; human; human experiment; male; normal human; oscillation; post hoc analysis; priority journal; visuomotor coordination; young adult; adolescent; eye movement; movement (physiology); pattern recognition; photostimulation; physiology; procedures; psychomotor performance; Adolescent; Adult; Eye Movements; Female; Fingers; Humans; Male; Movement; Pattern Recognition, Visual; Photic Stimulation; Psychomotor Performance; Young Adult",Article,Final,Scopus,2-s2.0-85050869231,Peter,,
"Chmiel A., Lijewska A., Szarkowska A., Dutka Ł.",25640596200;56404701000;54416458200;57190034357;,"Paraphrasing in respeaking–comparing linguistic competence of interpreters, translators and bilinguals",2018,Perspectives: Studies in Translation Theory and Practice,26,5,,725,744,,5.0,10.1080/0907676X.2017.1394331,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051467070&doi=10.1080%2f0907676X.2017.1394331&partnerID=40&md5=aa91d247274419401c29b6b2c2e8e6dd,"Respeaking is a method of producing subtitling for live events and TV programmes. Respeakers repeat speakers’ utterances so that they may be changed by speech recognition software into subtitles for the deaf and hard of hearing. Respeakers need to paraphrase the text so that it conforms with temporal and spatial constraints of subtitling. Due to the similarities between respeaking, interpreting and translation, we tested interpreters, translators and bilingual controls on a paraphrasing task to see whether interpreters or translators would manifest any advantage thanks to experience. Following respeaking training, the participants were asked to paraphrase sentences with semantic redundancies, oral discourse markers and false starts in a simultaneous and delayed condition. Contrary to our predictions, we found that experience did not modulate paraphrasing quality or speed in general, but interpreters did outperform other groups when eliminating semantic redundancies, which were also the most difficult reformulations to tackle for all participants. The data suggest that while interpreters and translators are not better predisposed to become respeakers than regular bilinguals, at least as regards the paraphrasing performance, certain aspects of the interpreting experience (the need to express meaning concisely within time constraints) may offer a slight advantage in producing well-formed respoken subtitles. © 2017, © 2017 Informa UK Limited, trading as Taylor & Francis Group.",interpreting; paraphrasing; Respeaking; subtitling for the deaf and hard-of-hearing; translation,,Article,Final,Scopus,2-s2.0-85051467070,Peter,,
"Youngblood N.E., Tirumala L.N., Galvez R.A.",36994960000;57203884081;57195461931;,Accessible media: The need to prepare students for creating accessible content,2018,Journalism and Mass Communication Educator,73,3,,334,345,,8.0,10.1177/1077695817714379,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053380340&doi=10.1177%2f1077695817714379&partnerID=40&md5=b750e4e8bacc93c3635b6cb5eb9d5792,"Electronic media accessibility has come a long way since the Telecommunications Act of 1996. In 2010, the Communication and Video Accessibility Act (CVAA) mandated closed captioning many online videos and called for making video blind accessible through audio descriptions. The Department of Justice (DOJ) ruled Americans With Disabilities Act (ADA) applied to the virtual world. Since January 2015, there have been over 240 online-accessibility lawsuits. As educators, we need to prepare students to understand what accessibility is and how to make electronic media accessible. This article outlines accessibility issues across the curriculum, including closed captioning, audio descriptions, and online documents, and calls for better integration of accessibility into the electronic media curriculum. © AEJMC 2017.",Accessibility; ADA; Audio description; Captioning; CVAA; Pedagogy; Section 508,,Article,Final,Scopus,2-s2.0-85053380340,Peter,,
"Alkhalifa S., Al-Razgan M.",57201184361;14024027700;,Enssat: wearable technology application for the deaf and hard of hearing,2018,Multimedia Tools and Applications,77,17,,22007,22031,,7.0,10.1007/s11042-018-5860-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043692103&doi=10.1007%2fs11042-018-5860-5&partnerID=40&md5=df3181de29177c427d6d2058b04a24cf,"A large number of people around the globe experience hearing difficulties of varying degrees. Therefore, recent research has focused on developing applications and systems to aid the Deaf and Hard of Hearing (DHH). Wearable devices have significant potential in assistive applications owing to their low cost and lightweight. Wearable devices help the DHH perform their daily activities easily without requiring assistance from others. We present Enssat, a bilingual (Arabic/English) smartphone-based hearing aid application that uses Google Glass to assist DHH individuals. The application performs important tasks: real-time transcription, real-time translation, and alert management. A user study was performed to evaluate the application. In this study, 10 DHH users evaluated the processes of transcription, translation, and alerting and 15 non-DHH users evaluated the translation function of the system. All of them evaluated the usability and effectiveness of the Enssat application. The results demonstrated the ease of use and utility of the application. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",Assistive technology; Deaf; Hard of hearing; Head-mounted; Speech-to-text; Transcription; Translation; Wearable computing,Hearing aids; Smartphones; Transcription; Translation (languages); Wearable computers; Assistive technology; Deaf; Hard of hearings; Head-mounted; Speech to texts; Wearable computing; Audition,Article,Final,Scopus,2-s2.0-85043692103,Peter,,
"Ojeda-Castelo J.J., Piedra-Fernandez J.A., Iribarne L., Bernal-Bravo C.",57193350453;8529553300;55908790400;57193343155;,KiNEEt: application for learning and rehabilitation in special educational needs,2018,Multimedia Tools and Applications,77,18,,24013,24039,,15.0,10.1007/s11042-018-5678-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041532263&doi=10.1007%2fs11042-018-5678-1&partnerID=40&md5=488112e48193b28fdee61d1378b76371,"In the 21st century, the landscape of education has changed dramatically due to the application of technology in teaching and learning. This is especially true for teaching children with special needs. Technology has made it possible for the special needs children to be actively involved in their learning. The use of interactive whiteboards in the Special Education Center Princesa Sofia presented a few challenges for both the teachers and the students, hence KiNEEt was developed to overcome these problems. KiNEEt is a system which has been developed with the major aim of improving physical and cognitive skills in students with special needs. The different activities in KiNEEt are configurable and the tutor can modify the settings according to the needs of the student. The activities are game-oriented to attract the students attention and motivate them to learn. KiNEEt is highly interactive and it will encourage the students to be active learners. Results showed that Microsoft Kinect is the most suitable platform for this device as the students will be able to use the computer while simultaneously improving their digital competence, cognitive and physical skills. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",Educational games; Human-computer interface; Motor impairments; Natural interaction,Computer games; Human computer interaction; Teaching; Educational game; Human computer interfaces; Interactive whiteboards; Motor impairments; Natural interactions; Special education; Special educational needs; Teaching and learning; Students,Article,Final,Scopus,2-s2.0-85041532263,Peter,,
"Burchill Z., Liu L., Florian Jaeger T.",57201640276;57201033157;57203370292;,Maintaining information about speech input during accent adaptation,2018,PLoS ONE,13,8,e0199358,,,,12.0,10.1371/journal.pone.0199358,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051436124&doi=10.1371%2fjournal.pone.0199358&partnerID=40&md5=e121c0b8efb847f3c33dc448f04c3b00,"Speech understanding can be thought of as inferring progressively more abstract representations from a rapidly unfolding signal. One common view of this process holds that lower-level information is discarded as soon as higher-level units have been inferred. However, there is evidence that subcategorical information about speech percepts is not immediately discarded, but is maintained past word boundaries and integrated with subsequent input. Previous evidence for such subcategorical information maintenance has come from paradigms that lack many of the demands typical to everyday language use. We ask whether information maintenance is also possible under more typical constraints, and in particular whether it can facilitate accent adaptation. In a web-based paradigm, participants listened to isolated foreign-accented words in one of three conditions: subtitles were displayed concurrently with the speech, after speech offset, or not displayed at all. The delays between speech offset and subtitle presentation were manipulated. In a subsequent test phase, participants then transcribed novel words in the same accent without the aid of subtitles. We find that subtitles facilitate accent adaptation, even when displayed with a 6 second delay. Listeners thus maintained subcategorical information for sufficiently long to allow it to benefit adaptation. We close by discussing what type of information listeners maintain—subcatego-rical phonetic information, or just uncertainty about speech categories. © 2018 Burchill et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"accent adaptation; Article; human; language ability; language processing; phonetics; speech analysis; speech audiometry; speech development; speech perception; web browser; adult; comprehension; controlled study; coping behavior; female; language; male; migrant; physiology; psychology; randomized controlled trial; speech; verbal behavior; Adaptation, Psychological; Adult; Comprehension; Emigrants and Immigrants; Female; Humans; Language; Male; Phonetics; Speech; Speech Perception; Verbal Behavior",Article,Final,Scopus,2-s2.0-85051436124,Peter,,
"Wen Y., van Heuven W.J.B.",57189849129;7003726457;,Limitations of translation activation in masked priming: Behavioural evidence from Chinese-English bilinguals and computational modelling,2018,Journal of Memory and Language,101,,,84,96,,9.0,10.1016/j.jml.2018.03.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044920009&doi=10.1016%2fj.jml.2018.03.004&partnerID=40&md5=04fb8c9aa43df4d24a5109d0f2ec8404,"Electrophysiological and behavioural evidence suggests that Chinese translations of English words are automatically activated when Chinese-English bilinguals read English words (e.g., Thierry & Wu, 2007; Wu & Thierry, 2010; Zhang, van Heuven, & Conklin, 2011). The present study investigated the impact of translation activation in three behavioural experiments with in total 118 Chinese-English bilinguals. First, we investigated whether Chinese phonology was the source of the effects of Chinese character repetition in the Chinese translations of English masked primes and targets (hidden repetition priming) observed in Zhang et al.'s (2011), and whether these hidden repetition priming effects were affected by Chinese morpheme complexity and prime duration. However, we failed to find any evidence of hidden repetition priming. An exact replication of Zhang et al. (2011) was conducted next, which again provided no evidence for hidden repetition priming. However, cross-language priming data collected with the same group of participants did reveal masked translation priming and crucially Chinese character repetition priming with masked Chinese primes and English targets (partially hidden repetition priming), indicating that the activation of Chinese translations in the masked priming paradigm is limited to English target words. Computational modeling work provided further support that translation form activation is limited to target words in the masked priming paradigm. © 2018 Elsevier Inc.",Chinese-English bilinguals; Computational modelling; Hidden repetition priming; Masked priming; Translation activation; Translation priming,adult; article; Chinese script; female; human; human experiment; language; male; phonetics; publication; repetition priming,Article,Final,Scopus,2-s2.0-85044920009,Peter,,
Butler J.,57202196333;,Integral Captions and Subtitles: Designing a Space for Embodied Rhetorics and Visual Access,2018,Rhetoric Review,37,3,,286,299,,3.0,10.1080/07350198.2018.1463500,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047421814&doi=10.1080%2f07350198.2018.1463500&partnerID=40&md5=c989cfaa9989ce66833b2dff233a57ee,"Integral captions and subtitles are specific forms of captions and subtitles that are designed to be essential elements of videos in coordination with sound, signs, and other modes of communication. Integral captions reflect the importance of embodied rhetorics in Deaf culture, particularly in the kinetic language of ASL and Deaf Space design practices. Designing a (Deaf) space for integral captions that embody multimodal and multilingual communication is an essential multimodal literacy practice that benefits d/Deaf and hearing composers and viewers. Five criteria that characterize integral captions provide instructors and scholars with a tool for captions and embodied rhetorics. Copyright © Taylor & Francis Group, LLC.",,,Article,Final,Scopus,2-s2.0-85047421814,Peter,,
"Kurahashi T., Sakuma R., Zempo K., Mizutani K., Wakatsuki N.",57200438237;57208684487;55329072700;7202243508;7003950965;,Retrospective Speech Balloons on Speech-Visible AR via Head-Mounted Display,2018,"Adjunct Proceedings - 2018 IEEE International Symposium on Mixed and Augmented Reality, ISMAR-Adjunct 2018",,,8699323,423,424,,3.0,10.1109/ISMAR-Adjunct.2018.00127,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065537802&doi=10.1109%2fISMAR-Adjunct.2018.00127&partnerID=40&md5=3e2941da8ef44dbe53f488dadc29af1b,"It was confirmed speech balloon captioning shown in 3 dimensional space (such as Augmented Reality) were better than caption shown in 2 dimension screen with the aim of information insurance for people with hearing impairment in previous research. In this research, we reconfirmed that multi line captions contributes to the user's comprehension of the speech balloon contents better than the single line caption. This research has also found that the system's ability which enables the user to look back on the previous parts of the conversation leads to the improvement of the user experience. In addition, when the amount of sentences increases, the captions presented not as a log, but as multi speech balloons like comics are more natural and do not obstruct the user's view. © 2018 IEEE.",Accessibility; Accessibility systems and tools; Human-centered computing; Human-centered computing; Interaction paradigms; Mixed / augmented reality; scroll type caption; speech Balloon; speech recognition; Support for person with hearing impairment,Audition; Augmented reality; Balloons; Helmet mounted displays; Speech; User interfaces; Accessibility; Hearing impairments; Human-centered computing; Interaction paradigm; Mixed/augmented reality; scroll type caption; Speech recognition,Conference Paper,Final,Scopus,2-s2.0-85065537802,Peter,,
"Orero P., Doherty S., Kruger J.-L., Matamala A., Pedersen J., Perego E., Romero-Fresco P., Rovira-Esteva S., Soler-Vilageliu O., Szarkowska A.",24921771100;56137159600;9277428700;25921557700;35748037600;36477731700;36675476200;57202691545;12778196600;54416458200;,Conducting experimental research in audiovisual translation (AVT): A position paper,2018,Journal of Specialised Translation,,30,,105,126,,40.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052841142&partnerID=40&md5=10b74b55e81c3ad21b4cae53fdf6868c,"Experimental studies on AVT have grown incrementally over the past decade. This growing body of research has explored several aspects of AVT reception and production using behavioural measures such as eye tracking, as well as venturing into physiological measures such as electroencephalography (EEG), galvanic skin response, and heart rate. As a novel approach to the field of AVT, the experimental approach has borrowed heavily from other fields with established experimental traditions, such as psycholinguistics, psychology, and cognitive science. However, these methodologies are often not implemented with the same rigour as in the disciplines from which they were taken, making for highly eclectic and, at times, inconsistent practices. The absence of a common framework and best practice for experimental research in AVT poses significant risk in addition to the potential reputational damage. Some of the most important risks are: the duplication of efforts, studies that cannot be replicated due to a lack of methodological standardisation and rigour, and findings that are, at best, impossible to generalise from and, at worst, invalid. Given the growing body of work in AVT taking a quasi-experimental approach, it is time to consolidate our position and establish a common framework in order to ensure the integrity of our endeavours. © 2018 University of Roehampton. All rights reserved.",Audiovisual Translation; Experimental research; Eye-tracking; Methodology; Subtitling,,Article,Final,Scopus,2-s2.0-85052841142,Peter,,
"Trussell J.W., Nordhaus J., Brusehaber A., Amari B.",7006607897;14058562500;57203133047;57203131401;,Morphology instruction in the science classroom for students who are deaf: A multiple probe across content analysis,2018,Journal of Deaf Studies and Deaf Education,23,3,,271,283,,9.0,10.1093/deafed/eny009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050602547&doi=10.1093%2fdeafed%2feny009&partnerID=40&md5=37d56abca3af73bd05581a1b00542975,"Deaf and hard-of-hearing (DHH) students have exhibited a morphological knowledge delay that begins in preschool and persists through college. Morphological knowledge is critical to vocabulary understanding and text comprehension in the science classroom. We investigated the effects of morphological instruction, commonly referred to as Word Detectives, on the morphological knowledge of college-age DHH students in a science course. We implemented a multiple probe across behaviors single-case experimental design study with nine student participants. The student participants attended the National Technical Institute for the Deaf. A functional relation was found between the morphological instruction and the student participants' improvement of morphological knowledge regarding the morphemes taught during instruction. These findings indicate that DHH students benefit from morphological instruction to build their vocabulary knowledge in contentarea classrooms, such as science courses. © The Author(s) 2018. Published by Oxford University Press. All rights reserved.",,adult; article; clinical article; college; content analysis; experimental design; female; human; male; morphology; student; vocabulary; astronomy; deaf education; education; hearing impairment; observer variation; procedures; psycholinguistics; psychology; science; teaching; vocabulary; Astronomy; Deafness; Education of Hearing Disabled; Female; Humans; Male; Observer Variation; Psycholinguistics; Science; Teaching; Vocabulary,Article,Final,Scopus,2-s2.0-85050602547,Peter,,
"Strelcyk O., Singh G.",26536750900;57213799538;,TV listening and hearing aids,2018,PLoS ONE,13,6,e020008,,,,12.0,10.1371/journal.pone.0200083,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049263862&doi=10.1371%2fjournal.pone.0200083&partnerID=40&md5=da2286a4f8813aa95e7013c70803e6ab,"TV listening plays a large role in the lives of hearing-impaired (HI) individuals. Yet, few studies have examined TV listening in this group. In this paper, we report the findings of an online survey on TV listening conducted among HI individuals with and without hearing aids (HAs) in the United States in 2015. The research investigated if and in what form TV listening experiences of unaided and aided HI individuals might differ with regard to their viewing habits, difficulties they experience, and compensation strategies they employ. 515 HI people of ages 50+ years participated, 260 of whom owned HAs. HA users reported that they watched TV or video on average for 6 hours 10 min per day, 57 minutes longer than the duration reported by non-HA owners. Furthermore, HA users indicated fewer difficulties when watching TV than non-HA owners, suggesting that HA usage alleviated difficulties with TV listening. Nevertheless, the most frequent problems were still encountered by more than 39% of the HA users. Difficulties increased with greater self-reported unaided hearing disability, and female participants indicated more problems than male participants. Finally, those with carpeted floors reported fewer difficulties than those without carpets. The most frequently used compensation strategies were changing TV or HA volumes and using closed captioning. Only few HA users used audio streaming accessories. Given the exploratory nature of this study, further research is needed to inform interventions and improve the TV listening experiences of HI viewers. © 2018 Strelcyk, Singh. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,"adult; Article; building; controlled study; female; hearing impairment; human; major clinical study; male; middle aged; self report; sex difference; speech discrimination; television viewing; United States; aged; hearing; hearing aid; hearing impairment; pathophysiology; television; very elderly; Aged; Aged, 80 and over; Female; Hearing; Hearing Aids; Hearing Loss; Humans; Male; Middle Aged; Television",Article,Final,Scopus,2-s2.0-85049263862,Peter,,
"Szarkowska A., Gerber-Morón O.",54416458200;57202588774;,Viewers can keep up with fast subtitles: Evidence from eye movements,2018,PLoS ONE,13,6,e0199331,,,,42.0,10.1371/journal.pone.0199331,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048801213&doi=10.1371%2fjournal.pone.0199331&partnerID=40&md5=19c971471c88cb857d1a16fa27e190d7,"People watch subtitled audiovisual materials more than ever before. With the proliferation of subtitled content, we are also witnessing an increase in subtitle speeds. However, there is an ongoing controversy about what optimum subtitle speeds should be. This study looks into whether viewers can keep up with increasingly fast subtitles and whether the way people cope with subtitled content depends on their familiarity with subtitling and on their knowledge of the language of the film soundtrack. We tested 74 English, Polish and Spanish viewers watching films subtitled at different speeds (12, 16 and 20 characters per second). The films were either in Hungarian, a language unknown to the participants (Experiment 1), or in English (Experiment 2). We measured viewers’ comprehension, self-reported cognitive load, scene and subtitle recognition, preferences and enjoyment. By analyzing people’s eye gaze, we were able to discover that most viewers could read the subtitles as well as follow the images, coping well even with fast subtitle speeds. Slow subtitles triggered more rereading, particularly in English clips, causing more frustration and less enjoyment. Faster subtitles with unreduced text were preferred in the case of English videos, and slower subtitles with text edited down in Hungarian videos. The results provide empirical grounds for revisiting current subtitling practices to enable more efficient processing of subtitled videos for viewers. © 2018 Szarkowska, Gerber-Morón. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",,adult; article; comprehension; eye movement; female; frustration; gaze; human; human experiment; language; male; velocity; videorecording; cognition; eye movement; language; physiology; vision; Cognition; Eye Movements; Humans; Language; Videotape Recording; Visual Perception,Article,Final,Scopus,2-s2.0-85048801213,Peter,,
"Ma F., Ai H.",56581090000;56640752400;,Chinese Learners of English See Chinese Words When Reading English Words,2018,Journal of Psycholinguistic Research,47,3,,505,521,,6.0,10.1007/s10936-017-9533-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034599965&doi=10.1007%2fs10936-017-9533-8&partnerID=40&md5=a47637f3483a0a06fe37b487a67e2152,"The present study examines when second language (L2) learners read words in the L2, whether the orthography and/or phonology of the translation words in the first language (L1) is activated and whether the patterns would be modulated by the proficiency in the L2. In two experiments, two groups of Chinese learners of English immersed in the L1 environment, one less proficient and the other more proficient in English, performed a translation recognition task. In this task, participants judged whether pairs of words, with an L2 word preceding an L1 word, were translation words or not. The critical conditions compared the performance of learners to reject distractors that were related to the translation word (e.g., [InlineMediaObject not available: see fulltext.], pronounced as /bei 1/) of an L2 word (e.g., cup) in orthography (e.g., [InlineMediaObject not available: see fulltext.], bad in Chinese, pronounced as /huai 4/) or phonology (e.g., [InlineMediaObject not available: see fulltext.], sad in Chinese, pronounced as /bei 1/). Results of Experiment 1 showed less proficient learners were slower and less accurate to reject translation orthography distractors, as compared to unrelated controls, demonstrating a robust translation orthography interference effect. In contrast, their performance was not significantly different when rejecting translation phonology distractors, relative to unrelated controls, showing no translation phonology interference. The same patterns were observed in more proficient learners in Experiment 2. Together, these results suggest that when Chinese learners of English read English words, the orthographic information, but not the phonological information of the Chinese translation words is activated. In addition, this activation is not modulated by L2 proficiency. © 2017, Springer Science+Business Media, LLC, part of Springer Nature.",Bilingual lexicon; First language activation; Orthography; Phonology,adolescent; Asian continental ancestry group; China; female; human; linguistics; male; reading; translating (language); Adolescent; Asian Continental Ancestry Group; China; Female; Humans; Linguistics; Male; Reading; Translating,Article,Final,Scopus,2-s2.0-85034599965,Peter,,
"Ameri S., Khoshsaligheh M., Farid A.K.",57190000792;56946904400;57201642310;,The reception of Persian dubbing: a survey on preferences and perception of quality standards in Iran,2018,Perspectives: Studies in Translatology,26,3,,435,451,,11.0,10.1080/0907676X.2017.1359323,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027881572&doi=10.1080%2f0907676X.2017.1359323&partnerID=40&md5=3528980192cd08c33ef3344cc6007360,"Dubbing is Iran’s main modality for translating audiovisual content. There has been, however, relatively limited research on public recipients of dubbing in Iran. This article reports the results of a mixed-methods study that uses qualitative data to create a quantitative instrument to survey the reception of Persian dubbing among a selection of Iranian lay viewers (n = 477). In the qualitative phase, using focus group interviews (n = 5), a pool of items on dubbing reception is generated and used to design an original questionnaire. The quantitative survey results indicate that the reception of Persian dubbing could be manifested in six broad dimensions: technicality, agents, faithfulness, censorship, domestication and preferences. Most notably, technicality, including character synchrony and isochrony, is the highest-rated and most valued quality standard by participants. It is also revealed that dubbing reception does not vary significantly across gender, or with regard to different fields of study as a factor, except for two dimensions–technicality and domestication. © 2017 Informa UK Limited, trading as Taylor & Francis Group.",audiovisual translation; dubbing; Iran; lay viewers; quality standards; reception,,Article,Final,Scopus,2-s2.0-85027881572,Peter,,
"Berke L., Kafle S., Huenerfauth M.",57200494669;57200500342;12240800100;,Methods for evaluation of imperfect captioning tools by Deaf or Hard-of-Hearing users at different reading literacy levels,2018,Conference on Human Factors in Computing Systems - Proceedings,2018-April,,,,,,23.0,10.1145/3173574.3173665,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046964644&doi=10.1145%2f3173574.3173665&partnerID=40&md5=bdc95deb30eac8dee0a5d715706547e9,"As Automatic Speech Recognition (ASR) improves in accuracy, it may become useful for transcribing spoken text in real-time for Deaf and Hard-of-Hearing (DHH) individuals. To quantify users' comprehension and opinion of automatic captions, which inevitably contain some errors, we must identify appropriate methodologies for evaluation studies with DHH users, including quantitative measurement instruments suitable to the various literacy levels among the DHH population. A literature review guided our selection of several probes (e.g. multiple-choice comprehension-question accuracy or response time, scalar-questions about user estimation of ASR errors or their impact, users' numerical estimation of accuracy), which we evaluated in a lab study with DHH users, wherein their literacy levels and the actual accuracy of each caption stimulus were factors. For some probes, participants with lower literacy had more positive subjective responses overall, and, for participants with particular literacy score ranges, some probes were insufficiently sensitive to distinguish between caption accuracy levels. © 2018 Association for Computing Machinery.",Accessibility; Captioning; Evaluation methods; Literacy; People who are deaf or hard-of-hearing,Character recognition; Human engineering; Instrument errors; Probes; Speech recognition; Accessibility; Captioning; Evaluation methods; Hard of hearings; Literacy; Audition,Conference Paper,Final,Scopus,2-s2.0-85046964644,Peter,,
"Peng Y.-H., Hsu M.-W., Taele P., Lin T.-Y., Lai P.-E., Hsu L., Chen T.-C., Wu T.-Y., Chen Y.-A., Tang H.-H., Chen M.Y.",57211016076;57194266784;25926107700;57202046963;57202050753;57202045679;57202046409;57188750586;57196264562;7403124549;35766280300;,Speechbubbles: Enhancing captioning experiences for Deaf and hard-of-hearing people in group conversations,2018,Conference on Human Factors in Computing Systems - Proceedings,2018-April,,,,,,35.0,10.1145/3173574.3173867,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046943114&doi=10.1145%2f3173574.3173867&partnerID=40&md5=61204fe704c1011e1179e3320e7c7075,"Deaf and hard-of-hearing (DHH) individuals encounter difficulties when engaged in group conversations with hearing individuals, due to factors such as simultaneous utterances from multiple speakers and speakers whom may be potentially out of view. We interviewed and co-designed with eight DHH participants to address the following challenges: 1) associating utterances with speakers, 2) ordering utterances from different speakers, 3) displaying optimal content length, and 4) visualizing utterances from out-of-view speakers. We evaluated multiple designs for each of the four challenges through a user study with twelve DHH participants. Our study results showed that participants significantly preferred speech bubble visualizations over traditional captions. These design preferences guided our development of SpeechBubbles, a realtime speech recognition interface prototype on an augmented reality head-mounted display. From our evaluations, we further demonstrated that DHH participants preferred our prototype over traditional captions for group conversations. Copyright © 2017 ACM.",Accessibility; Augmented reality; Closed captions; Deaf and hard of hearing; Hololens; Text bubbles; Word balloons,Augmented reality; Helmet mounted displays; Human engineering; Speech recognition; Accessibility; Closed captions; Hard of hearings; Hololens; Word balloons; Audition,Conference Paper,Final,Scopus,2-s2.0-85046943114,Peter,,
"Szarkowska A., Krejtz K., Dutka Ł., Pilipczuk O.",54416458200;55258716700;57190034357;56018914300;,Are interpreters better respeakers?,2018,Interpreter and Translator Trainer,12,2,,207,226,,8.0,10.1080/1750399X.2018.1465679,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046024255&doi=10.1080%2f1750399X.2018.1465679&partnerID=40&md5=027124210aeff34433ab12afdc2880db,"In this study, we examined whether interpreters and interpreting trainees are better predisposed to respeaking than people with no interpreting skills. We tested 57 participants (22 interpreters, 23 translators and 12 controls) while respeaking 5-minute videos with two parameters: speech rate (fast/slow) and number of speakers (one/many). Having measured the quality of the respeaking performance using two independent methods: the NER model and rating, we found that interpreters consistently achieved higher scores than the other two groups. The findings are discussed in the context of transfer of skills, expert performance and respeaking training. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.",audiovisual translation; competence; NER; interpreting; live subtitling; Respeaking,,Article,Final,Scopus,2-s2.0-85046024255,Peter,,
"Kushalnagar P., Moreland C.J., Simons A., Holcomb T.",17135146200;36572291300;57201094145;57201090416;,Communication barrier in family linked to increased risks for food insecurity among deaf people who use American Sign Language,2018,Public Health Nutrition,21,5,,912,916,,17.0,10.1017/S1368980017002865,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043390151&doi=10.1017%2fS1368980017002865&partnerID=40&md5=5688714608b66aef9c901a7bc15bfc97,"Objective Food security is defined as being able to access enough food that will help maintain an active, healthy lifestyle for those living in a household. While there are no studies on food security issues among deaf people, research shows that communication barriers early in life are linked to poor physical and mental health outcomes. Childhood communication barriers may also risk later food insecurity. Design/Setting/Subjects A single food security screener question found to have 82 % sensitivity in classifying families who are at risk for food insecurity was taken from the six-item US Household Food Security Survey Module. Questions related to food insecurity screener, depression diagnosis and retrospective communication experience were translated to American Sign Language and then included in an online survey. Over 600 deaf adult signers (18-95 years old) were recruited across the USA. Results After adjusting for covariates, deaf adults who reported being able to understand little to none of what their caregiver said during their formative years were about five times more likely to often experience difficulty with making food last or finding money to buy more food, and were about three times more likely to sometimes experience this difficulty, compared with deaf adults who reported to being able to understand some to all of what their caregiver said. Conclusions Our results have highlighted a marked risk for food insecurity and related outcomes among deaf people. This should raise serious concern among individuals who have the potential to effect change in deaf children's access to communication. Copyright © The Authors 2018.",Communication; Deaf; Family relationship; Food security; Language access,"adolescent; adult; aged; caregiver; catering service; child; communication barrier; comprehension; family; family size; female; handicapped child; hearing impairment; human; interpersonal communication; male; middle aged; questionnaire; retrospective study; risk factor; sign language; very elderly; young adult; Adolescent; Adult; Aged; Aged, 80 and over; Caregivers; Child; Communication; Communication Barriers; Comprehension; Deafness; Disabled Children; Family; Family Characteristics; Female; Food Supply; Humans; Male; Middle Aged; Retrospective Studies; Risk Factors; Sign Language; Surveys and Questionnaires; Young Adult",Article,Final,Scopus,2-s2.0-85043390151,Peter,,
"Monti L., Delnevo G.",57193159740;57193867382;,On improving GlovePi: Towards a many-to-many communication among deaf-blind users,2018,CCNC 2018 - 2018 15th IEEE Annual Consumer Communications and Networking Conference,2018-January,,,1,5,,10.0,10.1109/CCNC.2018.8319236,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046976100&doi=10.1109%2fCCNC.2018.8319236&partnerID=40&md5=c8f98711f43f86f61a30c14f7a4d1a15,"The wide diffusion of mobile devices, digital technologies and telecommunication providers and infrastructures greatly supports communication and social activities among people all over the world. This (r)evolution in communication could represent a great opportunity for those people who use assistive technologies due to some kinds of disability, but it could become a digital severe barrier at the same time. Assistive technologies supporting people with disabilities can be a strategic tool to enhance their inclusion, integration, and independence, in particular for persons with disabilities that involve more senses, such as deaf-blindness, which is the combination of blindness and deafness. Deaf-blind users can communicate by mainly exploiting the sense of touch. Focusing on this kind of communication, we have designed and developed GlovePi, a low-cost wearable device, based on a glove equipped with sensors, a raspberry-pi and mobile devices. In this paper, we present an improved version of the GlovePi system, which extends the form of communication, by supporting the many-to-many one, aiming to increase the inclusion of deaf-blind people in social life and daily activities. © 2018 IEEE.",accessibility; deaf-blind users; many-to-many communication; people with disability; wearable devices,Eye protection; Mobile devices; Wearable technology; accessibility; Deaf blinds; Many-to-many communications; People with disabilities; Wearable devices; Digital devices,Conference Paper,Final,Scopus,2-s2.0-85046976100,Peter,,
"Yeratziotis A., Zaphiris P.",34876032000;6602402531;,A Heuristic Evaluation for Deaf Web User Experience (HE4DWUX),2018,International Journal of Human-Computer Interaction,34,3,,195,217,,17.0,10.1080/10447318.2017.1339940,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028566064&doi=10.1080%2f10447318.2017.1339940&partnerID=40&md5=ea32732095e09d59d52e27a7f87afb3e,"Websites that are usable and accessible can have a positive impact on the overall user experience. Usability Inspection Methods (UIMs) can be applied to evaluate and measure the usability. The current research in the fields of Web Accessibility and Human –Computer Interaction (HCI) is in need of additional UIMs that can be applied to also measure the accessibility, in addition to the usability alone. In this article, a novel UIM in the form of a heuristic evaluation is presented. The heuristic evaluation aims to support HCI experts and Web developers in designing and evaluating websites that provide positive user experiences to users who are deaf. This article discusses the development of the Heuristic Evaluation for Deaf Web User Experience (HE4DWUX). Following an iteration cycle, version 2 of the HE4DWUX is presented in Appendix A. An existing three-phase process to develop heuristics for specific application domains was applied to construct the HE4DWUX. The outcome of this research is 12 heuristics, with each containing its own set of checklist items to operationalize its applicability in measuring the Web user experience for users who are deaf. The heuristics and their checklist items can identify important aspects of design that will impact the Web user experience for this particular user group. © 2017 Taylor & Francis Group, LLC.",,Human computer interaction; Iterative methods; Transportation; User interfaces; Websites; Computer interaction; Heuristic evaluation; Iteration cycle; Usability inspection; User experience; User groups; Web accessibility; Web developers; Web Design,Article,Final,Scopus,2-s2.0-85028566064,Peter,,
Meloncon L.,26026395700;,Orienting Access in Our Business and Professional Communication Classrooms,2018,Business and Professional Communication Quarterly,81,1,,34,51,,8.0,10.1177/2329490617739885,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041560762&doi=10.1177%2f2329490617739885&partnerID=40&md5=e8617096e43fc83b96528d2892ef8aa2,"A hallmark of business and professional communication is an emphasis on pragmatic but theoretically grounded work. Thus, business and professional communication scholars are ideally suited to turn the theories found in disability studies into practice. In this article, I do just that by creating a theory—orienting access—that draws on concepts from disability studies. Orienting access calls for business and professional communication faculty to consider alternate pedagogies to ensure that our classrooms are truly accessible to all students. It also models the behaviors to teach how to design and create information that is accessible for all audiences. © 2018, © 2018 by the Association for Business Communication.",access; orientation; pedagogy; practice; theory,,Article,Final,Scopus,2-s2.0-85041560762,Peter,,
Clegg G.M.,57201492655;,Unheard Complaints: Integrating Captioning Into Business and Professional Communication Presentations,2018,Business and Professional Communication Quarterly,81,1,,100,122,,5.0,10.1177/2329490617748710,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041558825&doi=10.1177%2f2329490617748710&partnerID=40&md5=563822e9e677a441d63aa7713038eee8,"This article explores pedagogical frameworks closely associated with d/Deaf and hard-of-hearing persons from the perspective of a disabled instructor to increase student awareness of the needs of diverse audiences they will encounter in the workforce. The author argues that students and instructors can use captioning theory to strategize one of the harder business communication genres, the presentation, for d/Deaf audiences to make communication more accessible. By raising critical awareness of the limits of technology, current trends in pedagogy, and disability, this article seeks to further the conversation about providing accessibility for disabled users in the classroom. © 2018, © 2018 by the Association for Business Communication.",captioning; deaf and hard-of-hearing pedagogy; deafness; presentation,,Article,Final,Scopus,2-s2.0-85041558825,Peter,,
Marshall S.J.,57200792766;,Shaping the university of the future: Using technology to catalyse change in university learning and teaching,2018,Shaping the University of the Future: Using Technology to Catalyse Change in University Learning and Teaching,,,,1,592,,45.0,10.1007/978-981-10-7620-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045418474&doi=10.1007%2f978-981-10-7620-6&partnerID=40&md5=164d8e4c5b6c3ee222b1cdd119b850c9,"This book focuses on developing an understanding of the complex interplay of forces acting on individual universities and higher education systems to enable leaders and practitioners to take purposeful and strategic action. It explores the challenging landscape of higher education and the pressures that are reshaping the university as a societal institution, describing the complex interplay of technological, sociological, political and economic forces driving change. The issues analysed are global in scope, reflecting the diversity of contexts, but also the common nature of the challenges facing institutions individually and collectively. The analysis draws on the lessons learnt and evidence from over fifty organisational case studies undertaken by the author over the past decade, exploring organisational change in higher education institutions in New Zealand, Australia, the United States and the United Kingdom, and on his engagement as president of the ACODE organisation with colleagues responsible for learning technological change in Australasia. The book helps institutions respond to technological change purposefully, in ways that build upon a clear understanding of the complex nature of the existing institution, its students and the organisational context. © Springer Nature Singapore Pte Ltd. 2018. All rights reserved.",,,Book,Final,Scopus,2-s2.0-85045418474,Peter,,
Batchelor K.,25421373200;,TRANSLATION AND PARATEXTS,2018,Translation and Paratexts,,,,1,202,,70.0,10.4324/9781351110112,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141196025&doi=10.4324%2f9781351110112&partnerID=40&md5=500146347254dc1e27f1358987a387b1,"As the 'thresholds' through which readers and viewers access texts, paratexts have already sparked important scholarship in literary theory, digital studies and media studies. Translation and Paratexts explores the relevance of paratexts for translation studies and provides a framework for further research. Writing in three parts, Kathryn Batchelor first offers a critical overview of recent scholarship, and in the second part introduces three original case studies to demonstrate the importance of paratextual theory. Batchelor interrogates English versions of Nietzsche, Chinese editions of Western translation theory, and examples of subtitled drama in the UK, before concluding with a final part outlining a theory of paratextuality for translation research, addressing questions of terminology and methodology. Translation and Paratexts is essential reading for students and researchers in translation studies, interpreting studies and literary translation. © 2018 Kathryn Batchelor.",,,Book,Final,Scopus,2-s2.0-85141196025,Peter,,
"Lin C.Y., Wang M.",36998734000;55553730708;,The use of lexical and sublexical cues in speech segmentation by second language learners of English,2018,Journal of Second Language Studies,1,1,,166,198,,,10.1075/jsls.17017.lin,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121030389&doi=10.1075%2fjsls.17017.lin&partnerID=40&md5=1a2301837eb6af3afb337e6f7b39ed26,"This study examined the use of lexical and sublexical cues in speech segmentation by Mandarin L2 learners of English, focusing on two types of lexical cue, lexical knowledge and semantic relatedness, and three coda (sublexical) cues,/n, s, ŋ/due to their varying phonotactic probabilities in Mandarin and English. Thirty-five native English speakers and 30 L2 learners participated in two experiments. Experiment 1 showed that learners were able to use lexicality as a cue to segment L2 speech. The lexicality effect significantly interacted with L2 proficiency. Experiment 2 showed that learners did not use semantic cues to the same extent as native listeners did. All participants experienced more difficulty with word boundary identification preceded by/s/. This difficulty may stem from weak allophonic cues of/s/in English. L2 learners with better proficiency may be better at recognizing familiar words from continuous speech, thus more efficiently utilizing the lexicality cue. © 2018 Journal of Second Language Studies. All Rights Reserved.",Lexical knowledge; Mandarin; Second language acquisition; Semantic relatedness; Speech segmentation,,Article,Final,Scopus,2-s2.0-85121030389,Peter,,
"Bokhove C., Downey C.",35931969900;36186788900;,Automated generation of ‘good enough’ transcripts as a first step to transcription of audio-recorded data,2018,Methodological Innovations,11,2,,,,,10.0,10.1177/2059799118790743,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107272587&doi=10.1177%2f2059799118790743&partnerID=40&md5=667e3bdee957b82219f890dd3a749f1e,"In the last decade, automated captioning services have appeared in mainstream technology use. Until now, the focus of these services have been on the technical aspects, supporting pupils with special educational needs and supporting teaching and learning of second language students. Only limited explorations have been attempted regarding its use for research purposes: transcription of audio recordings. This article presents a proof-of-concept exploration utilising three examples of automated transcription of audio recordings from different contexts; an interview, a public hearing and a classroom setting, and compares them against ‘manual’ transcription techniques in each case. It begins with an overview of literature on automated captioning and the use of voice recognition tools for the purposes of transcription. An account is provided of the specific processes and tools used for the generation of the automated captions followed by some basic processing of the captions to produce automated transcripts. Originality checking software was used to determine a percentage match between the automated transcript and a manual version as a basic measure of the potential usability of each of the automated transcripts. Some analysis of the more common and persistent mismatches observed between automated and manual transcripts is provided, revealing that the majority of mismatches would be easily identified and rectified in a review and edit of the automated transcript. Finally, some of the challenges and limitations of the approach are considered. These limitations notwithstanding, we conclude that this form of automated transcription provides ‘good enough’ transcription for first versions of transcripts. The time and cost advantages of this could be considerable, even for the production of summary or gisted transcripts. © The Author(s) 2018.",automated captions; automatic speech recognition; Interviews; qualitative data; technology; transcription,,Article,Final,Scopus,2-s2.0-85107272587,Peter,,
"Gerber-Morón O., Szarkowska A., Woll B.",57202588774;54416458200;7004344871;,The impact of text segmentation on subtitle reading,2018,Journal of Eye Movement Research,11,4,,1,18,,6.0,10.16910/11.4.2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102822383&doi=10.16910%2f11.4.2&partnerID=40&md5=b547926800a03d07226b82444d07d34d,"Understanding the way people watch subtitled films has become a central concern for subtitling researchers in recent years. Both subtitling scholars and professionals generally believe that in order to reduce cognitive load and enhance readability, line breaks in two-line subtitles should follow syntactic units. However, previous research has been inconclusive as to whether syntactic-based segmentation facilitates comprehension and reduces cognitive load. In this study, we assessed the impact of text segmentation on subtitle processing among different groups of viewers: hearing people with different mother tongues (English, Polish, and Spanish) and deaf hard of hearing, and hearing people with English as a first language. We measured three indicators of cognitive load (difficulty, effort, and frustration) as well as comprehension and eye tracking variables. Participants watched two video excerpts with syntactically and non-syntactically segmented subtitles. The aim was to determine whether syntactic-based text segmentation as well as the viewers’ linguistic background influence subtitle processing. Our findings show that non-syntactically segmented subtitles induced higher cognitive load, but they did not adversely affect comprehension. The results are discussed in the context of cognitive load, audiovisual translation, and deafness. © 2018. This article is licensed under a Creative Commons Attribution 4.0 International license.",audiovisual translation; cognitive load; eye movement; line breaks; media accessibility; reading; region of interest; revisits; segmentation; subtitling,,Article,Final,Scopus,2-s2.0-85102822383,Peter,,
"Rothe S., Tran K., Hussmann H.",57199996760;57203119664;23389275800;,Positioning of Subtitles in Cinematic Virtual Reality,2018,ICAT-EGVE 2018 - 28th International Conference on Artificial Reality and Telexistence and 23rd Eurographics Symposium on Virtual Environments,,,,1,8,,8.0,10.2312/egve.20181307,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085681965&doi=10.2312%2fegve.20181307&partnerID=40&md5=7377dc4a936caa88734e0d5914b11feb,"Cinematic Virtual Reality has been increasing in popularity in recent years.Watching 360° movies with a head mounted display, the viewer can freely choose the direction of view and thus the visible section of the movie. Therefore, a new approach for the placements of subtitles is needed. In a preliminary study we compared several static methods, where the position of the subtitles is not influenced by the movie content. The preferred method was used in the main study to compare it with dynamic, worldreferenced subtitling, where the subtitles are placed in the movie world. The position of the subtitles depends on the scene and is close to the speaking person. Even if the participants did not prefer one of these methods in general, for some cases in our experiments world-referenced subtitles led to a higher score of presence, less sickness and lower workload. © 2018 The Author(s).","Human-centered computing → Virtual reality; Multimedia information system → Artificial, augmented, and virtual realities","Helmet mounted displays; Motion pictures; User interfaces; Augmented and virtual realities; Cinematics; Head-mounted-displays; Human-centered computing; Human-centered computing → virtual reality; Multimedia information systems-artificial; Multimedium information system → artificial, augmented, and virtual reality; New approaches; Static method; Virtual reality",Conference Paper,Final,Scopus,2-s2.0-85085681965,Peter,,
Fernandez-Costales A.,55749871600;,On the reception of mobile content: New challenges in audiovisual translation research,2018,Benjamins Translation Library,141,,,297,319,,3.0,10.1075/btl.141.15fer,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064075648&doi=10.1075%2fbtl.141.15fer&partnerID=40&md5=80d4cd6c745a9831efc966d6a48c0ecb,"The reception of translated audiovisual contents in mobile devices has been largely ignored in Translation Studies. The current chapter is intended to identify key areas and challenges for research in Audiovisual Translation, so we can have a better understanding of how mobile contents are received and appreciated by users. Research in this field is modulated by the hybridity of audiovisual texts in mobile phones, since this type of text is multimodal, multichannel, and multiplatform. A second challenge is the potential target of mobile contents: the notion of audience(s) needs to be revisited in the new scenario, where ubiquity, immediacy, and global access to digital contents have sparked the diversification of users' profiles. The spotlight of this chapter is on research opportunities within the reception of mobile contents; the complexity of investigating new types of global audiences clearly demands interdisciplinary approaches and research designs. © 2018 John Benjamins Publishing Company.",Audience; Audiovisual translation; Mobile contents; Reception; Users,,Book Chapter,Final,Scopus,2-s2.0-85064075648,Peter,,
"Szarkowska A., Dutka Ł., Szychowska A., Pilipczuk O.",54416458200;57190034357;57224337432;56018914300;,Visual attention distribution in intralingual respeaking: An eye-tracking study,2018,Benjamins Translation Library,143,,,185,201,,,10.1075/btl.143.09sza,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064007657&doi=10.1075%2fbtl.143.09sza&partnerID=40&md5=296d9b30f30e719bd46bff87ed87ad26,"Respeaking is a method of producing real-time subtitles for live television programmes, enabling access to the media for people who are deaf, hard of hearing or who support their viewing with subtitles. Respeaking requires a set of skills akin to both interpreting and subtitling, including multitasking and management of concurrent sources of incoming information. In this chapter we present the results of an eye-tracking study on the visual attention of respeakers during an intralingual respeaking task (Polish to Polish). We tested 57 people while they were respeaking a 5-minute news programme. Participants also underwent a short proof-reading task. There were three groups of participants: interpreters, translators and a control group of people with no interpreting/translation experience. We examined the number of fixations and mean fixation duration on major screen areas: picture, subtitles, dictation area and subtitle panel. We found that translators who had experience in subtitling were able to manage their visual attention most efficiently as they fixated more often on key screen areas and their fixations were longer than in other groups. They also achieved the highest score on the proof-reading task, which suggest they could work in respeaking as editors. © 2018 John Benjamins Publishing Company",,,Book Chapter,Final,Scopus,2-s2.0-85064007657,Peter,,
Łabendowicz O.,57193705712;,The impact of AVT mode on audience reception,2018,Benjamins Translation Library,143,,,259,285,,,10.1075/btl.143.12lab,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063982857&doi=10.1075%2fbtl.143.12lab&partnerID=40&md5=177d2f819ddbdf642e528c05b5226c44,"Each mode of Audiovisual Translation (AVT) requires a slightly alternate approach. Whether a given audiovisual material is translated by the means of subtitling, dubbing or voice over often results in a very different translation of the same Source Text (ST), which is not surprising taking into consideration various technical constraints and translation tendencies typical of a given AVT mode. At the same time, audiences approach a given AV material with a prior set of expectations, which often include AVT mode preferences. Moreover, viewers' prior knowledge and fluency in Source Culture (SC) affects their perception of the end translation. According to Marchant et al. (2009: 154), “the overt visual attention detected by an eye-tracker gives us a window on internal systems” what, in turn, may be crucial for translators to improve their performance. The presented article is an attempt to show how audience really watches AV materials and how various modes of AVT affect viewers' reception of a given production. The main objective of the paper is to answer the question: How a mode of AVT affects audience's reception of humorous audiovisual materials deeply rooted in Source Culture? The analysis is based on the research involving eye-tracking devices conducted in May-July 2016 in Warsaw (Poland), during which the participants were asked to watch five short fragments of various humorous American productions deeply rooted in American culture (animated tv series: The Simpsons and South Park; animated feature film: Madagascar; tv series with actors: Gilmore Girls). © 2018 John Benjamins Publishing Company",,,Book Chapter,Final,Scopus,2-s2.0-85063982857,Peter,,
"De Houwer A., Ortega L.",10046487300;56209597100;,The Cambridge Handbook of Bilingualism,2018,The Cambridge Handbook of Bilingualism,,,,1,664,,14.0,10.1017/9781316831922,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062541736&doi=10.1017%2f9781316831922&partnerID=40&md5=0882d1ea559e589e7f70111fb57594ff,"The ability to speak two or more languages is a common human experience, whether for children born into bilingual families, young people enrolled in foreign language classes, or mature and older adults learning and using more than one language to meet life’s needs and desires. This Handbook offers a developmentally oriented and socially contextualized survey of research into individual bilingualism, comprising the learning, use and, as the case may be, unlearning of two or more spoken and signed languages and language varieties. A wide range of topics is covered, from ideologies, policy, the law, and economics, to exposure and input, language education, measurement of bilingual abilities, attrition and forgetting, and giftedness in bilinguals. Also explored are cross- and intra-disciplinary connections with psychology, clinical linguistics, second language acquisition, education, cognitive science, neurolinguistics, contact linguistics, and sign language research. © Cambridge University Press 2019.",,,Book,Final,Scopus,2-s2.0-85062541736,Peter,,
"Agulló B., Matamala A., Orero P.",57206483570;25921557700;24921771100;,From disabilities to capabilities: Testing subtitles in immersive environments with end users [De discapacidades a capacidades: Testando subtítulos en medios inmersivos con usuarios],2018,Hikma,17,,,195,220,,20.0,10.21071/hikma.v17i0.11167,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061726115&doi=10.21071%2fhikma.v17i0.11167&partnerID=40&md5=dc48236e0b81d3e1e23bf68dad724ed0,"User testing in Media Accessibility has often profiled users based on their disabilities. Subtitles for the deaf and hard of hearing, for instance, have been generally tested with their expected target audience, which is deaf and hard-of-hearing users. This article argues that selecting users based on sensory disabilities may not be the best strategy to obtain relevant results, as other capabilities––for instance, technological capabilities—may have a greater impact on the results. Moreover, the article argues that access services should not be exclusively for persons with disabilities but also for other audiences. If accessibility is mainstreamed, and ideally integrated in the creation and production processes, testing should expand beyond an exclusive approach based on accessibility to a more general approach based on usability where users with diverse capabilities are considered. To illustrate this point and propose a new approach to user testing in Media Accessibility, moving from a disability to a capability model, specific examples from the European Union funded project ImAc (Immersive Accessibility) are shown in a chronological order. Then, the article presents the initial testing, targeting persons with disabilities, and describes the poor data results leading to a new approach. A new testing focus is proposed, and the methodological shift is justified. After that, the second test in which the new approach is implemented is described, using the same stimuli but users with different levels of knowledge regarding new technologies. The article finishes with conclusions and final remarks in which the door is opened to move from an accessibility approach to testing to a usability approach. © 2018 Universidad de Cordoba, Servicio de Publicaciones. All Rights Reserved.",Immersive content; Media Accessibility; Subtitles; Subtitling for the deaf and hard of hearing; User profiling,,Article,Final,Scopus,2-s2.0-85061726115,Peter,,
"Holcomb T.K., Smith D.H.",7003999942;57204971596;,Deaf Eyes on Interpreting,2018,Deaf Eyes on Interpreting,,,,1,318,,5.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058181523&partnerID=40&md5=8751f48d84218a9166e57d8c5908fec9,"As the ASL-English interpreting field has become professionalized, there is a growing disconnect between interpreters and the Deaf consumers they serve. Whereas interpreting used to be a community-based practice, the field is growing into a research-based profession that begins in a classroom rather than in the Deaf community. Despite the many gains being made in the interpreting services profession, with an emphasis on the accuracy of the interpreted work, the perspectives of Deaf individuals are rarely documented in the literature. Opportunities for enhanced participation and full inclusion need to be considered in order for Deaf people to best represent themselves to the hearing, nonsigning public as competent and intelligent individuals. Deaf Eyes on Interpreting brings Deaf people to the forefront of the discussions about what constitutes quality interpreting services. The contributors are all Deaf professionals who use interpreters on a regular basis, and their insights and recommendations are based on research as well as on personal experiences. These multiple perspectives reveal strategies to maximize access to interpreted work and hearing environments and to facilitate trust and understanding between interpreters and Deaf consumers. Interpreter educators, interpreting students, professional interpreters, and Deaf individuals will all benefit from the approaches offered in this collection. © 2018 by Gallaudet University. All rights reserved.",,,Book,Final,Scopus,2-s2.0-85058181523,Peter,,
"Manchón L.M., Orero P.",55581210700;24921771100;,Usability tests for personalised subtitles,2018,Translation Spaces(Netherland),7,2,,263,284,,4.0,10.1075/ts.18016.man,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057777368&doi=10.1075%2fts.18016.man&partnerID=40&md5=895f92b148841189e46175c4b5d6e71b,"Over the past decade, subtitles have developed along with broadcast and broadband technology. Viewers nowadays enjoy new opportunities to customize subtitles to best meet their personal preferences or needs. This study aims to identify end-user subtitle setting preferences, to investigate whether these settings have an effect on content comprehension, and to explore subtitle usability for two groups of participants, those under the age of 65 and those over 65. In an experiment, three subtitle features were open to customization: the position, the box and the size of the subtitle, before and after participants watched an on-demand TV documentary produced by the Catalan public broadcaster TVC. Results confirm a preference for bottom and medium subtitles. Furthermore, while the under 65 segment made satisfactory use of the system, the over 65 segment experienced different levels of usability and reported different capabilities and problems. © John Benjamins Publishing Company.",Accessibility; Convergence; Customization; Subtitles; Usability; User-centric design,,Article,Final,Scopus,2-s2.0-85057777368,Peter,,
Allan K.,7003517206;,The oxford handbook of taboo words and language,2018,The Oxford Handbook of Taboo Words and Language,,,,1,448,,7.0,10.1093/oxfordhb/9780198808190.001.0001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057321479&doi=10.1093%2foxfordhb%2f9780198808190.001.0001&partnerID=40&md5=61d185892e83ea379abc44851c26f7c5,"The Oxford Handbook of Taboo Words and Language defines taboo as a proscription of behaviour for a specifiable community of one or more persons at a specifiable time in specifiable contexts. What is in fact tabooed is the use of those words and language in certain contexts; in short, the taboo applies to instances of language behaviour. For behaviour to be proscribed it must be perceived as in some way harmful to an individual or their community but the degree of harm can fall anywhere on a scale from a breach of etiquette to out-and-out fatality. All tabooed behaviours are deprecated and they lead to social if not legal sanction. Taboos are described and the reasons and beliefs behind them are investigated. Tabooed words are typically dysphemistic, think of insults and swearing; tabooed language is avoided through various kinds of euphemism. In twenty chapters, the volume offers comprehensive coverage of tabooed language as perceived by experts in general linguistics, cultural linguistics, sociolinguistics, anthropological linguistics, psycholinguistics, neurolinguistics, historical linguistics, linguistic philosophy, forensic linguistics, politeness research, publishing, advertising, and theology. Although the principal focus is the English language, reference is occasionally made to linguistic taboos in other languages in order to compare sociocultural attitudes. The existence of taboos and the need to manage taboo lead not only to the censoring of behaviour and the imposition of censorship but also to language change and language development. © editorial matter and organization Keith Allan 2019 and the chapters their several authors 2019.",Avoidance-language; Censorship; Dysphemism; Euphemism; Insult; Language change; Language development; Proscribed behaviour; Swearing; Taboo,,Book,Final,Scopus,2-s2.0-85057321479,Peter,,
"Lopez M., Kearney G., Hofstädter K.",56016901200;24381662200;57193645192;,"Audio description in the uk: What works, what doesn’t, and understanding the need for personalising access",2018,British Journal of Visual Impairment,36,3,,274,291,,11.0,10.1177/0264619618794750,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053342443&doi=10.1177%2f0264619618794750&partnerID=40&md5=197bdd073777d5ddfb3e1d604a4f8841,"Audio Description for film and television is a pre-recorded track that uses verbal descriptions to provide information on visual aspects of a film or TV programme. In the UK, it is currently the only accessibility strategy available for visually impaired audiences and although it provides access to a large number of people, its shortcomings also fail to engage others in audiovisual experiences. The Enhancing Audio Description project explores how digital audio technologies can be applied to the creation of alternatives to Audio Description with the aim of personalising access strategies. Such personalisation would allow users to select the method utilised to access audiovisual experiences, by having choices that include traditional forms of accessibility as well as sound design–based methods. The present article analyses the results of a survey and focus groups in which visually impaired participants discussed the advantages and disadvantages of AD and it demonstrates not only the diversity of experiences and needs of visually impaired groups but also their eagerness for change. © The Author(s) 2018 Article reuse guidelines: sagepub.com/journals-permissions.",Accessibility; Audio description; Audio technologies; Film; Sound design; Visual impairment,,Article,Final,Scopus,2-s2.0-85053342443,Peter,,
"Gerber-Morón O., Szarkowska A., Woll B.",57202588774;54416458200;7004344871;,The impact of text segmentation on subtitle reading,2018,Journal of Eye Movement Research,11,4,2,,,,8.0,10.16910/jemr.11.4.2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052838309&doi=10.16910%2fjemr.11.4.2&partnerID=40&md5=a7c755fa6c31cb05e327966ae3e86c1c,"Understanding the way people watch subtitled films has become a central concern for subtitling researchers in recent years. Both subtitling scholars and professionals generally believe that in order to reduce cognitive load and enhance readability, line breaks in twoline subtitles should follow syntactic units. However, previous research has been inconclusive as to whether syntactic-based segmentation facilitates comprehension and reduces cognitive load. In this study, we assessed the impact of text segmentation on subtitle processing among different groups of viewers: hearing people with different mother tongues (English, Polish, and Spanish) and deaf, hard of hearing, and hearing people with English as a first language. We measured three indicators of cognitive load (difficulty, effort, and frustration) as well as comprehension and eye tracking variables. Participants watched two video excerpts with syntactically and non-syntactically segmented subtitles. The aim was to determine whether syntactic-based text segmentation as well as the viewers' linguistic background influence subtitle processing. Our findings show that non-syntactically segmented subtitles induced higher cognitive load, but they did not adversely affect comprehension. The results are discussed in the context of cognitive load, audiovisual translation, and deafness. © 2018, International Group for Eye Movement Research.",Audiovisual translation; Cognitive load; eye movement; Line breaks; Media accessibility; Reading; Region of interest; Revisits; Segmentation; Subtitling,,Article,Final,Scopus,2-s2.0-85052838309,Peter,,
"Zekveld A.A., Koelewijn T., Kramer S.E.",8641362500;22985390400;7401609154;,The Pupil Dilation Response to Auditory Stimuli: Current State of Knowledge,2018,Trends in Hearing,22,,,,,,97.0,10.1177/2331216518777174,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051048081&doi=10.1177%2f2331216518777174&partnerID=40&md5=3ca45d732ea5838879465d4c631a1377,"The measurement of cognitive resource allocation during listening, or listening effort, provides valuable insight in the factors influencing auditory processing. In recent years, many studies inside and outside the field of hearing science have measured the pupil response evoked by auditory stimuli. The aim of the current review was to provide an exhaustive overview of these studies. The 146 studies included in this review originated from multiple domains, including hearing science and linguistics, but the review also covers research into motivation, memory, and emotion. The present review provides a unique overview of these studies and is organized according to the components of the Framework for Understanding Effortful Listening. A summary table presents the sample characteristics, an outline of the study design, stimuli, the pupil parameters analyzed, and the main findings of each study. The results indicate that the pupil response is sensitive to various task manipulations as well as interindividual differences. Many of the findings have been replicated. Frequent interactions between the independent factors affecting the pupil response have been reported, which indicates complex processes underlying cognitive resource allocation. This complexity should be taken into account in future studies that should focus more on interindividual differences, also including older participants. This review facilitates the careful design of new studies by indicating the factors that should be controlled for. In conclusion, measuring the pupil dilation response to auditory stimuli has been demonstrated to be sensitive method applicable to numerous research questions. The sensitivity of the measure calls for carefully designed stimuli. © The Author(s) 2018.",auditory processing; listening effort; pupil response; pupillometry; review,attention; auditory stimulation; auditory threshold; dilatation; drug effect; female; hearing; human; male; mydriasis; physiology; procedures; pupil; reaction time; sensitivity and specificity; speech perception; Acoustic Stimulation; Attention; Auditory Perception; Auditory Threshold; Dilatation; Female; Hearing; Humans; Male; Mydriasis; Pupil; Reaction Time; Sensitivity and Specificity; Speech Perception,Review,Final,Scopus,2-s2.0-85051048081,Peter,,
"Kushalnagar R., Kushalnagar K.",36142036500;57200500181;,Subtitleformatter: Making subtitles easier to read for deaf and hard of hearing viewers on personal devices,2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),10896 LNCS,,,211,219,,2.0,10.1007/978-3-319-94277-3_35,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049781873&doi=10.1007%2f978-3-319-94277-3_35&partnerID=40&md5=8414924238874e19ec2b9e68b074d389,"For deaf or hard of hearing (DHH) viewers who cannot understand speech, many countries require video producers/distributors to provide speech-to-text over the video, also called subtitles that can be turned on or off by the viewer. These subtitles must comply with national subtitle quality standards. The growth in video capable personal devices has shifted viewers away from watching broadcast video on a standardized television display and towards watching video on interactive personal devices. However, personal devices range widely from tiny watch displays to enormous television displays, with different proportions which impact subtitle readability. SubtitleFormatter automatically formats subtitles according to a display’s screen size and minimum font size for reading. A user study of subtitle formatting evaluates subtitle readability, and finds that viewers preferred SubtitleFormatted-segmented subtitles over wrap around (arbitrarily-formatted) subtitles. © The Author(s) 2018.",Deaf or hard of hearing; Speech-to-text; Subtitles,Audition; Broadcast video; Different proportions; Hard of hearings; Personal devices; Quality standard; Screen sizes; Speech to texts; Subtitles; Display devices,Conference Paper,Final,Scopus,2-s2.0-85049781873,Peter,,
"Wasim M., Siddiqui A.A., Shaikh A., Ahmed L., Ali S.F., Saeed F.",57202782578;57203048208;56111398600;55669520200;57202831170;57202837510;,Communicator for hearing-impaired persons using Pakistan Sign Language (PSL),2018,International Journal of Advanced Computer Science and Applications,9,5,,197,202,,3.0,10.14569/IJACSA.2018.090525,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049497360&doi=10.14569%2fIJACSA.2018.090525&partnerID=40&md5=fa97cb53daf83daa31ebd2013c9549d7,"Communication with a hearing-impaired individual is a big challenge for a normal person. Hearing-impaired people uses hand gesture language (sign language) to communicate with each other, which is not easy to understand by a normal person because he/she is not trained to understand sign language. This communication gap between a hearing-impaired and a normal person created big problem for hearing-impaired individuals during their shopping, hospitalization, at their schools and homes. Especially in case of emergency, it is very difficult to understand the statement of a hearing-impaired one's who uses sign language. In the last few years researchers and developers from all over the world presented different ideas and works to solve this problem but no such solution is available to resolve this issue and can create two-way communication between hearing-impaired and normal persons. This paper presented a detail description about a two-way communication system based on Pakistan Sign Language (PSL). This duplex system is developed through conversion from the text in simple English into hand gestures and vice versa. However, conversion from hand gestures is available not only in text but also with voice providing more convenience to normal person. Main objective is to facilitate a large population and making hearing-impaired persons, the vital part of our civilization. A normal person can enter the text (sentence) in application, after the checking of spelling and grammar, the text is divided into tokens and sub-tokens. A token is a gesture against each word of the text while sub-tokens are the gestures of each character of the words. The combination of tokens created the gestures of text. On the other hand when gestures were input in to the application, using image processing technique, the nature of hand gesture were recognized and converted into corresponding text or voice. © 2015 The Science and Information (SAI) Organization Limited.",Communicator; Hand gesture; Hearing-impaired; Pakistan Sign Language (PSL); Special person; Token,,Article,Final,Scopus,2-s2.0-85049497360,Peter,,
"Colton J.S., Holmes S.",57188682248;56365784000;,A social justice theory of active equality for technical communication,2018,Journal of Technical Writing and Communication,48,1,,4,30,,40.0,10.1177/0047281616647803,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048714985&doi=10.1177%2f0047281616647803&partnerID=40&md5=b78e14eada51ec2baa4db6fe7991e7a0,"Certain aspects of social justice research tacitly work from political frameworks of “passive equality.” Passive equality can limit a technical communicator’s ability to enact social justice in terms of (a) signaling the presence of an injustice and (b) waiting for the organization, institution, or state to make the correction (e.g., liberalism’s distributive justice). By contrast, this article foregrounds the political philosophy of Jacques Rancière as a way to cultivate a practice of “active equality” that enables technical communicators to enact social justice rather than wait for institutional redistribution. © The Author(s) 2016",Closed captioning; Equality; Politics; Rancière; Social justice,,Article,Final,Scopus,2-s2.0-85048714985,Peter,,
"Petry B., Huber J., Nanayakkara S.",55533933700;36096007600;23009527800;,Scaffolding the Music Listening and Music Making Experience for the Deaf,2018,Cognitive Science and Technology,,,,23,48,,2.0,10.1007/978-981-10-6404-3_3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046974248&doi=10.1007%2f978-981-10-6404-3_3&partnerID=40&md5=266b4f2d94f59fe8aa08cfe2c115d4f5,"Music is an important part of our daily life. We listen to the radio, enjoy concerts or make music. © Springer Nature Singapore Pte Ltd 2018.",,,Book Chapter,Final,Scopus,2-s2.0-85046974248,Peter,,
"Ono S., Takamoto Y., Matsuda Y.",57200655477;6701608863;57200647876;,LiveTalk: Real-time information sharing between hearing-impaired people and people with normal hearing,2018,Fujitsu Scientific and Technical Journal,54,1,,58,63,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042127564&partnerID=40&md5=c438407436e8281259ae7b924b6fbb9d,"In recent years, more and more people have been recognizing that communication is very important in pursuing collaborations and innovations. Recent efforts to involve people from a variety of backgrounds in the development process, such as brainstorming, evaluation, and modification, have shown that such efforts can create new user experience (UX). Notably, there are projects where participants include people with certain difficulties. In April 2015, Fujitsu launched a new communication tool, FUJITSU Software LiveTalk, aiming to include hearing-impaired people in the circle of community. Throughout the developmental phases, we collaborated with contributors who were hearing-impaired. In the development of LiveTalk, we observed the participants to identify characteristic behaviors of hearing-impaired people in their workplaces as well as challenges they encounter when communicating with people who can hear normally. Their opinions were also shared with us to help create a prototype equipped with features that addressed them. Through the user evaluation and feedback on this prototype, we repeatedly improved the model to make it easier to use. It is an application that realizes smooth bilateral communications between hearing-impaired people and people with normal hearing, based on a new UX design. This paper explains the development of LiveTalk. © 2018 Fujitsu Ltd. All rights reserved.",,Electrical engineering; Hardware; Communication tools; Development process; Fujitsu; Hearing impaired; Normal hearing; Real-time information sharing; User evaluations; User experiences (ux); Audition,Conference Paper,Final,Scopus,2-s2.0-85042127564,Peter,,
"Berke L., Caulfield C., Huenerfauth M.",57200494669;57200506187;12240800100;,Deaf and hard-of-hearing perspectives on imperfect automatic speech recognition for captioning one-on-one meetings,2017,ASSETS 2017 - Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility,,,,155,164,,25.0,10.1145/3132525.3132541,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041416230&doi=10.1145%2f3132525.3132541&partnerID=40&md5=20f6e22ee6413e44889893cabb3814da,"Recent advances in Automatic Speech Recognition (ASR) have made this technology a potential solution for transcribing audio input in real-time for people who are Deaf or Hard of Hearing (DHH). However, ASR is imperfect; users must cope with errors in the output. While some prior research has studied ASR-generated transcriptions to provide captions for DHH people, there has not been a systematic study of how to best present captions that may include errors from ASR software nor how to make use of the ASR system's word-level confidence. We conducted two studies, with 21 and 107 DHH participants, to compare various methods of visually presenting the ASR output with certainty values. Participants answered subjective preference questions and provided feedback on how ASR captioning could be used with confidence display markup. Users preferred captioning styles with which they were already most familiar (that did not display confidence information), and they were concerned about the accuracy of ASR systems. While they expressed interest in systems that display word confidence during captions, they were concerned that text appearance changes may be distracting. The findings of this study should be useful for researchers and companies developing automated captioning systems for DHH users. © 2017 Association for Computing Machinery.",Automatic Speech Recognition; Communication; Deaf and Hard of Hearing; Feedback; Real-Time Captions; User Study,Audition; Communication; Feedback; Speech communication; Systematic errors; Transcription; Transportation; Audio input; Automatic speech recognition; Confidence information; Hard of hearings; Real time; Systematic study; User study; Word level; Speech recognition,Conference Paper,Final,Scopus,2-s2.0-85041416230,Peter,,
"Glasser A., Kushalnagar K., Kushalnagar R.",57195128152;57200500181;36142036500;,"Deaf, hard of hearing, and hearing perspectives on using automatic speech recognition in conversation",2017,ASSETS 2017 - Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility,,,,427,432,,14.0,10.1145/3132525.3134781,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041415204&doi=10.1145%2f3132525.3134781&partnerID=40&md5=674777472ab66f4cfc45eb9e501b62ff,"Many personal devices have transitioned from visual-controlled interfaces to speech-controlled interfaces to reduce costs and interactive friction, supported by the rapid growth in capabilities of speech-controlled interfaces, e.g., Amazon Echo or Apple's Siri. A consequence is that people who are deaf or hard of hearing (DHH) may be unable to use these speech-controlled devices. We show that deaf speech has a high error rate compared to hearing speech, in commercial speech-controlled interfaces. Deaf speech had approximately a 78% word error rate (WER) compared to a hearing speech 18% WER. Our findings show that current speech-controlled interfaces are not usable by DHH people. Based on our findings, significant advances in speech recognition software or alternative approaches will be needed for deaf use of speech-controlled interfaces. We show that current speech-controlled interfaces are not usable by DHH people. © c 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.",Automatic Speech Recognition; Deaf; Deaf Speech; Hearing,Audition; Speech; Transportation; Automatic speech recognition; Deaf; Hard of hearings; Personal devices; Reduce costs; Speech controlled interfaces; Speech recognition softwares; Word error rate; Speech recognition,Conference Paper,Final,Scopus,2-s2.0-85041415204,Peter,,
"Kafle S., Huenerfauth M.",57200500342;12240800100;,Evaluating the usability of automatically generated captions for people who are deaf or hard of hearing,2017,ASSETS 2017 - Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility,,,,165,174,,25.0,10.1145/3132525.3132542,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041408073&doi=10.1145%2f3132525.3132542&partnerID=40&md5=69ca7e5990ec4f98851a8edb33348eaa,"The accuracy of Automated Speech Recognition (ASR) technology has improved, but it is still imperfect in many settings. Researchers who evaluate ASR performance often focus on improving the Word Error Rate (WER) metric, but WER has been found to have little correlation with human-subject performance on many applications. We propose a new captioning-focused evaluation metric that better predicts the impact of ASR recognition errors on the usability of automatically generated captions for people who are Deaf or Hard of Hearing (DHH). Through a user study with 30 DHH users, we compared our new metric with the traditional WER metric on a caption usability evaluation task. In a side-by-side comparison of pairs of ASR text output (with identical WER), the texts preferred by our new metric were preferred by DHH participants. Further, our metric had significantly higher correlation with DHH participants' subjective scores on the usability of a caption, as compared to the correlation between WER metric and participant subjective scores. This new metric could be used to select ASR systems for captioning applications, and it may be a better metric for ASR researchers to consider when optimizing ASR systems.",Accessibility for People who are Deaf or Hard-of-Hearing; Automatic Speech Recognition; Caption Usability Evaluation; Real-time Captioning System,Audition; Transportation; Usability engineering; Automated speech recognition; Automatic speech recognition; Automatically generated; Evaluation metrics; Hard of hearings; Real time; Recognition error; Usability evaluation; Speech recognition,Conference Paper,Final,Scopus,2-s2.0-85041408073,Peter,,
"Huenerfauth M., Patel K., Berke L.",12240800100;57188741594;57200494669;,Design and psychometric evaluation of an American sign language translation of the system usability scale,2017,ASSETS 2017 - Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility,,,,175,184,,10.0,10.1145/3132525.3132540,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041398345&doi=10.1145%2f3132525.3132540&partnerID=40&md5=f6ac7dc4fafc592a1766f33a72380a24,"In usability studies, designers and researchers frequently use subjective questions to evaluate participants' impression of the usability of some product. The System Usability Scale (SUS) is a popular standardized questionnaire consisting of ten English statements about the usability of a product, to which participants indicate their agreement on a five-point scale. Many deaf adults in the U.S. have lower levels of English reading literacy, but there are currently no standardized questionnaires similar to SUS for Deaf and Hard-of-Hearing (DHH) users who are fluent in American Sign Language (ASL). To facilitate the inclusion of such users in studies, we created an ASL translation of SUS following accepted methods of survey translation: using a bilingual team including native ASL signers who are members of the Deaf community, along with back-translation evaluation to determine whether the meaning of the original was preserved. To validate whether key psychometric properties were preserved during translation, we deployed the ASL instrument in a study with 30 DHH participants. By comparing the results to users' responses to another measurement instrument, along with scores from 10 additional DHH participants responding to the original English SUS, we verified the criterion validity and internal reliability of the new ""ASL-SUS."" We are disseminating the translated instrument to promote the inclusion of DHH users in HCI research studies or in usability testing of consumer products. © 2017 Association for Computing Machinery.",American Sign Language; ASL-SUS; Criterion Validity; Internal Reliability; SUS; System Usability Scale; Translation,Audition; Consumer products; Instrument testing; Surveys; Transportation; Usability engineering; American sign language; ASL-SUS; Criterion Validity; Internal reliabilities; System usability; Translation (languages),Conference Paper,Final,Scopus,2-s2.0-85041398345,Peter,,
"Peruma A., El-Glaly Y.N.",57194647753;55498398400;,CollabAll: Inclusive discussion support system for deafand hearing students,2017,ASSETS 2017 - Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility,,,,315,316,,2.0,10.1145/3132525.3134800,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041397436&doi=10.1145%2f3132525.3134800&partnerID=40&md5=4fb3338d55db782265e0736d923c6ed2,"Even with advances in technology, group meetings between hearing and deaf and hard-of-hearing (D/HH) students can be challenging for all participants. This paper introduces CollabAll, a system that aims to better facilitate productive meetings between D/HH and hearing students. CollabAll provides D/HH individuals with a mechanism to actively participate in making decisions and getting their point across in team meetings. CollabAll enables every team member to create discussion topics for their meeting, track the person currently communicating and the current topic being discussed along with providing a mechanism for 'polite' interruptions. Early feedback from participatory design and focus group studies indicated a positive user experience. © 2017 Copyright held by the owner/author(s).",Deaf; Discussions; Meetings; Mixed Groups; Turn Taking,Students; Transportation; Deaf; Discussions; Meetings; Mixed Groups; Turn-taking; Audition,Conference Paper,Final,Scopus,2-s2.0-85041397436,Peter,,
"Egusa R., Kawaguchi S., Sakai T., Mizoguchi H., Kusunoki F., Namatame M., Inagaki S.",55212262100;57194835479;57007551700;35430648400;6701837740;22433574600;7202273286;,Implementation and evaluation of accessible caption system in universal puppetry: A case study on hearing impaired children at the elementary school of deaf,2017,CHI PLAY 2017 Extended Abstracts - Extended Abstracts Publication of the Annual Symposium on Computer-Human Interaction in Play,,,,207,212,,,10.1145/3130859.3131298,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034774131&doi=10.1145%2f3130859.3131298&partnerID=40&md5=4926cab9c5601431fc5db346a6aa5cfc,"This study tested and evaluated two functions developed as a part of the balloon-type caption display system in a universal puppetry. Two functions are the automatic location-determining function and the emotional expression function. The automatic locationdetermining function is feature that make captions positioned according to the positions in which the puppets move. And emotional expression function changes the shape of balloons that represents emotions such as joy, anger, sadness, and pleasure. We conducted the evaluation experiment. Participants are fifteen students with hearing impairments from the elementary school of deaf. The result shows the effectiveness of the automatic location-determining function and the emotional expression function as a means to compensate for vocal information in puppetry for hearing impaired children. © 2017 Copyright is held by the owner/author(s).",Caption display system; Hearing impaired children; Puppetry; Range image sensor,Abstracting; Balloons; Display devices; Function evaluation; Human computer interaction; Interactive computer systems; Location; Display system; Elementary schools; Emotional expressions; Evaluation experiments; Hearing impaired; Hearing impairments; Puppetry; Range image sensor; Audition,Conference Paper,Final,Scopus,2-s2.0-85034774131,Peter,,
[No author name available],[No author id available],CHI PLAY 2017 Extended Abstracts - Extended Abstracts Publication of the Annual Symposium on Computer-Human Interaction in Play,2017,CHI PLAY 2017 Extended Abstracts - Extended Abstracts Publication of the Annual Symposium on Computer-Human Interaction in Play,,,,,,744.0,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034744305&partnerID=40&md5=df9ce8ae9d24d209d57e93b29bd909fe,"The proceedings contain 94 papers. The topics discussed include: using HTC vive and touchdesigner to projection-map moving objects in 3D space: a playful participatory artwork; JumpGym: exploring the impact of a jumping exergame for waiting areas; touching base on children's interactions with tablet games; the Ocean game: assessing children's engagement and learning in a museum setting using a treasure-hunt game; AMELIO: evaluating the team-building potential of a mixed reality escape room game; cognitive rehabilitation potential of a driving simulation game for braininjury: a pilot study; assessing implicit computational thinking in Zoombinis gameplay: pizza pass, fleens & Bubblewonder Abyss; implementation and evaluation of accessible caption system in universal puppetry: a case study on hearing impaired children at the elementary school of deaf; developing and testing scrollquest: a video game targeting rejection sensitivity in adolescents; can card games be used to assess mild cognitive impairment? a study of klondike solitaire and cognitive functions; simple acts for a better world - a gameful system for prosocial behavior: preliminary design and research plan; comparison of two inventory design concepts in a collaborative virtual reality serious game; from board game to digital game: designing a mobile game for children to learn about invasive species; design box case study: facilitating interdisciplinary collaboration and participatory design in game development; and recommendations for building gamified calibration technologies for BCI applications.",,,Conference Review,Final,Scopus,2-s2.0-85034744305,Peter,,
"Haricharan H.J., Heap M., Hacking D., Lau Y.K.",55523332000;7007063028;37033936600;56900550000;,Health promotion via SMS improves hypertension knowledge for deaf South Africans,2017,BMC Public Health,17,1,663,,,,11.0,10.1186/s12889-017-4619-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027522091&doi=10.1186%2fs12889-017-4619-7&partnerID=40&md5=aa4474813d9bb9e3df81f448ea9a5022,"Background: Signing Deaf South Africans have limited access to health information. As a result, their knowledge about health is limited. Cell phone usage in South Africa is high. This study aimed to assess whether a short message service (SMS)-based health promotion campaign could improve Deaf people's knowledge of hypertension and healthy living. Additionally, the study aimed to assess the acceptability of using SMSs for health promotion targeting Deaf people. Methods: A baseline questionnaire assessed participants' knowledge about hypertension before an SMS-based information campaign was conducted. After the campaign, an exit questionnaire was conducted, containing the same questions as the baseline questionnaire with additional questions about general acceptability and communication preferences. Results were compared between baseline and exit, using McNemar's test, paired t-test and Wilcoxon signed-rank test. Focus groups aimed to get further information on the impact and acceptability of SMSs. The focus groups were analysed using inductive thematic analysis. Results: The campaign recruited 82 participants for the baseline survey, but due to significant loss-to-follow-up and exclusions only 41 participants were included in the analysis of the survey. The majority (60%) were men. Eighty percent were employed, while 98% had not finished high school. The campaign showed a statistically significant improvement in overall knowledge about hypertension and healthy living amongst participants. Six individual questions out of 19 also showed a statistically significant improvement. Despite this, participants in focus groups found the medical terminology difficult to understand. Several ways of improving SMS campaigns for the Deaf were identified. These included using using pictures, using 'signed' SMSs, combining SMSs with signed drama and linking SMS-campaigns to an interactive communication service that would enable the Deaf to pose questions for clarification. Focus groups suggested that participants who were hypertensive during the campaign adopted a healthier lifestyle. Conclusion: SMSs were effective in improving Deaf people's knowledge of hypertension and healthy living. However, SMS-campaigns should be cognizant of Deaf people's unique needs and communication preference and explore how to accommodate these. Trial registration: The research was registered with the Pan African Clinical Trial Registry on December 1, 2015. Identification number: PACTR201512001353476. © 2017 The Author(s).",Cell phones; Deaf; Health information; Health literacy; Health promotion; Healthy behaviour; mHealth; SMS; South Africa; Text messages,"adult; attitude to health; clinical trial; female; health promotion; hearing impaired person; human; hypertension; male; middle aged; procedures; program evaluation; psychology; questionnaire; South Africa; statistics and numerical data; text messaging; Adult; Female; Health Knowledge, Attitudes, Practice; Health Promotion; Humans; Hypertension; Male; Middle Aged; Persons With Hearing Impairments; Program Evaluation; South Africa; Surveys and Questionnaires; Text Messaging",Article,Final,Scopus,2-s2.0-85027522091,Peter,,
"Szarkowska A., Pilipczuk O., Krejtz K.",54416458200;56018914300;55258716700;,Respeaking crisis points. An exploratory study into critical moments in the respeaking process,2017,Audiovisual Translation - Research and Use,,,,179,202,,1.0,10.3726/b11097,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034036010&doi=10.3726%2fb11097&partnerID=40&md5=7692138944e8c88d7ef6f86493e1a811,"In this paper we introduce respeaking crisis points (RCPs), understood as potentially problematic moments in the respeaking process, resulting from the difficulty of the source material and/or cognitive overload on the part of the respeakers. We present results of the respeaking study or Polish participants who respoke four videos intralingually (Polish to Polish) and one interlingually (from English to Polish). We measured the participants' cognitive load with EEG (Emotiv) using two measures: Concentration and frustration. By analysing peaks in both EEG measures, we show where respeaking crisis points occurred. Features that triggered RCPs include very slow and very fast speech rate, visual complexity of the material, overlapping speech, numbers and proper names, speaker changes, word play, syntactic complexity, and implied meaning. The results of this study are directly applicable to respeaker training and provide valuable insights into the respeaking process from the cognitive perspective. © 2017. Peter Lang GmbH. All rights reserved.",Audiovisual translation; Cognitive load; Live subtitling; Respeaking; Respeaking crisis points,,Book Chapter,Final,Scopus,2-s2.0-85034036010,Peter,,
Vatavu R.-D.,24504604700;,"Visual Impairments and Mobile Touchscreen Interaction: State-of-the-Art, Causes of Visual Impairment, and Design Guidelines",2017,International Journal of Human-Computer Interaction,33,6,,486,509,,22.0,10.1080/10447318.2017.1279827,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015672548&doi=10.1080%2f10447318.2017.1279827&partnerID=40&md5=de6894453c1788879fe90cfeea766720,"This article identifies, catalogues, and discusses factors that are responsible for causing visual impairment of either a pathological or situational nature for touch and gesture input on smart mobile devices. Because the vast majority of interactions with touchscreen devices are highly visual in nature, any factor that prevents a clear, direct view of the mobile device’s screen can have potential negative implications on the effectiveness and efficiency of the interaction. This work presents the first overview of such factors, which are grouped in a catalogue of users, devices, and environments. The elements of the catalogue (e.g., psychological factors that relate to the user, or the social acceptability of mobile device use in public that relates to the social environment) are discussed in the context of current eye pathology classification from medicine and the recent literature in human–computer interaction on mobile touch and gesture input for people with visual impairments, for which a state-of-the-art survey is conducted. The goal of this work is to help systematize research on visual impairments and mobile touchscreen interaction by providing a catalogue-based view of the main causes of visual impairments affecting touch and gesture input on smart mobile devices. © 2017 Taylor & Francis Group, LLC.",,mHealth; Mobile devices; Ophthalmology; Computer interaction; Effectiveness and efficiencies; Psychological factors; Smart mobile devices; Social acceptability; Social environment; Touch-screen interaction; Visual impairment; Human computer interaction,Article,Final,Scopus,2-s2.0-85015672548,Peter,,
"Miller A., Malasig J., Castro B., Hanson V.L., Nicolau H., Brandão A.",57194281820;57194270490;57189298700;7006835286;34881822600;57188749310;,The use of smart glasses for lecture comprehension by Deaf and hard of hearing students,2017,Conference on Human Factors in Computing Systems - Proceedings,Part F127655,,,1909,1915,,18.0,10.1145/3027063.3053117,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019617454&doi=10.1145%2f3027063.3053117&partnerID=40&md5=cd79886ad03b0e74f1668a2e512234ac,"Deaf and hard of hearing students must constantly switch between several visual sources to gather all necessary information during a classroom lecture (e.g., instructor, slides, sign language interpreter or captioning). Using smart glasses, this research tested a potential means to reduce the effects of visual field switches, proposing that consolidating sources into a single display may improve lecture comprehension. Results showed no statistically significant comprehension improvements with the glasses, but interviews indicated that participants found it easier to follow the lecture with glasses and saw the potential for them in the classroom. Future work highlights priorities for smart glasses consideration and new research directions. Copyright © 2017 by the Association for Computing Machinery, Inc. (ACM).",Accessibility; American sign language (ASL); Deaf; Education; Multimedia,Education; Glass; Human engineering; Students; Visual languages; Accessibility; American sign language; Classroom lecture; Comprehension improvement; Deaf; Hard of hearings; Multimedia; Visual fields; Audition,Conference Paper,Final,Scopus,2-s2.0-85019617454,Peter,,
"Hong H.-T., Su T.-Y., Lee P.-H., Hsieh P.-C., Chiu M.-J.",57194277445;57194274869;57194274295;57188685180;57194272776;,VisualLink: Strengthening the connection between hearing-impaired elderly and their family,2017,Conference on Human Factors in Computing Systems - Proceedings,Part F127655,,,67,73,,6.0,10.1145/3027063.3049269,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019545343&doi=10.1145%2f3027063.3049269&partnerID=40&md5=e0bdad4429ffaf4699422f1fd593ee38,"Elderly who suffer from hearing impairment usually have difficulties talking on a phone due to the absence of face-to-face signals. This difficulty gradually weakens the connection between the elderly and their beloved family living remotely. To assist these hearing-impaired elderly, we propose VisualLink, a visual aids system for them to better receive and comprehend a call. Specifically, VisualLink notifies elderly of a call using notifications on large screen (TV) and wearable device with haptic feedback. The elderly then pick up the call via a VisualLink phone with a traditional-look, which automatically uses speech recognition to transcribe the call and show the caption on the screen, with interactive visual content such as images authored by the caller. In the evaluation of the prototype, participants gave VisualLink positive feedback for its ease of use and usefulness for making them better connected to their family's life. Copyright © 2017 by the Association for Computing Machinery, Inc. (ACM).",Assistive technology; Experimentation; Hearing impairment; Human factor,Human engineering; Speech recognition; Telephone sets; Assistive technology; Experimentation; Face to face; Haptic feedbacks; Hearing impaired; Hearing impairments; Visual content; Wearable devices; Audition,Conference Paper,Final,Scopus,2-s2.0-85019545343,Peter,,
"Zhang X., Ross A.S., Caspi A., Fogarty J., Wobbrock J.O.",56074117900;57200504677;57194274586;7004668263;6603152369;,Interaction proxies for runtime repair and enhancement of mobile application accessibility,2017,Conference on Human Factors in Computing Systems - Proceedings,2017-May,,,6024,6037,,37.0,10.1145/3025453.3025846,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041395739&doi=10.1145%2f3025453.3025846&partnerID=40&md5=89403053529966aaa6085570b26e67b0,"We introduce interaction proxies as a strategy for runtime repair and enhancement of the accessibility of mobile applications. Conceptually, interaction proxies are inserted between an application's original interface and the manifest interface that a person uses to perceive and manipulate the application. This strategy allows third-party developers and researchers to modify an interaction without an application's source code, without rooting the phone, without otherwise modifying an application, while retaining all capabilities of the system (e.g., Android's full implementation of the TalkBack screen reader). This paper introduces interaction proxies, defines a design space of interaction re-mappings, identifies necessary implementation abstractions, presents details of implementing those abstractions in Android, and demonstrates a set of Android implementations of interaction proxies from throughout our design space. We then present a set of interviews with blind and low-vision people interacting with our prototype interaction proxies, using these interviews to explore the seamlessness of interaction, the perceived usefulness and potential of interaction proxies, and visions of how such enhancements could gain broad usage. By allowing third-party developers and researchers to improve an interaction, interaction proxies offer a new approach to personalizing mobile application accessibility and a new approach to catalyzing development, deployment, and evaluation of mobile accessibility enhancements. Copyright is held by the owner/author(s). Publication rights licensed to ACM. © 2017 ACM.",Accessibility; Interaction proxies; Runtime modification,Abstracting; Human engineering; Mobile computing; Repair; Accessibility; Interaction proxies; Mobile accessibilities; Mobile applications; New approaches; Perceived usefulness; Runtimes; Screen readers; Android (operating system),Conference Paper,Final,Scopus,2-s2.0-85041395739,Peter,,
"Kurzhals K., Cetinkaya E., Hu Y., Wang W., Weiskopf D.",55390097400;57201445847;56480863900;35147101600;6603960393;,Close to the action: Eye-tracking evaluation of speaker-following subtitles,2017,Conference on Human Factors in Computing Systems - Proceedings,2017-May,,,6559,6568,,20.0,10.1145/3025453.3025772,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041285467&doi=10.1145%2f3025453.3025772&partnerID=40&md5=9dea40170d9a562a3b891af78f13dba9,"The incorporation of subtitles in multimedia content plays an important role in communicating spoken content. For example, subtitles in the respective language are often preferred to expensive audio translation of foreign movies. The traditional representation of subtitles displays text centered at the bottom of the screen. This layout can lead to large distances between text and relevant image content, causing eye strain and even that we miss visual content. As a recent alternative, the technique of speaker-following subtitles places subtitle text in speech bubbles close to the current speaker. We conducted a controlled eye-tracking laboratory study (n = 40) to compare the regular approach (center-bottom subtitles) with content-sensitive, speaker-following subtitles. We compared different dialog-heavy video clips with the two layouts. Our results show that speaker-following subtitles lead to higher fixation counts on relevant image regions and reduce saccade length, which is an important factor for eye strain. Copyright is held by the owner/author(s). Publication rights licensed to ACM. © 2017 ACM.",Eye tracking; Subtitle layout; Video,Eye movements; Human engineering; Speech recognition; Image content; Image regions; Laboratory studies; Multimedia contents; Subtitle layout; Video; Video clips; Visual content; Eye tracking,Conference Paper,Final,Scopus,2-s2.0-85041285467,Peter,,
"Lazar J., Feng J.H., Hochheiser H.",7007005493;55247917500;6602502080;,Research Methods in Human-Computer Interaction,2017,Research Methods in Human-Computer Interaction,,,,1,560,,471.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052224749&partnerID=40&md5=67c0e8eefacbbed857271c3bd62a371d,"Research Methods in Human-Computer Interaction is a comprehensive guide to performing research and is essential reading for both quantitative and qualitative methods. Since the first edition was published in 2009, the book has been adopted for use at leading universities around the world, including Harvard University, Carnegie-Mellon University, the University of Washington, the University of Toronto, HiOA (Norway), KTH (Sweden), Tel Aviv University (Israel), and many others. Chapters cover a broad range of topics relevant to the collection and analysis of HCI data, going beyond experimental design and surveys, to cover ethnography, diaries, physiological measurements, case studies, crowdsourcing, and other essential elements in the well-informed HCI researcher's toolkit. Continual technological evolution has led to an explosion of new techniques and a need for this updated 2nd edition, to reflect the most recent research in the field and newer trends in research methodology. This Research Methods in HCI revision contains updates throughout, including more detail on statistical tests, coding qualitative data, and data collection via mobile devices and sensors. Other new material covers performing research with children, older adults, and people with cognitive impairments. © 2017 Elsevier Inc. All rights reserved.",,User interfaces; Carnegie Mellon University; Cognitive impairment; Physiological measurement; Quantitative and qualitative methods; Research methodologies; Technological evolution; University of Toronto; University of Washington; Human computer interaction,Book,Final,Scopus,2-s2.0-85052224749,Peter,,
"Bigham J.P., Williams K., Banerjee N., Zimmerman J.",16238221500;57013983200;57195126716;7401859828;,Scopist: Building a skill ladder into crowd transcription,2017,"Proceedings of the 14th Web for All Conference, W4A 2017",,,a2,,,,2.0,10.1145/3058555.3058562,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025592063&doi=10.1145%2f3058555.3058562&partnerID=40&md5=acd46218364913b1c695b8128afb5fb6,"Audio transcription is an important task for making content accessible to people who are deaf or hard of hearing. Much of the transcription work is increasingly done by crowd workers, people online who pick up the work as it becomes available often in small bits at a time. Whereas work typically provides a ladder for skill development - a series of opportunities to acquire new skills that lead to advancement - crowd transcription work generally does not. To demonstrate how crowd work might create a skill ladder, we created Scopist, a JavaScript application for learning an efficient text-entry method known as stenotype while doing audio transcription tasks. Scopist facilitates on-the-job learning to prepare crowd workers for remote, real-time captioning by supporting both touch-typing and chording. Real-time captioning is a difficult skill to master but is important for making live events accessible. We conducted 3 crowd studies of Scopist focusing on Scopist's performance and support for learning. We show that Scopist can distinguish touch-typing from stenotyping with 94% accuracy. Our research demonstrates a new way for workers on crowd platforms to align their work and skill development with the accessibility domain while they work. © 2017 ACM.",Crowd work; Crowdsourcing; Novice to expert transition; Stenography; Text-entry; Worker training,Audition; Crowdsourcing; Education; Ladders; Transcription; Crowd work; Hard of hearings; Javascript; Novice to expert transition; Skill development; Stenography; Text entry; Text entry methods; Personnel training,Conference Paper,Final,Scopus,2-s2.0-85025592063,Peter,,
"Nakajima S., Okochi N., Iizumi N., Tsuru M., Mitobe K., Yamagami T.",7403109726;56267403500;57193792349;57193792621;7003592507;56266556000;,The possibility and challenges for deaf-blind individuals to enjoy films in theater,2017,Journal of Advanced Computational Intelligence and Intelligent Informatics,21,2,,350,358,,,10.20965/jaciii.2017.p0350,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016558879&doi=10.20965%2fjaciii.2017.p0350&partnerID=40&md5=13dc3e260d0776115b6c343b80f3fcc9,"In recent times, the use of subtitles and audio descriptions in movies for individuals with either hearing or visual impairment and the need to develop systems to provide these have been realized. However, even the need and possibility for deaf-blind individuals to enjoy movies have not been discussed yet. This study created an environment for deaf-blind individuals to ""watch"" a film, and conducted a screening of feature-length films with subtitles and audio descriptions. Interviews of 26 deaf-blind individuals indicated that 56% had watched films in a theater after becoming deaf-blind and before the screening session. When watching the films, 26.9% of participants used individual monitoring devices, headphones, or other conventional video or audio equipment. Furthermore, 50% were able to use either subtitles or audio descriptions. Regardless of their impairment conditions, participants responded positively towards watching the film in the screening session. Among the deaf-blind, 42.1% of the partially sighted and deaf, blind and hard of hearing, and partially sighted and hard of hearing individuals appreciated a special aspect of the theater, i.e., ""sharing an opportunity and communication with others."".",Deaf-blindness; Information support; Leisure; People with visual and hearing impairments; Watching films,Audio equipment; Audio systems; Theaters; Audio description; Blind individuals; Deaf-blindness; Hearing impairments; Individual monitoring; Information support; Leisure; Visual impairment; Audition,Article,Final,Scopus,2-s2.0-85016558879,Peter,,
"Culbertson G., Shen S., Andersen E., Jung M.",56159903000;35957404000;36195621900;15073123400;,Have your cake and eat it too: Foreign language learning with a crowdsourced video captioning system,2017,"Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW",,,,286,296,,16.0,10.1145/2998181.2998268,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014771455&doi=10.1145%2f2998181.2998268&partnerID=40&md5=7228bdb513230831b800e61293a18f47,"Learning from captioned foreign language videos is highly effective, but the availability of such videos is limited. By using speech-to-text technology to generate partially correct transcripts as a starting point, we see an opportunity for learners to build accurate foreign language captions while learning at the same time. We present a system where learners correct captions using automatic transcription and machinegenerated suggested alternative words for scaffolding. In a lab study of 49 participants, we found that compared to watching the video with accurate caption, learning and quality of experience were not significantly impaired by the secondary caption correction task using interface designs either with or without scaffolding from speech-to-text generated alternative words. Nevertheless, aggregating corrections reduced word error rate from 19% to 5.5% without scaffolding from suggested-alternatives, and 1.8% with scaffolding. Feedback from participants suggest that emphasizing the learning community contribution aspect is important for motivating learners and reducing frustration. Copyright © 2017 ACM.",Crowdsourcing; Language learning; Video learning,Computer supported cooperative work; Crowdsourcing; Groupware; Interactive computer systems; Quality of service; Scaffolds; Speech recognition; Automatic transcription; Foreign language; Foreign language learning; Interface designs; Language learning; Learning community; Quality of experience (QoE); Video learning; Learning systems,Conference Paper,Final,Scopus,2-s2.0-85014771455,Peter,,
"Gugenheimer J., Plaumann K., Schaub F., Di Campli San Vito P., Duck S., Rabus M., Rukzio E.",55876769400;56940999100;35190916400;57193548001;57193545713;57193546996;18233783900;,The impact of assistive technology on communication quality between deaf and hearing individuals,2017,"Proceedings of the ACM Conference on Computer Supported Cooperative Work, CSCW",,,,669,682,,22.0,10.1145/2998181.2998203,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014744766&doi=10.1145%2f2998181.2998203&partnerID=40&md5=6924e43ab40d8c0cb67eabdad3f9ef1e,"Deaf individuals often experience communication difficulties in face-to-face interactions with hearing people. In order to support deaf individuals in such situations, an active stream of assistive technology (AT) research focuses on real-time translation of sign language. We investigate the impact of real-time translation-based ATs on communication quality between deaf and hearing individuals. We conducted a focus group and a Wizard of Oz study in which deaf and hearing participants jointiy interacted with different assistive technologies. We find that while ATs facilitate communication, communication quality is degraded by to breaks in the conversation. Using Co-Cultural Theory, we identify deaf people as a subordinate group inside a hearing society. Our results indicate that current ATs reinforce this subordination by emphasizing deficiency of mastering the dominant form of communication. Based on our findings, we propose a change in design perspective by enabling the hearing to sign rather than the deaf to ""hear"". We argue that ATs should not be seen as ""just"" a tool for the Deaf but rather as a collaborative technology. © 2017 ACM.",Assistive technology; Co-Cultural Theory; Deaf; Developing assistive technology; Social implications,Computation theory; Computer hardware description languages; Computer supported cooperative work; Groupware; Interactive computer systems; Translation (languages); Assistive technology; Collaborative technologies; Communication quality; Cultural theory; Deaf; Face-to-face interaction; Social implication; Wizard-of-oz studies; Audition,Conference Paper,Final,Scopus,2-s2.0-85014744766,Peter,,
"Wang F., Nagano H., Kashino K., Igarashi T.",56103888600;7202707871;6701924954;7403250321;,Visualizing Video Sounds With Sound Word Animation to Enrich User Experience,2017,IEEE Transactions on Multimedia,19,2,7576622,418,429,,16.0,10.1109/TMM.2016.2613641,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010297441&doi=10.1109%2fTMM.2016.2613641&partnerID=40&md5=0ac21351c1e6dc827534e8dc18056ae5,"Sound information in videos plays an important role in shaping the user experience. When sound is not accessibl in videos, text captions can provide sound information. However, conventional text captions are not very expressive for nonverbal sounds because they are designed to visualize speech sounds. Here, we present a framework to automatically transform nonverbal video sounds into animated sound words and position them near the sound source objects in the video for visualization. This provides natural visual representation of nonverbal sounds with rich information about the sound category and dynamics. To evaluate how the animated sound words generated by our framework affect the user experience, we implemented an experimental system and conducted a user study involving over 300 participants from an online crowdsourcing service. The results of the user study show that the animated sound words can effectively and naturally visualize the dynamics of sound while clarifying the position of the sound source as well as contribute to making video-watching more enjoyable and increasing the visual impact of videos. © 2016 IEEE.",Animation; entertainment; environmental sound processing; sound visualization; sound word; user experience; video annotation,Acoustic generators; Animation; Visualization; entertainment; Environmental sounds; Sound visualization; User experience; Video annotations; Video signal processing,Article,Final,Scopus,2-s2.0-85010297441,Peter,,
"Dharamsi T., Jawahar R., Mahesh K., Srinivasa G.",57225257980;57193355032;58084572400;15061874300;,Stringing Subtitles in Sign Language,2017,"Proceedings - IEEE 8th International Conference on Technology for Education, T4E 2016",,,7814830,228,231,,1.0,10.1109/T4E.2016.055,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013249511&doi=10.1109%2fT4E.2016.055&partnerID=40&md5=ea5e252419f879ac82feded48d1dfc76,"This paper presents a system designed as a sign language teaching aid. This system is capable of generating the sign-language equivalent of an input phrase or sentence in textual or audio form, with automated processing for language constructs such as tenses and plurals. For words or phrases, especially named entities not present in the repository, the system strings together images or video clips of alphabets to transcribe the input. The system also has the provision for a user to record their version of phrases and words as video clips (or still images) to augment the system's repository. This is particularly useful for pedagogy in Indian sign languages since most schools have their own vocabulary. The system was designed with inputs and continuous feedback from faculty at the Mathru Center for the Deaf, Dumb and Differently-Abled in Bengaluru. We envisage continuing to test the system and augment features to eventually open this up as a useful avenue for the pedagogy of sign languages. © 2016 IEEE.",automated transcription; sign language; teaching-aid; text parsing; text to video,Education; Syntactics; Transcription; Video cameras; Automated processing; Indian sign languages; Language constructs; Named entities; Sign language; Teaching aids; Text parsing; text to video; Teaching,Conference Paper,Final,Scopus,2-s2.0-85013249511,Peter,,
"Harris R.J., Borror S.W., Koblitz K.R., Pearn M., Rohrer T.C.",36182974200;57126224800;57126373400;57126131800;57125800600;,"Memory for Emotional Content from Entertainment Film: The Role of Subtitles, Sex, and Empathy",2017,Media Psychology,20,1,,28,59,,3.0,10.1080/15213269.2015.1121828,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958524953&doi=10.1080%2f15213269.2015.1121828&partnerID=40&md5=a0c1dacc30c8cc7236077cea115ebde3,"A major component of consuming filmed entertainment is vicariously experiencing emotions of the filmed characters, yet little is known about how this occurs. Four experiments assessed memory for emotions felt by characters in extended film clips from either a contemporary farce (Overboard) or a historical drama (Sense and Sensibility) under various conditions of native or foreign language in the dialogue or subtitles. English-speaking participants watched a clip and then assessed on 6-point scales specific positive and negative affect felt by characters at various points during the film. Both positive and negative affect felt by characters in both films were perceived and remembered better in conditions with English sound or subtitles than in conditions with no English channel, although, unexpectedly, spoken dialogue or subtitles alone were equally effective at conveying emotion. Overall, emotion memory from the contemporary farce was better than from the historical drama and was surprisingly good even in conditions with no English, especially for the farce. Conditions with only sound, subtitles, or pictures were very comparable to each other. Participants higher in trait fantasy empathy remembered emotions better. The components of an eventual model of emotional comprehension were sketched. Copyright © Taylor & Francis Group, LLC.",,,Article,Final,Scopus,2-s2.0-84958524953,Peter,,
"Goldin-Meadow S., Brentari D.",7004380699;6505907624;,"Gesture, sign, and language: The coming of age of sign language and gesture studies",2017,Behavioral and Brain Sciences,40,,,1,17,,139.0,10.1017/S0140525X15001247,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057631298&doi=10.1017%2fS0140525X15001247&partnerID=40&md5=bc8a1878d81ada93c9451153aef97525,"How does sign language compare with gesture, on the one hand, and spoken language on the other? Sign was once viewed as nothing more than a system of pictorial gestures without linguistic structure. More recently, researchers have argued that sign is no different from spoken language, with all of the same linguistic structures. The pendulum is currently swinging back toward the view that sign is gestural, or at least has gestural components. The goal of this review is to elucidate the relationships among sign language, gesture, and spoken language. We do so by taking a close look not only at how sign has been studied over the past 50 years, but also at how the spontaneous gestures that accompany speech have been studied. We conclude that signers gesture just as speakers do. Both produce imagistic gestures along with more categorical signs or words. Because at present it is difficult to tell where sign stops and gesture begins, we suggest that sign should not be compared with speech alone but should be compared with speech-plusgesture. Although it might be easier (and, in some cases, preferable) to blur the distinction between sign and gesture, we argue that distinguishing between sign (or speech) and gesture is essential to predict certain types of learning and allows us to understand the conditions under which gesture takes on properties of sign, and speech takes on properties of gesture. We end by calling for new technology that may help us better calibrate the borders between sign and gesture. © 2017 SAGE Publications Ltd.",Categorical; Gesture-speech mismatch; Gradient; Homesign; Imagistic; Learning; Morphology; Phonology; Syntax,article; gesture; human; human experiment; learning; morphology; phonetics; scientist; sign language; speech; classification; language development; learning; physiology; speech; Gestures; Humans; Language Development; Learning; Sign Language; Speech,Article,Final,Scopus,2-s2.0-85057631298,Peter,,
"Stone N.J., Chaparro A., Keebler J.R., Chaparro B.S., McConnell D.S.",7202511167;7005806657;25958102700;6603620214;7101708894;,Introduction to human factors: Applying psychology to design,2017,Introduction to Human Factors: Applying Psychology to Design,,,,1,404,,5.0,10.1201/9781315153704,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054227189&doi=10.1201%2f9781315153704&partnerID=40&md5=afb9d09be4f7b2329f0294fbca2ed108,"This is a comprehensive, but accessible text that introduces students to the fields of human factors and ergonomics. The book is intended for undergraduate students, written from the psychological science perspective along with various pedagogical components that will enhance student comprehension and learning. This book is ideal for those introductory courses that wish to introduce students to the multifaceted areas of human factors and ergonomics along with practical knowledge the students can apply in their own lives. © 2018 by Taylor & Francis Group, LLC.",,,Book,Final,Scopus,2-s2.0-85054227189,Peter,,
"Matamala A., Romero-Fresco P., Daniluk L.",25921557700;36675476200;57202040632;,The use of respeaking for the transcription of non-fictional genres an exploratory study,2017,inTRAlinea,19,,,,,,5.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046894668&partnerID=40&md5=5aa90a65daad7264b53798890933e207,"Transcription is not only a useful tool for audiovisual translation, but also a task that is being increasingly performed by translators in different scenarios. This article presents the results of an experiment in which three transcription methods are compared: Manual transcription, respeaking, and revision of a transcript generated by speech recognition. The emphasis is put on respeaking, which is expected to be a useful method to speed up the process of transcription and have a positive impact on the transcribers� experience. Both objective and subjective measures were obtained: On the one hand, the time spent on each task and the output quality based on the NER metrics and on the other, the participants� opinions before and after the task, namely the self-reported effort, boredom, confidence in the accuracy of the transcript and overall quality. � inTRAlinea & A. Matamala, P. Romero-Fresco & L. Daniluk (2017).",Audiovisual translation; Respeaking; Speech recognition; Subtitling; Transcription,,Review,Final,Scopus,2-s2.0-85046894668,Peter,,
"Mendes F., Kožuh I., Debevc M.",57200200999;54412464700;56816724400;,Accessible movies for disabled people,2017,Proceedings of the International Conference on Interfaces and Human Computer Interaction 2017 - Part of the Multi Conference on Computer Science and Information Systems 2017,,,,251,254,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040200812&partnerID=40&md5=673b28afe1f2e545e26a03dea7a66bf8,"Throughout the years disabled people have been facing barriers when watching movies in theaters or at home due to the lack of tools providing sufficient level of accessibility. Accordingly, when it comes to improving accessibility of movies for the deaf and hard-of-hearing, as well as for the blind and the visually-impaired, there has been an important debate on which technological solutions are to be the most appropriate. Solutions, such as audio descriptions and captions, are widely adopted but they represent a higher cost that very few movie theaters want to support. In a different context, sign language interpreter videos are also an alternative, although it has not become widely adopted. Moreover, many websites contain video players with keyboard traps that may ruin the user experience. Nevertheless, recent advances in technology are now enabling the automatic generation of captions for movies as well as the price decrease in hardware parts. Plus, with the adoption of smartphones and mobile applications, new alternatives are appearing at a fraction of the cost (if any) to the consumer. In this paper, we present a broad overview of the topic, its status, and forecast to the future. © 2017.",Accessibility; Disability; Inclusion; Movie; Technology,Audition; Costs; Inclusions; Motion pictures; Technology; Transportation; Vision aids; Accessibility; Automatic Generation; Disability; Level of accessibility; Mobile applications; Movie; Technological solution; Visually impaired; Human computer interaction,Conference Paper,Final,Scopus,2-s2.0-85040200812,Peter,,
"Egusa R., Kawaguchi S., Sakai T., Kusunoki F., Mizoguchi H., Namatame M., Inagaki S.",55212262100;57194835479;57007551700;6701837740;35430648400;22433574600;7202273286;,Development of an automatic location-determining function for balloon-Type dialogue in a puppet show system for the hearing impaired,2017,CSEDU 2017 - Proceedings of the 9th International Conference on Computer Supported Education,2,,,340,344,,1.0,10.5220/0006377203400344,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023181328&doi=10.5220%2f0006377203400344&partnerID=40&md5=ce464af03cb892246457e01452065ef6,"People with hearing impairments have a tendency to experience difficulties in obtaining audio information. They have difficult to watch puppet shows. In this study, we have undertaken the development of a dialogue presentation function in a puppet show system for the hearing impaired. The dialogue presentation function that was developed is an automatic location-determination system for balloon-Type dialogue. The balloontype dialogue turns the lines of dialogue into text and displays it as balloons in the background animation of the puppet show. This function automatically places the balloon-Type dialogue in the vicinity of the locations of the puppets. In the evaluation test, 24 college students with a hearing impairment were used as participants, and a comparison was made between a system that implemented the automatic locationdetermination function for balloon-Type dialogue and a system that did not implement said function. These results indicate the effectiveness of the location tracking function for balloon-Type dialogue as a means for ensuring access to audio information in puppet shows. © 2017 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.",Balloons; Dialogue presentation; People with hearing impairment; Puppet show,Balloons; E-learning; Location; Sandwich structures; Students; Audio information; Automatic location; College students; Dialogue presentation; Evaluation test; Hearing impaired; Hearing impairments; Puppet show; Audition,Conference Paper,Final,Scopus,2-s2.0-85023181328,Peter,,
"Smith C., Allman T., Crocker S.",53876647500;6602299459;57194382788;,Reading between the lines: Accessing information via Youtube’s automatic captioning,2017,Online Learning Journal,21,1,,115,131,,6.0,10.24059/olj.v21i1.823,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019899088&doi=10.24059%2folj.v21i1.823&partnerID=40&md5=65d0bb40cda71cbc81e705e9f02ef677,"This study and discussion center upon the use of YouTube’s automatic captioning feature with college-age adult readers. The study required 75 participants with college experience to view brief middle school science videos with automatic captioning on YouTube and answer comprehension questions based on material presented auditorily and/or through the automatic captions. Participants were divided into groups and presented with the captioned videos with or without sound. The videos, which all focused on the solar system, contained low and high instances of errors within the captions. The research found that comprehension of the automatic caption text varied significantly based on how the participants viewed the videos, with significantly more errors in comprehension for the group that viewed the high error video with automatic captioning only. © 2017, The Online Learning Consortium. All rights reserved.",Access; Accessibility; Captioning; Deaf; Distance education; Hard of hearing; Online learning,,Article,Final,Scopus,2-s2.0-85019899088,Peter,,
Orrego-Carmona D.,57201389752;,A reception study on non-professional subtitling: Do audiences notice any difference?,2016,Across Languages and Cultures,17,2,,163,181,,28.0,10.1556/084.2016.17.2.2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044620582&doi=10.1556%2f084.2016.17.2.2&partnerID=40&md5=7be12753bdf2e4e21ecad2bef15b7373,"The audience's reluctance to wait for the international release of audiovisual products, coupled with the easy access to audiovisual material and subtitling tools on the Internet, has triggered an increase in the production and use of non-professional subtitling. However, up to now, we know little of how people receive the subtitles and how much they understand when watching products with non-professional subtitles. This paper presents the results of a study that explores the audience reception of subtitled TV series using professional and non-professional subtitling. Fifty-two participants were shown three excerpts from The Big Bang Theory with three subtitled versions: the professional version extracted from the Spanish DVD and two non-professional versions produced by two different nonprofessional subtitling communities. Data were collected through questionnaires, eye tracking and interviews. The results show that non-professional subtitles do not necessarily affect audience reception negatively. Further, both eye tracking and self-reported data yielded interesting insights into audience reception. Based on the findings, it is possible to say that there are non-professional translations that are as good as their professional counterparts. © 2016 Akademiai Kiadó, Budapest.",Audience; Eye tracking; Non-professional subtitling; Reception; Subtitling,,Article,Final,Scopus,2-s2.0-85044620582,Peter,,
"Szarkowska A., Krejtz I., Pilipczuk O., Dutka Ł., Kruger J.-L.",54416458200;54395481500;56018914300;57190034357;9277428700;,"The effects of text editing and subtitle presentation rate on the comprehension and reading patterns of interlingual and intralingual subtitles among deaf, hard of hearing and hearing viewers",2016,Across Languages and Cultures,17,2,,183,204,,33.0,10.1556/084.2016.17.2.3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044568621&doi=10.1556%2f084.2016.17.2.3&partnerID=40&md5=a12a9713ca3b9bfa55ec77e8cbc83496,"In this paper we examine the influence of text editing (edited vs. verbatim subtitles) and subtitle presentation rates (12 vs. 15 characters per second) on the comprehension and reading patterns of interlingual and intralingual subtitles among a group of 44 deaf, 33 hard of hearing and 60 hearing Polish adult subjects. The results of the eyetracking study show no benefit of editing down the text of subtitles, particularly in the case of intralingual subtitling and deaf viewers. Verbatim subtitles displayed with the higher presentation rate yielded slightly better comprehension results, were skipped less often, and resulted in more effective reading patterns. Deaf and hard of hearing participants had lower comprehension than hearing people; they also had a higher number of fixations per subtitle and were found to dwell on subtitles longer than the hearing. © 2016 Akadémiai Kiadó, Budapest.",Editing; Presentation rate; Reading speed; Subtitling; Verbatim,,Article,Final,Scopus,2-s2.0-85044568621,Peter,,
Perego E.,36477731700;,"History, development, challenges and opportunities of empirical research in audiovisual translation",2016,Across Languages and Cultures,17,2,,155,162,,11.0,10.1556/084.2016.17.2.1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995579051&doi=10.1556%2f084.2016.17.2.1&partnerID=40&md5=6b378a0cf669ec245e6a87e52ec7e906,"After drawing a brief history of audiovisual translation (AVT), the paper gives a definition of empirical research and it analyzes when, how and why empirical research started to develop and grow systematically in this field of research. The paper also emphasizes the role of empirical research as a tool enabling us to know more about the actual effectiveness of AVT on its audiences as well as to develop awareness of the audience preferences and viewing habits. Consequently, it functions as an important purveyor of knowledge providing a solid basis for shaping quality and tailor made products suiting diverse types of end-users - be them standard or vulnerable users. © 2016 Akadémiai Kiadó, Budapest",Audiovisual translation; Empirical research; History; Interdisciplinarity; User experience,,Article,Final,Scopus,2-s2.0-84995579051,Peter,,
"Tsonos D., Kouroupetroglou G.",23398816700;8652509400;,Prosodic mapping of text font based on the dimensional theory of emotions: a case study on style and size,2016,"Eurasip Journal on Audio, Speech, and Music Processing",2016,1,8,,,,3.0,10.1186/s13636-016-0087-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961122983&doi=10.1186%2fs13636-016-0087-8&partnerID=40&md5=fbf4f85b154daabfaa8112701317b992,"Current text-to-speech systems do not support the effective provision of the semantics and the cognitive aspects of the documents’ typographic cues (e.g., font type, style, and size). A novel approach is introduced for the acoustic rendition of text font based on the emotional analogy between the visual (text font cues) and the acoustic (speech prosody) modalities. The methodology is based on: a) modeling reader’s emotional state response (“Pleasure”, “Arousal” and “Dominance”) induced by the document’s font cues and b) the acoustic mapping of the emotional state using expressive speech synthesis. A case study was conducted for the proposed methodology by calculating the prosodic values on specific font cues (several font styles and font sizes) and by examining listeners’ preferences on the acoustic rendition of bold, italics, bold-italics, and various font sizes. The experimental results after the user evaluation indicate that the acoustic rendition of font size variations as well as bold and italics is recognized successfully, but bold-italics are confused with bold, due to the similarities of their prosodic variations. © 2016, Tsonos and Kouroupetroglou.",Document accessibility; Document-to-audio; Emotions; Expressive speech synthesis; Text signals; Text-to-speech; Typographic cues; Typographic profile,Cognitive systems; Mapping; Semantics; Speech; Speech synthesis; Document accessibility; Document-to-audio; Emotions; Expressive speech synthesis; Text to speech; Typographic cues; Typographic profile; Speech communication,Article,Final,Scopus,2-s2.0-84961122983,Peter,,
"Petry B., Illandara T., Nanayakkara S.",55533933700;57192559312;23009527800;,MuSS-Bits: Sensor-display blocks for deaf people to explore musical sounds,2016,"Proceedings of the 28th Australian Computer-Human Interaction Conference, OzCHI 2016",,,,72,80,,14.0,10.1145/3010915.3010939,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011982173&doi=10.1145%2f3010915.3010939&partnerID=40&md5=3bdb32f81609f750fb6156dbe6abe3dc,"Hearing loss makes learning a musical instrument a challenging task. Prior work suggests that a universal sensory substitution system that works uniformly across all deaf users may not exist given the diversity within the deaf community. In this paper, we present Music Sensory Substitution (MuSS) Bits, wireless sensor-display pairs that enable exploration of musical sound as well as customization of visual and vibrotactile feedback to cater to individual requirements and preferences. MuSS-Bits are portable, easy to deploy on the user's body, on an instrument, or in the environment, and provide real-time feedback. We review existing music sensory substitution systems, discuss the design space for MuSS-Bits, present details of a prototypical implementation and illustrate interaction possibilities including initial user reactions. Copyright © 2016 ACM.",Assistive technologies; Deaf; Learning; Music; Sensory substitution,Audition; Feedback; Interactive computer systems; Assistive technology; Deaf; Learning; Music; Sensory substitution; Human computer interaction,Conference Paper,Final,Scopus,2-s2.0-85011982173,Peter,,
"Armstrong M., Brown A., Crabb M., Hughes C.J., Jones R., Sandford J.",57040522200;57212395814;56421925900;57040431000;56160498000;57206564173;,Understanding the Diverse Needs of Subtitle Users in a Rapidly Evolving Media Landscape,2016,SMPTE Motion Imaging Journal,125,9,7803451,33,41,,5.0,10.5594/JMI.2016.2614919,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009064479&doi=10.5594%2fJMI.2016.2614919&partnerID=40&md5=08072cb5d115b596c91811d0eeeaba97,"Audiences are increasingly using services, such as video on demand and the Web, to watch television programs. Broadcasters need to make subtitles available across all these new platforms. These platforms also create new design opportunities for subtitles along with the ability to customize them to an individual's needs. To explore these new opportunities for subtitles, we have begun the process of reviewing the guidance for subtitles on television and evaluating the original user research. We have found that existing guidelines have been shaped by a mixture of technical constraints, industry practice, and user research, constrained by existing technical standards. This paper provides an overview of the subtitle research at BBC R&D over the past two years. Our research is revealing significant diversity in the needs and preferences of frequent subtitle users, and points to the need for personalization in the way subtitles are displayed. We are developing a new approach to the authoring and display of subtitles that can respond to the user requirements by adjusting the subtitle layout on the client device. © 2002 Society of Motion Picture and Television Engineers, Inc.",Closed captions; diversity; personalization; responsive; subtitles accessibility; television; usability; UX; video; VoD; web,Display devices; Television; Closed captions; diversity; Personalizations; responsive; subtitles accessibility; usability; video; Video on demand,Conference Paper,Final,Scopus,2-s2.0-85009064479,Peter,,
"Wang L., Zhou B., Zhou W., Yang Y.",57059663100;56763164400;57741410000;35486177300;,Odor-induced mood state modulates language comprehension by affecting processing strategies,2016,Scientific Reports,6,,36229,,,,8.0,10.1038/srep36229,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994018402&doi=10.1038%2fsrep36229&partnerID=40&md5=22c1d02ded95c4e54587f19c69863218,"It is controversial whether mood affects cognition by triggering specific processing strategies or by limiting processing resources. The current event-related potential (ERP) study pursued this issue by examining how mood modulates the processing of task relevant/irrelevant information. In question-answer pairs, a question context marked a critical word in the answer sentence as focus (and thus relevant) or non-focus (thereby irrelevant). At the same time, participants were exposed to either a pleasant or unpleasant odor to elicit different mood states. Overall, we observed larger N400s when the critical words in the answer sentences were semantically incongruent (rather than congruent) with the question context. However, such N400 effect was only found for focused words accompanied by a pleasant odor and for both focused and non-focused words accompanied by an unpleasant odor, but not for non-focused words accompanied by a pleasant odor. These results indicate top-down attentional shift to the focused information in a positive mood state and non-selective attention allocated to the focused and non-focused information in a less positive mood state, lending support to the ""processing strategy"" hypothesis. By using a novel approach to induce mood states, our study provides fresh insights into the mechanisms underlying mood modulation of language comprehension. © The Author(s) 2016.",,attention; comprehension; controlled study; exposure; human; human experiment; language; modulation; mood; odor,Article,Final,Scopus,2-s2.0-84994018402,Peter,,
"Kawas S., Karalis G., Wen T., Ladner R.E.",35304872000;57194046396;57192544549;7005099015;,Improving real-Time captioning experiences for deaf and hard of hearing students,2016,ASSETS 2016 - Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility,,,,15,23,,36.0,10.1145/2982142.2982164,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006802374&doi=10.1145%2f2982142.2982164&partnerID=40&md5=2aa249d404e06422da3111bd3e12519a,"We take a qualitative approach to understanding deaf and hard of hearing (DHH) students' experiences with real-Time captioning as an access technology in mainstream university classrooms. We consider both existing human-based captioning as well as new machine-based solutions that use automatic speech recognition (ASR). We employed a variety of qualitative research methods to gather data about students' captioning experiences including in-class observations, interviews, diary studies, and usability evaluations. We also conducted a co-design workshop with 8 stakeholders after our initial research findings. Our results show that accuracy and reliability of the technology are still the most important issues across captioning solutions. However, we additionally found that current captioning solutions tend to limit students' autonomy in the classroom and present a variety of user experience shortcomings, such as complex setups, poor feedback and limited control over caption presentation. Based on these findings, we propose design requirements and recommend features for real-Time captioning in mainstream classrooms.",Automatic speech recognition; Co-design; Deaf and hard of hearing; Inclusive classrooms; Real-Time captions,Audition; Education; Speech recognition; Transportation; Automatic speech recognition; Co-designs; Hard of hearings; Inclusive classrooms; Real time; Students,Conference Paper,Final,Scopus,2-s2.0-85006802374,Peter,,
"Elliot G.L., Stinson M., Mallory J., Easton D., Huenerfauth M.",57192558202;7005000200;57196791067;56106310000;12240800100;,Deaf and hard of hearing individuals' perceptions of communication with hearing colleagues in small,2016,ASSETS 2016 - Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility,,,,271,272,,20.0,10.1145/2982142.2982198,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006743964&doi=10.1145%2f2982142.2982198&partnerID=40&md5=c47ee608284268f5d1eeeee9398345f6,This survey-based study investigated the the perception of deaf and hard of hearing (DHH) individuals' perceived need for technologies that may facilitate communication when meeting in small groups with hearing colleagues. Participants were 108 DHH postsecondary students who participated in co-op (internship) and capstone experiences at workplaces with hearing employees within the past two years. Participants' responses to a survey indicated that they were generally not satisfied with their current strategies and technologies for communicating with hearing persons in small groups.,Communication; Deaf; Hard-of-hearing; Small groups; Survey,Communication; Surveying; Surveys; Transportation; Capstone experience; Deaf; Hard of hearings; Small groups; Audition,Conference Paper,Final,Scopus,2-s2.0-85006743964,Peter,,
"Brandão A., Nicolau H., Tadas S., Hanson V.L.",57188749310;34881822600;57224241807;7006835286;,SlidePacer: A presentation delivery tool for instructors of deaf and hard of hearing students,2016,ASSETS 2016 - Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility,,,,25,32,,8.0,10.1145/2982142.2982177,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006741787&doi=10.1145%2f2982142.2982177&partnerID=40&md5=8bffa48693d8cc929639afc4ca22c390,"Following multimedia lectures in mainstream classrooms is challenging for deaf and hard-of-hearing (DHH) students, even when provided with accessibility services. Due to multiple visual sources of information (e.g. teacher, slides, interpreter), these students struggle to divide their attention among several simultaneous sources, which may result in missing important parts of the lecture; as a result, access to information is limited in comparison to their hearing peers, having a negative effect in their academic achievements. In this paper we propose a novel approach to improve classroom accessibility, which focuses on improving the delivery of multimedia lectures. We introduce SlidePacer, a tool that promotes coordination between instructors and sign language interpreters, creating a single instructional unit and synchronizing verbal and visual information sources. We conducted a user study with 60 participants on the effects of SlidePacer in terms of learning performance and gaze behaviors. Results show that SlidePacer is effective in providing increased access to multimedia information; however, we did not find significant improvements in learning performance. We finish by discussing our results and limitations of our user study, and suggest future research avenues that build on these insights.",Deaf; Interpreter; Learning; Lecture; Multimedia; Pace; Presentation; Visual sources,Audition; Education; Multimedia services; Teaching; Transportation; Visual languages; Deaf; Interpreter; Learning; Lecture; Multimedia; Pace; Presentation; Students,Conference Paper,Final,Scopus,2-s2.0-85006741787,Peter,,
"Zelic G., Varlet M., Kim J., Davis C.",55039088900;36961417300;35740855000;55447671800;,Influence of pacer continuity on continuous and discontinuous visuo-motor synchronisation,2016,Acta Psychologica,169,,,61,70,,10.0,10.1016/j.actpsy.2016.05.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969498382&doi=10.1016%2fj.actpsy.2016.05.008&partnerID=40&md5=8b13a3d6c2cb181144e9cb3e1e6ea736,"Previous research has reported that synchronising movements with an external pacer, known as sensorimotor synchronisation (SMS), is more stable when the movements are discrete/discontinuous rather than continuous. A standard explanation considers that more efficient mechanisms are involved for regulating synchronisation when producing discontinuous movements. To date, however, only discontinuous pacers (e.g., metronomes) have been investigated to compare discontinuous and continuous SMS. We propose an alternative explanation whereby the discontinuous SMS has benefited from the matching between the (dis)continuous nature of the pacer and the (dis)continuous nature of the movements of synchronisation. The present experiment tested this explanation by examining the relative stability of discontinuous and continuous SMS when synchronising with a continuous pacer. Twelve participants finger tapped (discontinuous SMS) or continuously oscillated their forearm (continuous SMS) in synchrony with an oscillatory visual target. The continuity of the pacer was manipulated by varying the kinematic (harmonic to Rayleigh-like oscillations) and the frequency (0.5 and 1 Hz) of the target oscillations. Overall, the results showed a more stable continuous than discontinuous SMS. Furthermore, the stability of the discontinuous SMS improved when increasing the discontinuity of the target displacements (high nonlinear kinematic and low frequency), showing an interaction between movement type and pacer continuity in SMS. © 2016 Elsevier B.V.",Continuity; Finger tapping; Forearm tracking; Regulation of synchronisation; Visuo-motor synchronisation,"adult; attention; biomechanics; female; finger; human; learning; male; movement (physiology); pattern recognition; physiology; psychomotor performance; young adult; Adult; Association Learning; Attention; Biomechanical Phenomena; Female; Fingers; Humans; Male; Movement; Pattern Recognition, Visual; Psychomotor Performance; Young Adult",Article,Final,Scopus,2-s2.0-84969498382,Peter,,
Young R.O.,56409214600;,Persuasive communication: How audiences decide: Second Edition,2016,Persuasive Communication: How Audiences Decide: Second Edition,,,,1,464,,,10.4324/9781315687117,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026348741&doi=10.4324%2f9781315687117&partnerID=40&md5=3ff841ad0b6a51ec31f40e808f3a7d4b,"This updated and expanded edition of Persuasive Communication offers a comprehensive introduction to persuasion and real-world decision making. Drawing on empirical research from social psychology, neuroscience, business communication research, cognitive science, and behavioral economics, Young reveals the thought processes of many different audiences-from investors to CEOs-to help students better understand why audiences make the decisions they make and how to influence them. The book covers a broad range of communication techniques, richly illustrated with compelling examples, including resumes, speeches, and slide presentations, to help students recognize persuasive methods that do, and do not, work. A detailed analysis of the emotions and biases that go into decision making arms students with perceptive insights into human behavior and helps them apply this understanding with various decision-making aids. Students will learn how to impact potential employers, clients, and other audiences essential to their success. This book will prove fascinating to many, and especially useful for students of persuasion, rhetoric, and business communication. © 2017 Richard O. Young. All rights reserved.",,,Book,Final,Scopus,2-s2.0-85026348741,Peter,,
Faniglione D.,57224898049;,Disabled Student Voice on Online Video Lectures: A Small Step towards Blended Learning Approaches in an Inclusive Curriculum Design,2016,International Journal of Learning,2,1,,1,7,,,10.18178/IJLT.2.1.1-07,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108501607&doi=10.18178%2fIJLT.2.1.1-07&partnerID=40&md5=dece50642c87b4da4a500492e340134c,"This paper reports on an ongoing research project at Birmingham City University (BCU), where students with disability are being consulted in order to inform the design and the implementation of an inclusive curriculum design through blended learning approaches. One focus group was conducted with BCU students with disability. Participants were asked to provide opinions on previous experiences with teacher-generated video lectures, the flipped classroom paradigm, video presentation formats and web interface features for online video dissemination. Although the data collected is limited to a very small sample, participants provided valuable feedback, which is informing and redefining inclusive design strategies, with the needs of students with disabilities at their core. The flipped classroom approach was perceived as a welcomed innovation, which could compensate the lack of classroom accessibility (i.e.: difficulty of note taking). An unexpected finding was the perception of teacher-generated video presentations being somehow associated with “less effort” on the teacher's side, and a lower production value when compared with a professionally filmed video presentation. Also, based on past experiences, students were generally sceptical of the ability and will of their teachers to engage in the process of creating video lectures, highlighting the necessity of teaching staff training and support. © 2016 International Journal of Learning and Teaching. All Rights Reserved.",accessibility; blended learning; disability; E-learning; flipped classroom; usability; video lectures,,Article,Final,Scopus,2-s2.0-85108501607,Peter,,
"Szarkowska A., Krejtz K., Dutka L., Pilipczuk O.",54416458200;55258716700;57190034357;56018914300;,Cognitive load in intralingual and interlingual respeaking-a preliminary study,2016,Poznan Studies in Contemporary Linguistics,52,2,,209,233,,19.0,10.1515/psicl-2016-0008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976604564&doi=10.1515%2fpsicl-2016-0008&partnerID=40&md5=cb239393b1535c4e78cd3757bd16bebf,"In this paper we present preliminary results of the study on the cognitive load in intralingual and interlingual respeaking. We tested 57 subjects from three groups: interpreters, translators and controls while respeaking 5-minute videos in two language combinations: Polish to Polish (intralingual) and English to Polish (interlingual). Using two measures of cognitive load: self-report and EEG (Emotiv), we found that in most cases cognitive load was higher in interlingual respeaking. Self-reported mental effort that the participants had to expend to complete the respeaking tasks was lower in the group of interpreters, suggesting some parallels between interpreting and respeaking competences. EEG measures showed significant differences between respeaking tasks and experimental groups in cognitive load over time. © 2016 Faculty of English, Adam Mickiewicz University, Poznań, Poland 2016.",audiovisual translation; cognitive load; interpreting; live subtitling; Respeaking,,Article,Final,Scopus,2-s2.0-84976604564,Peter,,
"Gaur Y., Lasecki W.S., Metze F., Bigham J.P.",55584294900;54383728900;6505996325;16238221500;,The effects of automatic speech recognition quality on human transcription latency,2016,W4A 2016 - 13th Web for All Conference,,,a23,,,,21.0,10.1145/2899475.2899478,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983541308&doi=10.1145%2f2899475.2899478&partnerID=40&md5=d9926f6f77b1905d1c01e8e66872a362,"Transcription makes speech accessible to deaf and hard of hearing people. This conversion of speech to text is still done manually by humans, despite high cost, because the quality of automated speech recognition (ASR) is still too low in real-world settings. Manual conversion can require more than 5 times the original audio time, which also introduces significant latency. Giving transcriptionists ASR output as a starting point seems like a reasonable approach to making humans more efficient and thereby reducing this cost, but the effectiveness of this approach is clearly related to the quality of the speech recognition output. At high error rates, fixing inaccurate speech recognition output may take longer than producing the transcription from scratch, and transcriptionists may not realize when transcription output is too inaccurate to be useful. In this paper, we empirically explore how the latency of transcriptions created by participants recruited on Amazon Mechanical Turk vary based on the accuracy of speech recognition output. We present results from 2 studies which indicate that starting with the ASR output is worse unless it is sufficiently accurate (Word Error Rate of under 30%). © 2016 ACM.",Automatic speech recognition; Captioning; Crowd programming; Human computation,Audition; Character recognition; Speech; Transcription; Amazon mechanical turks; Automated speech recognition; Automatic speech recognition; Captioning; Hard of hearings; Human computation; Real world setting; Word error rate; Speech recognition,Conference Paper,Final,Scopus,2-s2.0-84983541308,Peter,,
"Krejtz I., Szarkowska A., Łogińska M.",54395481500;54416458200;57188841449;,Reading function and content words in subtitled videos,2016,Journal of Deaf Studies and Deaf Education,21,2,enw002,222,232,,17.0,10.1093/deafed/env061,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963719899&doi=10.1093%2fdeafed%2fenv061&partnerID=40&md5=c0a0c28217f17eb8d760c7f2e4469c3e,"In this study, we examined how function and content words are read in intra- and interlingual subtitles. We monitored eye movements of a group of 39 deaf, 27 hard of hearing, and 56 hearing Polish participants while they viewed English and Polish videos with Polish subtitles. We found that function words and short content words received less visual attention than longer content words, which was reflected in shorter dwell time, lower number of fixations, shorter first fixation duration, and lower subject hit count. Deaf participants dwelled significantly longer on function words than other participants, which may be an indication of their difficulty in processing this type of words. The findings are discussed in the context of classical reading research and applied research on subtitling. © The Author 2015. Published by Oxford University Press.",,adolescent; adult; aged; attention; case control study; eye movement; female; Hearing Loss; human; linguistics; male; middle aged; physiology; Poland; reading; time factor; videorecording; young adult; Adolescent; Adult; Aged; Attention; Case-Control Studies; Eye Movements; Female; Hearing Loss; Humans; Male; Middle Aged; Poland; Reading; Time Factors; Videotape Recording; Vocabulary; Young Adult,Article,Final,Scopus,2-s2.0-84963719899,Peter,,
"Krull V., Humes L.E.",24344206300;7004438599;,Text as a Supplement to Speech in Young and Older Adults,2016,Ear and Hearing,37,2,,164,176,,10.0,10.1097/AUD.0000000000000234,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960348334&doi=10.1097%2fAUD.0000000000000234&partnerID=40&md5=c75b86e4f8d377f40b18dcbe1cf14511,"Objective: The purpose of this experiment was to quantify the contribution of visual text to auditory speech recognition in background noise. Specifically, the authors tested the hypothesis that partially accurate visual text from an automatic speech recognizer could be used successfully to supplement speech understanding in difficult listening conditions in older adults, with normal or impaired hearing. The working hypotheses were based on what is known regarding audiovisual speech perception in the elderly from speechreading literature. We hypothesized that (1) combining auditory and visual text information will result in improved recognition accuracy compared with auditory or visual text information alone, (2) benefit from supplementing speech with visual text (auditory and visual enhancement) in young adults will be greater than that in older adults, and (3) individual differences in performance on perceptual measures would be associated with cognitive abilities. Design: Fifteen young adults with normal hearing, 15 older adults with normal hearing, and 15 older adults with hearing loss participated in this study. All participants completed sentence recognition tasks in auditory-only, text-only, and combined auditory-text conditions. The auditory sentence stimuli were spectrally shaped to restore audibility for the older participants with impaired hearing. All participants also completed various cognitive measures, including measures of working memory, processing speed, verbal comprehension, perceptual and cognitive speed, processing efficiency, inhibition, and the ability to form wholes from parts. Group effects were examined for each of the perceptual and cognitive measures. Audiovisual benefit was calculated relative to performance on auditory-and visual-text only conditions. Finally, the relationship between perceptual measures and other independent measures were examined using principal-component factor analyses, followed by regression analyses. Results: Both young and older adults performed similarly on 9 out of 10 perceptual measures (auditory, visual, and combined measures). Combining degraded speech with partially correct text from an automatic speech recognizer improved the understanding of speech in both young and older adults, relative to both auditory-and text-only performance. In all subjects, cognition emerged as a key predictor for a general speech-text integration ability. Conclusions: These results suggest that neither age nor hearing loss affected the ability of subjects to benefit from text when used to support speech, after ensuring audibility through spectral shaping. These results also suggest that the benefit obtained by supplementing auditory input with partially accurate text is modulated by cognitive ability, specifically lexical and verbal skills. Copyright © 2015 Wolters Kluwer Health, Inc.",Aging; Auditory enhancement; Automatic speech recognition; Cognition; Hearing loss; Speech understanding; Visual enhancement; Visual text,"adolescent; age; aged; auditory stimulation; automatic speech recognition; case control study; comprehension; female; human; lip reading; male; middle aged; noise; pathophysiology; perception deafness; photostimulation; physiology; reading; short term memory; speech perception; very elderly; vision; young adult; Acoustic Stimulation; Adolescent; Age Factors; Aged; Aged, 80 and over; Case-Control Studies; Comprehension; Female; Hearing Loss, Sensorineural; Humans; Lipreading; Male; Memory, Short-Term; Middle Aged; Noise; Photic Stimulation; Reading; Speech Perception; Speech Recognition Software; Visual Perception; Young Adult",Article,Final,Scopus,2-s2.0-84960348334,Peter,,
"Luo Y., Yan M., Yan S., Zhou X., Inhoff A.W.",55836791100;56420162000;57688818400;36116357700;7006594891;,Syllabic tone articulation influences the identification and use of words during Chinese sentence reading: Evidence from ERP and eye movement recordings,2016,"Cognitive, Affective and Behavioral Neuroscience",16,1,,72,92,,7.0,10.3758/s13415-015-0368-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958748374&doi=10.3758%2fs13415-015-0368-1&partnerID=40&md5=6bd0bb8292c61ab8eed491e2ce89df26,"In two experiments, we examined the contribution of articulation-specific features to visual word recognition during the reading of Chinese. In spoken Standard Chinese, a syllable with a full tone can be tone-neutralized through sound weakening and pitch contour change, and there are two types of two-character compound words with respect to their articulation variation. One type requires articulation of a full tone for each constituent character, and the other requires a full- and a neutral-tone articulation for the first and second characters, respectively. Words of these two types with identical first characters were selected and embedded in sentences. Native speakers of Standard Chinese were recruited to read the sentences. In Experiment 1, the individual words of a sentence were presented serially at a fixed pace while event-related potentials were recorded. This resulted in less-negative N100 and anterior N250 amplitudes and in more-negative N400 amplitudes when targets contained a neutral tone. Complete sentences were visible in Experiment 2, and eye movements were recorded while participants read. Analyses of oculomotor activity revealed shorter viewing durations and fewer refixations on—and fewer regressive saccades to—target words when their second syllable was articulated with a neutral rather than a full tone. Together, the results indicate that readers represent articulation-specific word properties, that these representations are routinely activated early during the silent reading of Chinese sentences, and that the representations are also used during later stages of word processing. © 2015, Psychonomic Society, Inc.",Articulation duration; Chinese; Lexical tone; Neutral tone; Sentence reading; Syllabic tone,"adult; Asian continental ancestry group; evoked response; eye movement; female; human; male; pattern recognition; photostimulation; physiology; procedures; reading; semantics; young adult; Adult; Asian Continental Ancestry Group; Evoked Potentials; Eye Movements; Female; Humans; Male; Pattern Recognition, Visual; Photic Stimulation; Reading; Semantics; Young Adult",Article,Final,Scopus,2-s2.0-84958748374,Peter,,
Mangiron C.,55587445400;,Reception of game subtitles: An empirical study,2016,Translator,22,1,,72,93,,21.0,10.1080/13556509.2015.1110000,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963718953&doi=10.1080%2f13556509.2015.1110000&partnerID=40&md5=8f3d6f806b7e15a06a78cce394d2b6e8,"Subtitling practices in game localisation remain, to date, largely unexplored, and the existing standards as widely applied to subtitling for TV, DVD and cinema have not been adopted by the game industry. This can pose an accessibility barrier for deaf and hard of hearing players as there are, on occasion, no subtitles provided for audio game components, such as sound effects. This article presents a small-scale exploratory study concerning the reception of subtitles in video games by means of user tests with eye tracking technology and a questionnaire. The purpose of the study was to determine what type of subtitles would be most suitable for video games, given their interactive and ludic nature, based not only on users' preferences but also on quantitative data obtained with an eye tracking technology. The article also highlights the need for the development of best practice and standards in subtitling for this emerging digital medium, which would enhance game accessibility and the gaming experience not only for deaf and hard of hearing players but for all players. © 2016 Taylor & Francis.",Accessibility; Eye tracking; Subtitle reception; Subtitles for the deaf and hard of hearing; Video game subtitles,,Article,Final,Scopus,2-s2.0-84963718953,Peter,,
"Youngblood N.E., Lysaght R.",36994960000;57404622600;,Accessibility and Use of Online Video Captions by Local Television News Websites,2015,Electronic News,9,4,,242,256,,3.0,10.1177/1931243115604885,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053391841&doi=10.1177%2f1931243115604885&partnerID=40&md5=9e31b9801ae1cf8ec0df608850eb4007,"In July 2014, the Federal Communications Commission expanded broadcaster requirements for captioning online videos. The new guidelines, which go into effect in January 2016, require broadcasters to caption online clips from shows originally aired with captions. We conducted a content analysis and examined a stratified sample of one fifth of U.S. Designated Market Areas to see how prepared local TV news sites are for the rule changes and how accessible the sites are for users with disabilities. Barely 60% of sites captioned any of the sampled video clips and almost all sites had major accessibility issues. © The Author(s) 2015.",accessibility; content analysis; digital divide; FCC regulation; online news,,Article,Final,Scopus,2-s2.0-85053391841,Peter,,
"Wölfel M., Schlippe T., Stitz A.",8870295600;42062203500;57170844900;,Voice driven type design,2015,"2015 International Conference on Speech Technology and Human-Computer Dialogue, SpeD 2015",,,7343095,,,,6.0,10.1109/SPED.2015.7343095,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960854904&doi=10.1109%2fSPED.2015.7343095&partnerID=40&md5=5a75d749691326fc8cbe23d79447f48f,"With voice driven type design (VDTD), we introduce a novel concept to present written information in the digital age. While the shape of a single typographical character has been treated as an unchangeable property until today, we present an innovative method to adjust the shape of each single character according to particular acoustic features in the spoken reference. Thereby, we allow to keep some individuality and to gain additional value in written text which offers different applications - providing meta-information in subtitles and chats, supporting deaf and hearing impaired people, illustrating intonation and accentuation in books for language learners, giving hints how to sing - up to artistic expression. By conducting a user study we have demonstrated that - using our proposed approach - loudness, pitch and speed can be represented visually by changing the shape of each character. By complementing homogeneous type design with these parameters, the original intention and characteristics of the speaker (personal expression and intonation) are better supported. © 2015 IEEE.",adaptive character shape; responsive type; speech analyzis; speech representation; type design; typography,Design; Human computer interaction; Acoustic features; adaptive character shape; Deaf and hearing impaired; Innovative method; responsive type; Type designs; typography; Written information; Audition,Conference Paper,Final,Scopus,2-s2.0-84960854904,Peter,,
"Prietch S.S., De Souza N.S., Filgueiras L.V.L.",55925755100;56237719100;23392315900;,Application requirements for deaf students to use in inclusive classrooms,2015,"Proceedings of the 7th Latin American Conference on Human Computer Interaction, CLIHC 2015",,,2824898,,,,5.0,10.1145/2824893.2824898,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979688358&doi=10.1145%2f2824893.2824898&partnerID=40&md5=d10db7553a12982afb5658095a8f0b03,"The goal was to search for an alternative to mediate communication between people who are Deaf or Hard of Hearing (D/HH) and hearing individuals, in which the major motivation was to provide empowerment for D/HH students, using technology to provide them with more autonomy in inclusive classrooms. With that in mind, a systematic literature review was conducted to elucidate requirements for a mobile application that includes a Speech-To-Text (STT) system, as well as an opinion research accomplished with D/HH participants to know potential users' priorities among the elucidated requirements. As a result, the preference to communicate in Libras (Brazilian Sing Language) prevails, although users consider important to use a mobile application as the one proposed. Copyright 2015 ACM.",Automatic speech recognition; Deaf or hard of hearing students; Mobile devices; User interface design,Audition; Mobile computing; Mobile devices; Mobile telecommunication systems; Speech recognition; Students; User interfaces; Application requirements; Automatic speech recognition; Deaf students; Hard of hearings; Mobile applications; Potential users; Systematic literature review; User interface designs; Human computer interaction,Conference Paper,Final,Scopus,2-s2.0-84979688358,Peter,,
"Cardoso M.E.D.A., Trindade D.D.F.G., Neitzel R.A.L.D.S.",56964443700;57190673585;56964340600;,The accessibility in Web sites in the optical of including deaf [A acessibilidade em web sites na ótica da inclusão dos surdos],2015,ACM International Conference Proceeding Series,,,a46,,,,,10.1145/3148456.3148502,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051532397&doi=10.1145%2f3148456.3148502&partnerID=40&md5=cc6cda67f1796012be5a92d61f2ad245,"The Web information technology promotes many facilities to people. It allows them to gain more time in performing transactions, payments, purchases and access to information. However, this type of technology has still been far from the deaf community reality, because of the lack of accessibility provided by these computer systems. In this context, this paper investigates the needs of the deaf community for access to Web sites with support for evaluation mechanisms. Initially, the accessibility guidelines for the deaf were identified using the view of several authors. Afterwards, the accessibility evaluation mechanisms were used to analyze some Web Site News. Thus, it was possible to highlight the positive and negative accessibility issues found in Web Sites. © 2015 Association for Computing Machinery.",Accessibility; Accessibility evaluation; Deaf community web sites,Human engineering; Accessibility; Accessibility evaluation; Accessibility guidelines; Type of technology; Websites,Conference Paper,Final,Scopus,2-s2.0-85051532397,Peter,,
"Crabb M., Jones R., Armstrong M., Hughes C.J.",56421925900;56160498000;57040522200;57040431000;,Online news videos: The UX of subtitle position,2015,ASSETS 2015 - Proceedings of the 17th International ACM SIGACCESS Conference on Computers and Accessibility,,,,215,222,,24.0,10.1145/2700648.2809866,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962705490&doi=10.1145%2f2700648.2809866&partnerID=40&md5=aab7c35f4c478788e953236d6233b52c,"Millions of people rely on subtitles when watching video content. The current change in media viewing behaviour involving computers has resulted in a large proportion of people turning to online sources as opposed to regular television for news information. This work analyses the user experience of viewing subtitled news videos presented as part of a web page. A lab-based user experiment was carried out with frequent subtitle users, focusing on determining whether changes in video dimension and subtitle location could affect the user experience attached to viewing subtitled content. A significant improvement in user experience was seen when changing the subtitle location from the standard position of within a video at the bottom to below the video clip. Additionally, participants responded positively when given the ability to change the position of subtitles in real time, allowing for a more personalised viewing experience. This recommendation for an alternative subtitle positioning that can be controlled by the user is unlike current subtitling practice. It provides evidence that further user-based research examining subtitle usage outside of the traditional television interface is required. © 2015 ACM.",Experiment methodology; Laboratory experiment; Subtitles; User experience,Transportation; Current change; Laboratory experiments; News information; Online sources; Subtitles; User experience; User experiments; Video contents; Websites,Conference Paper,Final,Scopus,2-s2.0-84962705490,Peter,,
Gaur Y.,55584294900;,The effects of automatic speech recognition quality on human transcription latency,2015,ASSETS 2015 - Proceedings of the 17th International ACM SIGACCESS Conference on Computers and Accessibility,,,,367,368,,6.0,10.1145/2700648.2811331,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962660454&doi=10.1145%2f2700648.2811331&partnerID=40&md5=8b1a31ec211f78d2311ed5913c9bee66,"Converting speech to text quickly is the fundamental task for making aural content accessible to deaf and hard of hearing. Despite high cost, this is done by human captionists, as automatic speech recognition (ASR) does not give satisfactory performance in real world settings. Offering ASR output to captionists as a starting point seems more facile and economical, yet the effectiveness of this approach is clearly dependent on the quality of ASR because fixing inaccurate ASR output may take longer than producing the transcriptions without ASR support. In this paper, we empirically study how the time required by captionists to produce transcriptions from partially correct ASR output varies based on the accuracy of the ASR output. Our studies with 160 participants recruited on Amazon's Mechanical Turk indicate that starting with the ASR output is worse unless it is sufficiently accurate (Word Error Rate (WER) is under 30%). © 2015 ACM.",,Audition; Transcription; Transportation; Amazon's mechanical turks; Automatic speech recognition; Hard of hearings; High costs; Real world setting; Word error rate; Speech recognition,Conference Paper,Final,Scopus,2-s2.0-84962660454,Peter,,
"Shiver B., Wolfe R.",54403867400;7402000426;,Evaluating alternatives for better deaf accessibility to selected web-based multimedia,2015,ASSETS 2015 - Proceedings of the 17th International ACM SIGACCESS Conference on Computers and Accessibility,,,,231,238,,19.0,10.1145/2700648.2809857,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962660242&doi=10.1145%2f2700648.2809857&partnerID=40&md5=eb7fc11b85ca865dc9a719a1ea2e74d5,"The proliferation of video and audio media on the Internet has created a distinct disadvantage for deaf Internet users. Despite technological and legislative milestones in recent decades in making television and movies more accessible, there has been less progress with online access. A major obstacle to providing captions for Internet media is the high cost of captioning and transcribing services. This paper reports on two studies that focused on multimedia accessibility for Internet users who were born deaf or became deaf at an early age. An initial study attempted to identify priorities for deaf accessibility improvement. A total of 20 deaf and hard-of-hearing participants were interviewed via videophone about their Internet usage and the issues that were the most frustrating. The most common theme was concern over a lack of accessibility for online news. In the second study, a total of 95 deaf and hard-of-hearing participants evaluated different caption styles, some of which were generated through automatic speech recognition. Results from the second study confirm that captioning online videos makes the Internet more accessible to the deaf users, even when the captions are automatically generated. However color-coded captions used to highlight confidence levels were found neither to be beneficial nor detrimental; yet when asked directly about the benefit of color-coding, participants strongly favored the concept. © 2015 ACM.",Automatic speech recognition; Captioning; Deaf; Multimedia accessibility; Speech-to-text; Web accessibility,Audition; Character recognition; Internet; Transportation; Usability engineering; Websites; World Wide Web; Automatic speech recognition; Captioning; Deaf; Multimedia accessibility; Web accessibility; Speech recognition,Conference Paper,Final,Scopus,2-s2.0-84962660242,Peter,,
"Pascual A., Ribera M., Granollers T.",35186398400;23670757400;56962781000;,Impact of web accessibility barriers on users with a hearing impairment [Impacto de las barreras de accesibilidad web en usuarios con discapacidad auditiva],2015,DYNA (Colombia),82,193,,233,240,,8.0,10.15446/dyna.v82n193.53499,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944459089&doi=10.15446%2fdyna.v82n193.53499&partnerID=40&md5=df0a62bff0acde5f826198c872cd7dad,"Several user tests were carried out on people with a hearing impairment to evaluate the impact of different web accessibility barriers on two similar web sites, one accessible and the other not accessible. The tests’ focus was to analyze users’ moods when faced with different accessibility barriers. Results show “complex text” and “multimedia content without text alternative” as the most critical barriers for users with this profile. Our investigation contributes to a better understanding of users when confronting accessibility barriers, and to emphasize the need of web content authors to use plain language and to provide captions and sign language alternatives in video content. © The author; licensee Universidad Nacional de Colombia.",Accessible content; Barriers; Communicability; Deaf users; Hearing impairment; User test; WCAG 2.0; Web accessibility,,Article,Final,Scopus,2-s2.0-84944459089,Peter,,
"Evans L., Wu Y., Price E.",57117152200;57189852753;55876587200;,A new hope? Experiences of accessibility of services in deaf and hard-of-hearing audiences post-digital television switchover,2015,International Journal of Digital Television,6,3,,347,366,,1.0,10.1386/jdtv.6.3.347_1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975299327&doi=10.1386%2fjdtv.6.3.347_1&partnerID=40&md5=5c3c579232a38d6c4913b44afdc1119d,"One promise of the switchover from analogue to digital television was new accessibility solutions. In the case of deaf and hard-of-hearing audiences who rely on subtitling for comprehension, the digital switchover makes it possible for greater provision of subtitling or improvements in accuracy. Utilizing quantitative data from a questionnaire completed by 339 participants with varying degrees of hearing difficulty in Wales, this article assesses perceptions of subtitling pre-and postdigital switch. A within-group comparison across age groups is also used to assess whether improvements in service are age defined. The results found that difficulties in accessing quality subtitling are still experienced by this audience post-digital switchover and that there are no significant differences in age in experiencing these difficulties. Knowledge of other digital services is subject to a significant difference in age, which indicates more work is required to inform older audience members of the affordances of digital television. © 2015 Intellect Ltd Reviews.",Affordances; Barriers; Deaf and hard-of hearing audiences; Digital television; Interactive services; Subtitling; Wales,,Article,Final,Scopus,2-s2.0-84975299327,Peter,,
"Wang F., Nagano H., Kashino K., Igarashi T.",56103888600;7202707871;6701924954;7403250321;,Visualizing video sounds with sound word animation,2015,Proceedings - IEEE International Conference on Multimedia and Expo,2015-August,,7177422,,,,3.0,10.1109/ICME.2015.7177422,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946024923&doi=10.1109%2fICME.2015.7177422&partnerID=40&md5=2ca79c7aac3b1f586c0ea7a58616a9f7,"Text captions are important means to provide sound information in videos when the sound is not accessible. However, conventional text captions are far less expressive for non-verbal sounds since they are designed to visualize speech sound. To address this problem, we propose a method for automatically transforming non-verbal video sounds to animated sound words, and positioning them near the sound source objects in the video for visualization. This provides natural visual representation of non-verbal sounds with rich information about the sound category and dynamics. We conducted a user study with over 300 participants using an online crowdsourcing service. The results showed that animated sound words could not only effectively and naturally visualize the dynamics of sound while clarify the position of the sound source, but also contribute to making video watching more enjoyable and increasing the visual impact of the video. © 2015 IEEE.",entertainment; environmental sound processing; sound visualization; Sound word; video annotation,Acoustic generators; Entertainment; Environmental sounds; Sound source; Sound visualization; Speech sounds; Video annotations; Visual impacts; Visual representations; Visualizing video; Visualization,Conference Paper,Final,Scopus,2-s2.0-84946024923,Peter,,
"Kushalnagar P., Naturale J., Paludneviciene R., Smith S.R., Werfel E., Doolittle R., Jacobs S., DeCaro J.",17135146200;56214961600;56209867600;57190853503;56214898700;7102622974;7402313068;6603549343;,Health Websites: Accessibility and Usability for American Sign Language Users,2015,Health Communication,30,8,,830,837,,17.0,10.1080/10410236.2013.853226,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928646883&doi=10.1080%2f10410236.2013.853226&partnerID=40&md5=801430222b4e0967c496f3d114fe2c7e,"To date, there have been efforts toward creating better health information access for Deaf American Sign Language (ASL) users. However, the usability of websites with access to health information in ASL has not been evaluated. Our article focuses on the usability of four health websites that include ASL videos. We seek to obtain ASL users’ perspectives on the navigation of these ASL-accessible websites, finding the health information that they needed, and perceived ease of understanding ASL video content. ASL users (n = 32) were instructed to find specific information on four ASL-accessible websites, and answered questions related to (a) navigation to find the task, (b) website usability, and (c) ease of understanding ASL video content for each of the four websites. Participants also gave feedback on what they would like to see in an ASL health library website, including the benefit of added captioning and/or signer model to medical illustration of health videos. Participants who had lower health literacy had greater difficulty in finding information on ASL-accessible health websites. This article also describes the participants’ preferences for an ideal ASL-accessible health website, and concludes with a discussion on the role of accessible websites in promoting health literacy in ASL users. © , Copyright © Taylor & Francis Group, LLC.",,adolescent; adult; female; health literacy; hearing impaired person; human; Internet; male; psychology; sign language; statistics and numerical data; United States; videorecording; young adult; Adolescent; Adult; Female; Health Literacy; Humans; Internet; Male; Persons With Hearing Impairments; Sign Language; United States; Videotape Recording; Young Adult,Article,Final,Scopus,2-s2.0-84928646883,Peter,,
Seidel N.,54791369600;,Making web video accessible - interaction design patterns for assistive video learning environments,2015,ACM International Conference Proceeding Series,08-12-July-2015,,a17,,,,3.0,10.1145/2855321.2855339,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982856615&doi=10.1145%2f2855321.2855339&partnerID=40&md5=546eae567e1fb581a6fbfc53689fc720,"This paper discusses four interaction design patterns that constitute assistive solutions to enhance the accessibility of audible and visual information inside learning environments. These patterns result from a content analysis of 118 video environments that were designated for learning. The analysis yields a pattern language of currently 40 patterns that describes common solutions of recurring problems in the design and development of video learning environments. Beside the complete representation of the accessibility patterns CLOSED CAPTIONS, TRANSCRIPT, ZOOM, and SHORTCUT COMMANDS, this paper draws attention to possible ways of structuring the pattern language.",Accessibility; Hypervideo; Interaction design patterns; Interactive video,Computational linguistics; Design; Multimedia systems; Transportation; Accessibility; Design and Development; Hypervideo; Interaction design patterns; Interactive video; Learning environments; Pattern languages; Visual information; Computer aided instruction,Conference Paper,Final,Scopus,2-s2.0-84982856615,Peter,,
"Lazar J., Goldstein D., Taylor A.",7007005493;57213580755;7405887933;,Ensuring Digital Accessibility through Process and Policy,2015,Ensuring Digital Accessibility through Process and Policy,,,,1,230,,77.0,10.1016/C2013-0-13367-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943402798&doi=10.1016%2fC2013-0-13367-3&partnerID=40&md5=6a1f31d98b70b98a300664dc0f732b6e,"Ensuring Digital Accessibility through Process and Policy provides readers with a must-have resource to digital accessibility from both a technical and policy perspective. Inaccessible digital interfaces and content often lead to forms of societal discrimination that may be illegal under various laws. This book is unique in that it provides a multi-disciplinary understanding of digital accessibility. The book discusses the history of accessible computing, an understanding of why digital accessibility is socially and legally important, and provides both technical details (interface standards, evaluation methods) and legal details (laws, lawsuits, and regulations). The book provides real-world examples throughout, highlighting organizations that are doing an effective job with providing equal access to digital information for people with disabilities. This isn't a book strictly about interface design, nor is it a book strictly about law. For people who are charged with implementing accessible technology and content, this book will serve as a one-stop guide to understanding digital accessibility, offering an overview of current laws, regulations, technical standards, evaluation techniques, as well as best practices and suggestions for implementing solutions and monitoring for compliance. This combination of skills from the three authors-law, technical, and research, with experience in both corporate, government, and educational settings, is unique to this book, and does not exist in any other book about any aspect of IT accessibility. The authors' combination of skills marks a unique and valuable perspective, and provides insider knowledge on current best practices, corporate policies, and technical instructions. Together, we can ensure that the world of digital information is open to all users. © 2015 Elsevier Inc. All rights reserved.",,,Book,Final,Scopus,2-s2.0-84943402798,Peter,,
"Brown A., Jones R., Crabb M.",57212395814;56160498000;56421925900;,"Dynamic subtitles: The user experience, andy brown",2015,TVX 2015 - Proceedings of the ACM International Conference on Interactive Experiences for TV and Online Video,,,,103,112,,33.0,10.1145/2745197.2745204,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960854048&doi=10.1145%2f2745197.2745204&partnerID=40&md5=82163345f72ba5798ae3b7712609ebfa,"Subtitles (closed captions) on television are typically placed at the bottom-centre of the screen. However, placing subtitles in varying positions, according to the underlying video content (dynamic subtitles'), has the potential to make the overall viewing experience less dis- jointed and more immersive. This paper describes the testing of such subtitles with hearing-impaired users, and a new analysis of previously collected eye-tracking data. The qualitative data demonstrates that dynamic subtitles can lead to an improved User Experience, although not for all types of subtitle user. The eye-tracking data was analysed to compare the gaze patterns of subtitle users with a baseline of those for people viewing with- out subtitles. It was found that gaze patterns of people watching dynamic subtitles were closer to the baseline than those of people watching with traditional subtitles. Finally, some of the factors that need to be considered when authoring dynamic subtitles are discussed.",Accessibility; Attention approximation; Eye-tracking; HCI; Subtitles; TV; User experience,Audition; Human computer interaction; Target tracking; Television; Accessibility; Attention approximation; Eye-tracking; Subtitles; User experience; Interactive television,Conference Paper,Final,Scopus,2-s2.0-84960854048,Peter,,
"Heimler B., van Zoest W., Baruffaldi F., Donk M., Rinaldi P., Caselli M.C., Pavani F.",56014325800;57203612705;55802538400;6701580286;57694516400;7007060153;6603904412;,Finding the balance between capture and control: Oculomotor selection in early deaf adults,2015,Brain and Cognition,96,,,12,27,,16.0,10.1016/j.bandc.2015.03.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925868414&doi=10.1016%2fj.bandc.2015.03.001&partnerID=40&md5=c934353b5ac31537056db1d8c9dc6ba7,"Previous work investigating the consequence of bilateral deafness on attentional selection suggests that experience-dependent changes in this population may result in increased automatic processing of stimulus-driven visual information (e.g., saliency). However, adaptive behavior also requires observers to prioritize goal-driven information relevant to the task at hand. In order to investigate whether auditory deprivation alters the balance between these two components of attentional selection, we assessed the time-course of overt visual selection in deaf adults. Twenty early-deaf adults and twenty hearing controls performed an oculomotor additional singleton paradigm. Participants made a speeded eye-movement to a unique orientation target, embedded among homogenous non-targets and one additional unique orientation distractor that was more, equally or less salient than the target. Saliency was manipulated through color. For deaf participants proficiency in sign language was assessed. Overall, results showed that fast initiated saccades were saliency-driven, whereas later initiated saccades were goal-driven. However, deaf participants were overall slower than hearing controls at initiating saccades and also less captured by task-irrelevant salient distractors. The delayed oculomotor behavior of deaf adults was not explained by any of the linguistic measures acquired. Importantly, a multinomial model applied to the data revealed a comparable evolution over time of the underlying saliency- and goal-driven processes between the two groups, confirming the crucial role of saccadic latencies in determining the outcome of visual selection performance. The present findings indicate that prioritization of saliency-driven information is not an unavoidable phenomenon in deafness. Possible neural correlates of the documented behavioral effect are also discussed. © 2015 Elsevier Inc.",Early-deafness; Goal-driven; Oculomotor capture; Overt visual selection; Plasticity; Stimulus-driven,"adult; Article; clinical article; controlled study; disease severity; eye movement control; female; hearing impairment; human; latent period; male; priority journal; saccadic eye movement; sign language; task performance; visual adaptation; visual feedback; visual information; visual orientation; visual stimulation; young adult; attention; eye movement; hearing impairment; motivation; pathophysiology; pattern recognition; physiology; psychological model; psychomotor performance; Adult; Attention; Deafness; Eye Movements; Female; Goals; Humans; Male; Models, Psychological; Pattern Recognition, Visual; Psychomotor Performance; Saccades; Young Adult",Article,Final,Scopus,2-s2.0-84925868414,Peter,,
"Macoir J., Laforce R., Brisson M., Wilson M.A.",6603133532;6603914161;16480215900;54950694500;,Preservation of lexical-semantic knowledge of adjectives in the semantic variant of primary progressive aphasia: Implications for theoretical models of semantic memory,2015,Journal of Neurolinguistics,34,,,1,14,,4.0,10.1016/j.jneuroling.2014.11.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916233263&doi=10.1016%2fj.jneuroling.2014.11.003&partnerID=40&md5=94214b87ae59146446bab7766f354ce8,"Adjectives are lexical words used to modify nouns and refer to physical and concrete properties as well as abstract properties denoted by nouns. Although adjectives directly refer to semantic attributes, no study has explicitly examined the comprehension of adjectives in the semantic variant of primary progressive aphasia (svPPA). In this study, we explored the comprehension of adjectival meaning in four participants with svPPA using two experimental designs assessing the comprehension of adjectives per se and the comprehension of adjectives when associated with concrete nouns. We found evidence that participants: a) were unimpaired on tasks probing knowledge about adjectives referring to dimension and physical properties, to human propensity and value, and to color, but b) showed substantial difficulty when asked to associate the same adjectives with object nouns. These results raise important questions about the lexical-semantic representation of adjectives and, more generally, about the organization of words in the lexical-semantic system. The observed difficulties could be due to the impairment in binding different modality-specific properties and combining verbal conceptual knowledge, after anterior temporal lobe atrophy. © 2014 Elsevier Ltd.",Adjectival meaning; Semantic memory; Semantic variant of primary progressive aphasia,adjective meaning; adult; aged; anterior temporal lobe; Article; case report; color discrimination; comprehension; concept formation; controlled study; decision making; female; human; learning and memory test; male; primary progressive aphasia; propensity score; semantic memory; semantics; task performance; temporal lobe,Article,Final,Scopus,2-s2.0-84916233263,Peter,,
"Jain D., Findlater L., Gilkeson J., Holland B., Duraiswami R., Zotkin D., Vogler C., Froehlich J.E.",57014317900;10040303000;57015245500;57014588100;7004261429;6602241431;7005789370;7101665384;,Head-mounted display visualizations to support sound awareness for the deaf and hard of hearing,2015,Conference on Human Factors in Computing Systems - Proceedings,2015-April,,,241,250,,61.0,10.1145/2702123.2702393,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951086834&doi=10.1145%2f2702123.2702393&partnerID=40&md5=2df40764f0c6db519132be8d2d578f99,"Persons with hearing loss use visual signals such as gestures and lip movement to interpret speech. While hearing AIDS and cochlear implants can improve sound recognition, they generally do not help the wearer localize sound necessary to leverage these visual cues. In this paper, we design and evaluate visualizations for spatially locating sound on a headmounted display (HMD). To investigate this design space, we developed eight high-level visual sound feedback dimensions. For each dimension, we created 3-12 example visualizations and evaluated these as a design probe with 24 deaf and hard of hearing participants (Study 1). We then implemented a real-time proof-of-concept HMD prototype and solicited feedback from 4 new participants (Study 2). Study 1 findings reaffirm past work on challenges faced by persons with hearing loss in group conversations, provide support for the general idea of sound awareness visualizations on HMDs, and reveal preferences for specific design options. Although preliminary, Study 2 further contextualizes the design probe and uncovers directions for future work. © Copyright 2015 ACM.",Accessibility; Conversation support; Deaf; Hard of hearing; Head-mounted display; Sound visualization; Wearable,Audition; Cochlear implants; Design; Hearing aids; Human computer interaction; Human engineering; Probes; Visualization; Accessibility; Deaf; Hard of hearings; Head mounted displays; Sound visualization; Wearable; Helmet mounted displays,Conference Paper,Final,Scopus,2-s2.0-84951086834,Peter,,
"Chua M., Adams R.S.",36809073400;7403932356;,Using realtime transcription to do member-checking during interviews,2015,"Proceedings - Frontiers in Education Conference, FIE",2015-February,February,7044251,,,,2.0,10.1109/FIE.2014.7044251,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938150596&doi=10.1109%2fFIE.2014.7044251&partnerID=40&md5=dbe066e6e5fbf3e2c1654ee661db5a4d,"CART (Communication Access Real-Time Translation), a realtime transcription service also known as 'open captioning,' can serve as a time-efficient and cost-effective tool for interviews and member-checks with technologically savvy populations such as those in engineering education research. CART can provide verbatim or near-verbatim full-text captures of even rapid technical conversations. CART professionals are widely available at universities as an accessibility provision for d/Deaf and hard-of-hearing students. Several engineering education research pilots indicate that CART can change the researcher/subject dynamic and reduce or eliminate the need for a separate member-check session. This paper explains the procedures and considerations behind employing CART for a qualitative study, including setup details for realtime transcription and the tradeoffs of pursuing such an approach. Following these pragmatic details, we investigate the implications of this technique on researcher/subject positionality and the role of transcription in research methodology. We also introduce the idea of 'grounded indigenous coding' as a methodological innovation, whereby realtime transcription enables participants to reflect on the words they have just spoken ('indigenous coding') while remaining grounded in the verbatim precision of a transcript. © 2014 IEEE.",Accessibility; Grounded indigenous coding; Member checks; Qualitative methods; Transcription,Audition; Codes (symbols); Cost effectiveness; Cost engineering; Education; Engineering research; Transcription; Transportation; Accessibility; Communication Access Real-Time Translation; Engineering education research; Grounded indigenous coding; Member checks; Methodological innovations; Qualitative method; Research methodologies; Engineering education,Conference Paper,Final,Scopus,2-s2.0-84938150596,Peter,,
Sandford J.,57206564173;,The impact of subtitle display rate on enjoyment under normal television viewing conditions,2015,IBC 2015,,,,,,,4.0,10.1049/ibc.2015.0018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088771238&doi=10.1049%2fibc.2015.0018&partnerID=40&md5=4307f9e751512eef36bd2d9f6a86bd79,"One of the properties often identified as having an impact on the television viewing experience for subtitle users is the rate of subtitling (measured in words per minute) (1,2). Previous studies on the subject have often restricted participants from using residual hearing or lip-reading as they naturally would when viewing television (3,4,5,6). Additionally, some studies were carried out with potentially biased participants (5,6). No research has been done to date at a large scale on the rate of scrolling subtitles as are often used in live subtitling (5,6). This paper presents the results of a study examining the impact of subtitle display rate on enjoyment for a representative sample of subtitle users. Specially created and off-air material was used with both block and scrolling subtitles. Sound was available and lip-reading was possible. The results challenge previous assumptions. © 2015 IBC 2015. All rights reserved.",,Audition; Lip reading; Representative sample; Residual hearing; Scrolling subtitles; Viewing conditions; User experience,Conference Paper,Final,Scopus,2-s2.0-85088771238,Peter,,
"Armstrong M., Brown A., Crabb M., Hughes C.J., Jones R., Sandford J.",57040522200;57212395814;56421925900;57040431000;56160498000;57206564173;,Understanding the diverse needs of subtitle users in a rapidly evolving media landscape,2015,IBC 2015,,,,,,,7.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022200239&partnerID=40&md5=8e5b5ccaf8c69053124293f85ec7ae28,"Audiences are increasingly using services such as video on demand and the web to watch television programmes. Broadcasters need to make subtitles available across all these new platforms. These platforms also create new design opportunities for subtitles along with the ability to customise them to an individual's needs. To explore these new opportunities for subtitles we have begun the process of reviewing the guidance for subtitles on television and evaluating the original user research. We have found that existing guidelines have been shaped by a mixture of technical constraints, industry practice and user research, constrained by existing technical standards. This paper provides an overview of the subtitle research at BBC R&D over the past two years. Our research is revealing significant diversity in the needs and preferences of frequent subtitle users, and points to the need for personalisation in the way subtitles are displayed. We are developing a new approach to the authoring and display of subtitles that can respond to the user requirements by adjusting the subtitle layout in the client device. © 2015 IBC 2015. All rights reserved.",,Display devices; Client devices; Industry practices; New approaches; Personalisation; Technical constraints; Technical standards; Television programmes; User requirements; Video on demand,Conference Paper,Final,Scopus,2-s2.0-85022200239,Peter,,
"Fernández-Torné A., Matamala A.",57189370627;25921557700;,Text-to-speech vs. Human voiced audio descriptions: A reception study in films dubbed into Catalan,2015,Journal of Specialised Translation,,24,,61,88,,37.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992026613&partnerID=40&md5=02f8913d52f671c98fd17a48aa91ac48,"This article presents an experiment that aims to determine whether blind and visually impaired people would accept the implementation of text-to-speech in the audio description of dubbed feature films in the Catalan context. A user study was conducted with 67 blind and partially sighted people who assessed two synthetic voices when applied to audio description, as compared to two natural voices. All of the voices had been previously selected in a preliminary test. The analysis of the data (both quantitative and qualitative) concludes that most participants accept Catalan text-to-speech audio description as an alternative solution to the standard human-voiced audio description. However, natural voices obtain statistically higher scores than synthetic voices and are still the preferred solution. © 2015 University of Roehampton. All rights reserved.",Accessibility; Audio description; Audiovisual translation; Blind; Catalan language; Speech synthesis; Text-to-speech; Visually impaired,,Review,Final,Scopus,2-s2.0-84992026613,Peter,,
"De Araújo Cardoso M.E., De Freitas Guilhermino D., Da Silva Neitzel R.A.L., Garcia L.S., Junior R.E.",56964443700;56964398200;56964340600;24824601400;56963688200;,Accessibility in E-commerce tools: An analysis of the optical inclusion of the deaf,2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9175,,,162,173,,3.0,10.1007/978-3-319-20678-3_16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947281131&doi=10.1007%2f978-3-319-20678-3_16&partnerID=40&md5=da091b2e91fc34ac20edc948bdf0d7f2,"The deaf communities are members of a unique culture and language, the Sign Language. Worldwide, the spoken/oral language is predominant, however, Deaf may encounter several hindrances to establish social relationships using spoken/oral language. E-commerce systems are significantly important not only to the listeners, but also to the Deaf, as E-commerce systems are the main vehicle for online shopping. Currently, the majority of the population shops online; nevertheless, the conditions in which information is disclosed in such systems may not appropriately respect the particularities of the Deaf. In this context, this paper supports the hypothesis that, identifying the accessibility requirements for the Deaf, the development of inclusive E-commerce systems is feasible and, thus, ensuring that the benefits and utilities provided by E-commerce systems are also accessible by deaf people. Therefore, in order to prove our hypothesis, the implications that the Sign Language (first language of the Deaf infers to the communication, to improve the accessibility of such environments, must be identified. This paper investigates the necessities of the deaf community when accessing Web systems, and based on evaluation mechanisms, analyses the environments developed using E-commerce tools concerning accessibility aspects. © Springer International Publishing Switzerland 2015.",Accessibility; Deaf community; E-commerce tools; Web accessibility,Commerce; Computational linguistics; Electronic commerce; Online systems; Transportation; Websites; Accessibility; Accessibility requirements; Deaf community; E-commerce systems; E-commerce tools; Online shopping; Social relationships; Web accessibility; Human computer interaction,Conference Paper,Final,Scopus,2-s2.0-84947281131,Peter,,
"Ellis K., Kent M.",36496810100;56224876400;,"Accessible television: The new frontier in disability media studies brings together industry innovation, government legislation and online activism",2015,First Monday,20,9,,,,1.0,13.0,10.5210/fm.v20i9.6170,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945969063&doi=10.5210%2ffm.v20i9.6170&partnerID=40&md5=80a01e0fa150c0ad426e1c7d14f1591a,"As television moves beyond digital broadcast modes of distribution towards online modes of delivery, this paper considers the opportunities and challenges for people with disabilities. With accessibility relying on a complex mix of regulation, legislation and industry innovation, the paper questions whether predictions of improved accessibility are an automatic outcome of new television technologies. The paper asks 'where to next?' for disability and the Internet through an emphasis on the importance of television in an accessible new media environment. The paper draws on government policies, the activist intervention of a number of people with disabilities as documented online, and primary research into Australian television audiences with disabilities that took place in 2013 and 2014.",,,Article,Final,Scopus,2-s2.0-84945969063,Peter,,
"Prietch S.S., Filgueiras L.V.L.",55925755100;23392315900;,Technology acceptance evaluation by deaf students considering the inclusive education context,2015,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),9296,,,20,37,,3.0,10.1007/978-3-319-22701-6_2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945538334&doi=10.1007%2f978-3-319-22701-6_2&partnerID=40&md5=bd62106a343e3c8adbfd0f70a76194b0,"As a consequence of the National Policy on Special Education on the Perspective of Inclusive Education in Brazil, established in 2007, mainstream schools have begun receiving a greater number of Deaf or Hard of Hearing (D/HH) students that previously attended specialized schools. However, data point to the declining number of D/HH students enrolled from primary school to secondary school; i.e., there are reasons to believe that educational barriers are imposed on the means these students have of conquering a complete education. In this context, the goal of this work is to propose a technology acceptance model that takes into account constructs that involve aspects of the inclusive education context, as well as performing a pilot test on the interaction of 16 D/HH users with a mobile application, called SESSAI, to evaluate the model. SESSAI consists of a technology-mediated form of communication, which allows hearing persons and D/HH individuals to interact through an automatic recognition system. Among the constructs of the model, one of them refers to the potential educational barriers experienced by D/HH students in inclusive classrooms. With regard to research methodology, the study was developed in cycles of literature review and conduction of tests. The proposed model has shown positive results in capturing factors that influence technology acceptance given the domain specific context, since they incorporate aspects of pragmatic quality and hedonic quality (emotional user experience), and also considers issues related to perceived usefulness in minimizing potential educational barriers, future expectations, and facilitating conditions. We conclude that the model encompasses both users’ personal motivation and context of use aspects, and it can be used for the purpose for which it was proposed. Further investigations need to be conducted in order to adjust the model questionnaire and to recruit a broader number of participants. © IFIP International Federation for Information Processing 2015.",Assistive technology; Country specific developments; Human-computer interaction; Media in education; Technology-mediated communication,Acceptance tests; Audition; Behavioral research; Education; Education computing; Engineering education; Human computer interaction; Teaching; Assistive technology; Automatic recognition system; Country-specific development; Facilitating conditions; Media in education; Research methodologies; Technology acceptance model; Technology-mediated communications; Students,Conference Paper,Final,Scopus,2-s2.0-84945538334,Peter,,
"Xiao X., Chen X., Palmer J.L.",55587801800;56582916600;55236687300;,Chinese Deaf viewers' comprehension of sign language interpreting on television: An experimental study,2015,Interpreting,17,1,,91,117,,16.0,10.1075/intp.17.1.05xia,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926461773&doi=10.1075%2fintp.17.1.05xia&partnerID=40&md5=ffcaa93257bddf8075527916472be13d,"Over 200 television channels in China broadcast news with signed language interpreting, making this one of the most visible forms of public accessibility for Deaf citizens. However, previous surveys have reported that most viewers have difficulty understanding the sign language interpreter. This experimental study examines how well a group of 49 Deaf individuals do, comparing their level of comprehension with that of twenty hearing viewers whose medium of access to program content is spoken Mandarin. All participants completed simple comprehension questions, in written form, after viewing twenty short news clips. These were shown once to the hearing viewers, and twice to Deaf viewers so as to compensate for any intrinsic difficulty related to the limited visual clarity of televised signed language interpreting. Results show that, even with interpretation, the Deaf viewers do not benefit equally from the news clips. Analysis of the interpretations suggests that the interpreters' lack of Chinese Sign Language fluency might have contributed to the Deaf viewers' lesser comprehension. In addition to insufficient training, the high pressure the interpreters experience in relation to interpreting in media settings might have a negative effect on the quality of their interpretation. © John Benjamins Publishing Company.",Chinese Sign Language; Comprehension testing; Deaf; Signed language interpreting; Television news,,Article,Final,Scopus,2-s2.0-84926461773,Peter,,
"Sharmin S., Špakov O., Räihä K.-J.",35772967600;12241013900;6601968578;,Dynamic text presentation in print interpreting - An eye movement study of reading behaviour,2015,International Journal of Human Computer Studies,78,,,17,30,,6.0,10.1016/j.ijhcs.2015.01.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923618729&doi=10.1016%2fj.ijhcs.2015.01.010&partnerID=40&md5=4a47e1fbb3fd870a2fbe891621bf221d,"Print interpreting supports people with a hearing disability by giving them access to spoken language. In print interpreting, the interpreter types the spoken text in real time for the hard-of-hearing client to read. This results in dynamic text presentation. An eye movement study was conducted to compare two types of dynamic text presentation formats in print interpreting: letter-by-letter and word-by-word. Gaze path analysis with 20 hearing participants showed different types of reading behaviour during reading of two pieces of text in these two presentation formats. Our analysis revealed that the text presentation format has a significant effect on reading behaviour. Rereading and regressions occurred significantly more often with the word-by-word format than with the letter-by-letter format. We also found a significant difference between the number of regressions starting at the words that end a sentence and that of regressions starting at all other words. The frequency of rereading was significantly higher for incorrectly typed or abbreviated words than for the other words. Analysis of the post-test questionnaire found almost equal acceptance of the word-by-word and letter-by-letter formats by the participants. A follow-up study with 18 hard-of-hearing participants showed a similar trend in results. The findings of this study highlight the importance of developing print interpreting tools that allow the interpreter and the client to choose the options that best facilitate the communication. They also bring up the need to develop new eye movement metrics for analysing the reading of dynamic text, and provide first results on a new dynamic presentation context. © 2015 Published by Elsevier Ltd.",Dynamic text presentation; Eye movements; Print interpreting; Reading; Regressions,Acceptance tests; Audition; Regression analysis; Dynamic presentation; Dynamic text; Follow-up Studies; Hard of hearings; Hearing disabilities; Presentation formats; Reading; Regressions; Eye movements,Article,Final,Scopus,2-s2.0-84923618729,Peter,,
"Lasecki W.S., Kushalnagar R., Bigham J.P.",54383728900;36142036500;16238221500;,Legion scribe: Real-time captioning by non-experts,2014,ASSETS14 - Proceedings of the 16th International ACM SIGACCESS Conference on Computers and Accessibility,,,,303,304,,9.0,10.1145/2661334.2661352,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911395780&doi=10.1145%2f2661334.2661352&partnerID=40&md5=803211f4b1fd76ab571191526454d55c,"The promise of affordable, automatic approaches to realtime captioning imagines a future in which deaf and hard of hearing (DHH) users have immediate access to speech in the world around them my simply picking up their phone or other mobile device. While the challenges of processing highly variable natural language has prevented automated approaches from completing this task reliably enough for use in settings such as classrooms or workplaces [4], recent work in crowd-powered approaches have allowed groups of non-expert captionists to provide a similarly-flexible source of captions for DHH users. This is in contrast to current human-powered approaches, which use highly-trained professional captionists who can type up to 250 words per minute (WPM), but also can cost over $100/hr. In this paper, we describe a real-time demo of Legion: Scribe (or just ""Scribe""), a crowd-powered captioning system that allows untrained participants and volunteers to provide reliable captions with less than 5 seconds of latency by computationally merging their input into a single collective answer that is more accurate and more complete than any one worker could have generated alone.",Captioning; Crowdsourcing; Deaf; Hard of hearing; Real-time human computation; Speech-to-text,Crowdsourcing; Mobile devices; Automated approach; Automatic approaches; Captioning; Deaf; Hard of hearings; Human computation; Human-powered; Natural languages; Audition,Conference Paper,Final,Scopus,2-s2.0-84911395780,Peter,,
"Cambra C., Penacchio O., Silvestre N., Leal A.",7801358308;6507111711;15137439300;7004298024;,Visual attention to subtitles when viewing a cartoon by deaf and hearing children: an eye-tracking pilot study,2014,Perspectives: Studies in Translatology,22,4,,607,617,,17.0,10.1080/0907676X.2014.923477,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84914666193&doi=10.1080%2f0907676X.2014.923477&partnerID=40&md5=e062e71cef2429b21eec6b8beaf4ef50,"Watching a subtitled programme is a complex activity, as it entails paying attention to various stimuli simultaneously, some of which are visual (images and subtitles) and others auditory (oral language and background sounds). The aim of this study is to analyse the ocular movement of a group of children including both hearing and deaf children when watching a television cartoon using an eye tracker. The sample comprises 22 children (11 of whom are deaf and 11 of whom are hearing) aged between seven and 11. The results show that both hearing and deaf children spend more time looking at the images than at the subtitles, with the character's lips being the facial feature to which they pay most attention. Participant age and reading speed are variables that significantly affect the degree of attention paid to subtitles: the youngest children with the slowest reading speed lose their attention as the cartoon progresses. However, participants' auditory condition (deaf or hearing) does not show significant differences regarding maintaining attention on subtitles. © 2014, Taylor & Francis.",cartoon; deaf viewers; eye-tracking; reading speed; subtitling; television,,Article,Final,Scopus,2-s2.0-84914666193,Peter,,
"Pascual A., Ribera M., Granollers T.",35186398400;23670757400;56962781000;,Impact of web accessibility barriers on users with hearing impairment,2014,ACM International Conference Proceeding Series,10-12-September-2014,,a8,,,,6.0,10.1145/2662253.2662261,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985996126&doi=10.1145%2f2662253.2662261&partnerID=40&md5=f7be4752757d2dcc3e3845bca7174427,"Several users tests were carried out with people with a hearing impairment to evaluate the impact of different web accessibility barriers on two similar web sites, one accessible and the other not accessible. The tests focus was to analyze user's mood when faced to different accessibility barriers. Results show ""complex text"" and ""multimedia content without text alternative"" barriers as the most critical for users with this profile. Our investigation contributes to a better understanding of users confronting accessibility barriers, and to emphasize the need of web content authors writing content using plain language and providing captions or sign language to video content. Copyright 2010 ACM.",Accessible content; Barriers; Communicability; Deaf users; Hearing impairment; User test; WCAG 2.0; Web accesibility,Audition; Websites; Accesibility; Accessible content; Barriers; Communicability; Deaf users; Hearing impairments; User tests; WCAG 2.0; Human computer interaction,Conference Paper,Final,Scopus,2-s2.0-84985996126,Peter,,
"Duncan J., Rhoades E.A., Fitzpatrick E.M.",26535778100;35901494600;35577968800;,Auditory (Re)Habilitation for Adolescents with Hearing Loss: Theory and Practice,2014,Auditory (Re)Habilitation for Adolescents with Hearing Loss: Theory and Practice,,,,1,448,,3.0,10.1093/acprof:oso/9780195381405.001.0001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940099227&doi=10.1093%2facprof%3aoso%2f9780195381405.001.0001&partnerID=40&md5=97558698c7f55b20dd84930513b2c794,"There is a growing realization that many adolescents with hearing loss require special attention. Despite the benefits of early diagnosis, early amplification, and early intervention, some adolescents with hearing loss do not achieve age equivalent developmental milestones. The purpose of this book is to assist auditory (re)habilitation practitioners in mitigating the negative effects of hearing loss on communicative, socio-emotional, and academic performance of adolescents who rely on auditory-based spoken language to communicate. It is essential that adolescents whose parents choose auditory-based spoken language, receive systematic, consistent, well-planned, appropriate auditory (re)habilitation. This book presents an evidence-based approach to auditory (re)habilitation for adolescents with hearing loss. Practitioners are provided with theoretical and practical strategies for intervention, targeting a historically overlooked population. Practitioners will find its framework, based on enhancing adolescent inner resources, an informative and unique approach toward enabling adolescent self-determination. © Oxford University Press 2014. All rights reserved.",Adolescent; Audiology; Auditory (re)habilitation; Cochlear implants; Cognitive; Deaf; Hearing aid; Hearing loss; Language; Literacy; Phonological awareness; Speech; Spoken communication,,Book,Final,Scopus,2-s2.0-84940099227,Peter,,
"BavaHarji M., Alavi Z.K., Letchumanan K.",39761698300;56114217900;56115277500;,"Captioned instructional video: Effects on content comprehension, vocabulary acquisition and language proficiency",2014,English Language Teaching,7,5,,1,16,,12.0,10.5539/elt.v7n5p1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898632038&doi=10.5539%2felt.v7n5p1&partnerID=40&md5=26356b0bd5e570621bdbff78233b3570,"This experimental design study examined the effects of viewing captioned instructional videos on EFL learners' content comprehension, vocabulary acquisition and language proficiency. It also examined the participants' perception of viewing the captioned instructional videos. The 92 EFL students in two classes, who were undertaking the Tape and Video Interpretation course, participated in this study. The randomly assigned experimental class viewed 30 episodes of captioned Connect with English and the control class viewed the same episodes without captions. Adopting the quantitative approach, a Michigan English Test, Content-Specific Tests and a questionnaire were administered to examine the participants' content comprehension, vocabulary acquisition and language proficiency development as well as the experimental group's perception towards viewing captioned instructional videos. Although, both groups recorded gains, the findings were in favor of the use of captioned instructional videos. The results showed that the effects of viewing captioned instructional videos are greater on vocabulary acquisition and language proficiency development than on content comprehension. The participants' perceptions of the use of captioned instructional video were consistent with the results. They felt that it enhanced their language learning, but did not affect their comprehension of the movie and that captions were not a form of distraction. Pedagogical implications for EFL instructions, especially where multimedia technology tools may be limited is that, captioned instructional videos can be deemed as a promising media to enhance language learning.",Captioned instructional video; Content comprehension; EFL learners; English language proficiency development; Vocabulary acquisition,,Article,Final,Scopus,2-s2.0-84898632038,Peter,,
"Stinson M.S., Elliot L.B., Easton D.",7005000200;7003340239;56106310000;,Deaf/hard-of-hearing and other postsecondary learners' retention of STEM content with tablet computer-based notes,2014,Journal of Deaf Studies and Deaf Education,19,2,,252,269,,9.0,10.1093/deafed/ent049,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897892384&doi=10.1093%2fdeafed%2fent049&partnerID=40&md5=a1c6433bacc82dbf09a7e53fff5fe9b9,"Four groups of postsecondary students, 25 who were deaf/ hard of hearing (D/HH), 25 with a learning disability, 25 who were English language learners (ELLs), and 25 without an identified disability studied notes that included text and graphical information based on a physics or a marine biology lecture. The latter 3 groups were normally hearing. All groups had higher scores on post- than on pretests for each lecture, with each group showing generally similar gains in amount of material learned from the pretest to the posttest. For each lecture, the D/HH students scored lower on the pre- and posttests than the other 3 groups of participants. Results indicated that students acquired measurable amounts of information from studying these types of notes for relatively short periods and that the notes have equal potential to support the acquisition of information by each of these groups of students. © The Author 2013. Published by Oxford University Press. All rights reserved.",,computer; education; engineering; hearing impairment; human; learning; learning disorder; mathematics; physiology; psychology; qualitative research; science; student; technology; utilization; Computers; Educational Measurement; Engineering; Hearing Loss; Humans; Learning; Learning Disorders; Mathematics; Qualitative Research; Science; Students; Technology,Article,Final,Scopus,2-s2.0-84897892384,Peter,,
"Bisson M.-J., Van Heuven W.J.B., Conklin K., Tunney R.J.",55487149600;7003726457;23967922100;6701528109;,Processing of native and foreign language subtitles in films: An eye tracking study,2014,Applied Psycholinguistics,35,2,,399,418,,85.0,10.1017/S0142716412000434,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894585716&doi=10.1017%2fS0142716412000434&partnerID=40&md5=9777adc9ee18af63c26de80d150dc021,"Foreign language (FL) films with subtitles are becoming increasingly popular, and many European countries use subtitling as a cheaper alternative to dubbing. However, the extent to which people process subtitles under different subtitling conditions remains unclear. In this study, participants watched part of a film under standard (FL soundtrack and native language subtitles), reversed (native language soundtrack and FL subtitles), or intralingual (FL soundtrack and FL subtitles) subtitling conditions while their eye movements were recorded. The results revealed that participants read the subtitles irrespective of the subtitling condition. However, participants exhibited more regular reading of the subtitles when the film soundtrack was in an unknown FL. To investigate the incidental acquisition of FL vocabulary, participants also completed an unexpected auditory vocabulary test. Because the results showed no vocabulary acquisition, the need for more sensitive measures of vocabulary acquisition are discussed. Finally, the reading of the subtitles is discussed in relation to the saliency of subtitles and automatic reading behavior. © Cambridge University Press 2012 Â The online version of this article is published within an Open Access environment subject to the conditions of the Creative Commons Attribution-NonCommercial-ShareAlike licence <.",,,Article,Final,Scopus,2-s2.0-84894585716,Peter,,
"Istenic Starcic A., Bagon S.",35766943000;55809595900;,"ICT-supported learning for inclusion of people with special needs: Review of seven educational technology journals, 1970-2011",2014,British Journal of Educational Technology,45,2,,202,230,,59.0,10.1111/bjet.12086,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894263611&doi=10.1111%2fbjet.12086&partnerID=40&md5=16416774d2ddafdc22a26b45caf6281f,"Research and development of information and communication technology (ICT)-supported learning for people with disabilities has not received adequate attention. It is also difficult to access research findings and developments in this field. Under the ENABLE Network of ICT Supported Learning for Disabled People (2011-2014) project, an emerging European Union reference point portal for end-users will provide this information for a broad audience. In the design phase of the project idea, the authors of this paper conducted a review of papers indexed in Web of Science to provide a needs assessment and a design template for the project objectives. The results of the search clearly showed that ICT-supported learning for people with special educational needs is in the domain of the educational technology journals, with more papers published in the British Journal of Educational Technology than in any other journal. This paper presents the results of a content analysis of all papers published from 1970 to 2011 in seven educational technology journals indexed in Web of Science. More papers were published from 2006 to 2011 (44.7%) than during any other of five periods examined. Findings in terms of ICT intervention, disability groups, groups of study participants by relationship with ICT, and research design, together with trends in published studies in terms of mainstreaming and inclusion, are presented. The main objective of the study was to identify the level of inclusion through analysis of educational context (special schools [30.51%], mainstream schools [28.81%] and general support for life [40.68%]). Based on content analysis, ICT interventions were classified into the two categories of technical intervention in the pedagogical context (62.71% of all papers published) and technical intervention in the wider context (37.29% of all papers published), with nine paper types identified: papers on ICT access, papers on teaching and learning methods, papers on development and testing of ICT solutions, reviews, assessments, papers on inclusion, papers on behavioural and social development, papers on use of information technology and papers on interaction. Papers were also categorised according to types of disability and according to groups of study participants by relationship with ICT. Published papers were divided into four categories by research design: descriptive (49.15%), developmental (26.27%), experimental (17.8%), and developmental and experimental (6.78%). During the period from 1970 to 2000, papers examined design of learning materials with regard to particular categories of disability and particular accessibility needs, while papers published after 2000 also discussed universal design. Based on the review, the authors of this paper have identified a need for application of universal design principles in research and development of learning environments to provide equal accessibility and inclusive education. © 2013 British Educational Research Association.",,,Review,Final,Scopus,2-s2.0-84894263611,Peter,,
"Hinkin M.P., Harris R.J., Miranda A.T.",25925889600;36182974200;56050800100;,Verbal redundancy aids memory for filmed entertainment dialogue,2014,Journal of Psychology: Interdisciplinary and Applied,148,2,,161,176,,9.0,10.1080/00223980.2013.767774,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891884826&doi=10.1080%2f00223980.2013.767774&partnerID=40&md5=9221b141e6a120eef162e75f1286e664,"Three studies investigated the effects of presentation modality and redundancy of verbal content on recognition memory for entertainment film dialogue. U.S. participants watched two brief movie clips and afterward answered multiple-choice questions about information from the dialogue. Experiment 1 compared recognition memory for spoken dialogue in the native language (English) with subtitles in English, French, or no subtitles. Experiment 2 compared memory for material in English subtitles with spoken dialogue in English, French, or no sound. Experiment 3 examined three control conditions with no spoken or captioned material in the native language. All participants watched the same video clips and answered the same questions. Performance was consistently good whenever English dialogue appeared in either the subtitles or sound, and best of all when it appeared in both, supporting the facilitation of verbal redundancy. Performance was also better when English was only in the subtitles than when it was only spoken. Unexpectedly, sound or subtitles in an unfamiliar language (French) modestly improved performance, as long as there was also a familiar channel. Results extend multimedia research on verbal redundancy for expository material to verbal information in entertainment media. © 2014 Copyright Taylor and Francis Group, LLC.",film entertainment; memory for film; subtitles; verbal redundancy,"article; attention; audiovisual equipment; comparative study; human; learning; multilingualism; psychological aspect; reading; recognition; short term memory; speech perception; student; vision; Attention; Humans; Memory, Short-Term; Motion Pictures as Topic; Multilingualism; Reading; Recognition (Psychology); Speech Perception; Students; Verbal Learning; Visual Perception",Article,Final,Scopus,2-s2.0-84891884826,Peter,,
Baker M.,12244996700;,The changing landscape of translation and interpreting studies,2014,A Companion to Translation Studies,,,,13,27,,24.0,10.1002/9781118613504.ch1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101894793&doi=10.1002%2f9781118613504.ch1&partnerID=40&md5=18224ee1c8bab01894cd40c27b8997a6,"Translation and interpreting studies is now a vast and growing area of scholarship. This essay offers a broad overview of some of its main concerns, focusing on a number of themes that have received growing attention in the 1990s and the early part of the twenty-first century. These are: representation; minority-majority relations; globalization, the global economy, and global resistance. The essay concludes with a brief discussion of future directions, including further engagement with questions of ethics and trust, and the impact of new media cultures and technologies. © 2014 John Wiley & Sons, Ltd.",Ethics; Globalization; Minority; New media; Representation; Technology; User-generated translation,,Book Chapter,Final,Scopus,2-s2.0-85101894793,Peter,,
"Shortliffe E.H., Cimino J.J.",7006598518;7006337145;,Biomedical informatics: Computer applications in health care and biomedicine: Fourth edition,2014,Biomedical Informatics: Computer Applications in Health Care and Biomedicine: Fourth Edition,,,,1,965,,31.0,10.1007/978-1-4471-4474-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028500234&doi=10.1007%2f978-1-4471-4474-8&partnerID=40&md5=430ff219efd776c9e545c4b001bfe398,"The practice of modern medicine and biomedical research requires sophisticated information technologies with which to manage patient information, plan diagnostic procedures, interpret laboratory results, and carry out investigations. Biomedical Informatics provides both a conceptual framework and a practical inspiration for this swiftly emerging scientific discipline at the intersection of computer science, decision science, information science, cognitive science, and biomedicine. Now revised and in its third edition, this text meets the growing demand by practitioners, researchers, and students for a comprehensive introduction to key topics in the field. Authored by leaders in medical informatics and extensively tested in their courses, the chapters in this volume constitute an effective textbook for students of medical informatics and its areas of application. The book is also a useful reference work for individual readers needing to understand the role that computers can play in the provision of clinical services and the pursuit of biological questions. The volume is organized so as first to explain basic concepts and then to illustrate them with specific systems and technologies. © Springer-Verlag London 2014. All rights reserved.",,Bioinformatics; Diagnosis; Biomedical informatics; Biomedical research; Cognitive science; Conceptual frameworks; Diagnostic procedure; Medical informatics; Patient information; Scientific discipline; Medical computing,Book,Final,Scopus,2-s2.0-85028500234,Peter,,
"Szarkowska A., Zbikowska J., Krejtz I.",54416458200;55602697700;54395481500;,Strategies for rendering multilingualism in subtitling for the deaf and hard of hearing,2014,"Linguistica Antverpiensia, New Series – Themes in Translation Studies",13,1,,273,291,,8.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028008864&partnerID=40&md5=dd486397ee900e384097862ffd4c76eb,"In this paper we present a set of strategies for rendering the presence of multiple languages in multilingual films in subtitling for the deaf and hard of hearing (SDH): vehicular matching, explicit attribution, colour-coding and linguistic homogenisation. We also report on an online study among deaf and hard of hearing Polish participants regarding their preferences for specific SDH strategies. The findings show that, even when they do not know the foreign language involved, most participants prefer more informative strategies where indications of multilingualism are made explicit.",,,Article,Final,Scopus,2-s2.0-85028008864,Peter,,
"Zarate S., Eliahoo J.",57214838654;6506656776;,Word recognition and content comprehension of subtitles for television by deaf children,2014,Journal of Specialised Translation,,21,,133,152,,14.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017635879&partnerID=40&md5=efb8014589284f00381335a6183d8455,"This project explores how deaf children read subtitles on television. The participants – recruited from years 3 to 6 of a mainstream school with a hearing impairment unit – were exposed to both broadcast and enhanced subtitles and their performances were compared. In particular, the focus is on identifying enhancements that can help children to understand subtitle content and to recognise new or difficult words. Among the enhancements introduced were repetition and highlighting of new or difficult words through the use of a bigger and different typeface, use of longer reading times, text reduction and careful spotting. This pilot study provides some useful information for future empirical experimental research on subtitling for deaf children. © 2014 University of Roehampton. All rights reserved.",Content comprehension; Deaf children; Subtitling; Word recognition,,Article,Final,Scopus,2-s2.0-85017635879,Peter,,
"Brady E., Bigham J.P.",54402279500;16238221500;,Crowdsourcing accessibility: Human-Powered access technologies,2014,Foundations and Trends in Human-Computer Interaction,8,4,,273,372,,17.0,10.1561/1100000050,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973645313&doi=10.1561%2f1100000050&partnerID=40&md5=6d81da2626b0ab69836fcc1ced4a3a9e,"People with disabilities have always engaged the people around them in order to circumvent inaccessible situations, allowing them to live more independently and get things done in their everyday lives. Increasing connectivity is allowing this approach to be extended to wherever and whenever it is needed. Technology can leverage this human work force to accomplish tasks beyond the capabilities of computers, increasing how accessible the world is for people with disabilities. This article outlines the growth of online human support, outlines a number of projects in this space, and presents a set of challenges and opportunities for this work going forward. in E. © 2015 E. Brady and J. P. Bigham.",,Computer applications; Access technology; Human support; Human-powered; People with disabilities; Work force; Human computer interaction,Article,Final,Scopus,2-s2.0-84973645313,Peter,,
Blanck P.,7005084017;,Equality the struggle for web accessibility by persons with cognitive disabilities,2014,Equality the Struggle for Web Accessibility by Persons with Cognitive Disabilities,,,,1,467,,52.0,10.1017/CBO9781107280151,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952937754&doi=10.1017%2fCBO9781107280151&partnerID=40&md5=865535956dbecc817bf0adf8fa7086a3,"Never before have the civil rights of people with disabilities aligned so well with developments in information and communication technology. The center of the technology revolution is the Internet’s World Wide Web, which fosters unprecedented opportunities for engagement in democratic society. The Americans with Disabilities Act likewise is helping to ensure equal participation in society by people with disabilities. Globally, the Convention on the Rights of Persons with Disabilities further affirms that persons with disabilities are entitled to the full and equal enjoyment of fundamental personal freedoms. This book is about the lived struggle for disability rights, with a focus on Web equality for people with cognitive disabilities, such as intellectual disabilities, autism, and print-related disabilities. The principles derived from the right to the Web — freedom of speech and individual dignity are bound to lead toward full and meaningful involvement in society for persons with cognitive and other disabilities. © Peter Blanck 2014. All rights reserved.",,,Book,Final,Scopus,2-s2.0-84952937754,Peter,,
"Shadiev R., Hwang W.-Y., Chen N.-S., Huang Y.-M.",24825659700;55703771100;7401912239;8630348700;,Review of speech-to-text recognition technology for enhancing learning,2014,Educational Technology and Society,17,4,,65,84,,47.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908510752&partnerID=40&md5=8b7757ebe21a5b1a2c35915936869b1a,"This paper reviewed literature from 1999 to 2014 inclusively on how Speech-to-Text Recognition (STR) technology has been applied to enhance learning. The first aim of this review is to understand how STR technology has been used to support learning over the past fifteen years, and the second is to analyze all research evidence to understand how Speech-to-Text Recognition technology can enhance learning. The findings are discussed from different perspectives as follows: (a) potentials of STR technology, (b) its use by specific groups of users in different domains, (c) quantitative and/or qualitative research methodology used, and (d) STR technology implications. Some STR literature review showed that in earlier stage of development, the STR technology was applied to assist learning only for specific users, i.e., students with cognitive and physical disabilities, or foreign students. Educators and researchers started to apply STR technology in a traditional learning environment to assist broader group of users, while STR technology has been rapidly advancing over the years. The review revealed a number of distinct advantages of using STR for learning. That is, STR-generated texts enable students to understand learning content of a lecture better, to confirm missed or misheard parts of a speech, to take notes or complete homework, and to prepare for exams. Furthermore, some implications over the STR technology in pedagogical and technological aspects were discussed in the review, such as the design of technology-based learning activities, accuracy rate of the STR process and learning behaviors to use STR-texts that may limit the STR educational value. Thus, the review furthermore discussed some potential solutions for the future research.",Group of users; Literature review; Speech-to-text recognition technology; Supporting and enhancing learning,,Article,Final,Scopus,2-s2.0-84908510752,Peter,,
"Booker A., Malcarne V.L., Sadler G.R.",55908417200;6701495867;7005152183;,Evaluating outcomes of community-based cancer education interventions: A 10-year review of studies,2014,Journal of Cancer Education,29,2,,233,240,,6.0,10.1007/s13187-013-0578-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905576055&doi=10.1007%2fs13187-013-0578-6&partnerID=40&md5=87b6a5c97adfbf8a72ec16f2e1dfb657,"The public is encouraged to participate in cancer education programs because it is believed that acquiring health-promoting knowledge will motivate participants to make the recommended, evidence-based behavioral modifications that should lead to reductions in cancer morbidity and mortality. Because of the extended time that elapses between conducting a health education program and the amassing of the scientific evidence needed to establish that an education program has ultimately resulted in a reduction in morbidity and mortality, researchers have sought more proximal and intermediate outcome measures as substitutes for the more distal desired outcomes. This paper presents an analysis of research published in the Journal of Cancer Education from 2000 through 2010, in which the impact of cancer education interventions was evaluated. The focus was to identify the proximal, intermediate, and distal outcome measures used to evaluate the impact of cancer education interventions. The results showed that researchers primarily focus on measuring the varied proximal outcomes (e.g., knowledge and attitude changes) of cancer education interventions. Intermediate outcome measures (the desired behavior change itself) received less attention, while distal outcomes (changes in morbidity and mortality) were never measured. This review gives cancer education researchers a review of the proximal and intermediate outcome measures and strategies that behavioral scientists recently used to overcome the challenges of measuring distal outcomes. Future reviews could expand this analysis to studies published in other journals and health disciplines. © Springer Science+Business Media New York 2013.",Cancer education; Intervention; Measurement; Outcomes; Review,"attitude to health; community care; health education; human; intervention study; Neoplasms; organization and management; patient education; program evaluation; psychology; Community Health Services; Health Education; Health Knowledge, Attitudes, Practice; Humans; Intervention Studies; Neoplasms; Patient Education as Topic; Program Evaluation",Review,Final,Scopus,2-s2.0-84905576055,Peter,,
"Valor Miró J.D., Spencer R.N., Pérez González de Martos A., Garcés Díaz-Munío G., Turró C., Civera J., Juan A.",55536102400;56239631900;55535904600;56239474000;18435401600;8512992700;7005475190;,Evaluating intelligent interfaces for post-editing automatic transcriptions of online video lectures,2014,Open Learning,29,1,,72,85,,11.0,10.1080/02680513.2014.909722,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903515625&doi=10.1080%2f02680513.2014.909722&partnerID=40&md5=bd1aee110d0de92226d6c3bde370638d,"Video lectures are fast becoming an everyday educational resource in higher education. They are being incorporated into existing university curricula around the world, while also emerging as a key component of the open education movement. In 2007, the Universitat Politècnica de València (UPV) implemented its poliMedia lecture capture system for the creation and publication of quality educational video content and now has a collection of over 10,000 video objects. In 2011, it embarked on the EU-subsidised transLectures project to add automatic subtitles to these videos in both Spanish and other languages. By doing so, it allows access to their educational content by non-native speakers and the deaf and hard-of-hearing, as well as enabling advanced repository management functions. In this paper, following a short introduction to poliMedia, transLectures and Docència en Xarxa (Teaching Online), the UPV's action plan to boost the use of digital resources at the university, we will discuss the three-stage evaluation process carried out with the collaboration of UPV lecturers to find the best interaction protocol for the task of post-editing automatic subtitles. © 2014 The Open University.",automatic speech recognition; modes of interaction; online learning; user evaluations; video lectures,,Article,Final,Scopus,2-s2.0-84903515625,Peter,,
"Borgia F., Bianchini C.S., De Marsico M.",55303419200;55302971600;6508106114;,Towards improving the e-learning experience for deaf students: e-LUX,2014,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),8514 LNCS,PART 2,,221,232,,5.0,10.1007/978-3-319-07440-5_21,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903441819&doi=10.1007%2f978-3-319-07440-5_21&partnerID=40&md5=afa98bf87c5de4a12dbfa25524454469,"Deaf people are more heavily affected by the digital divide than many would expect. Moreover, most accessibility guidelines addressing their needs just deal with captioning and audio-content transcription. However, this approach to the problem does not consider that deaf people have big troubles with vocal languages, even in their written form. At present, only a few organizations, like W3C, produced guidelines dealing with one of their most distinctive expressions: Sign Language (SL). SL is, in fact, the visual-gestural language used by many deaf people to communicate with each other. The present work aims at supporting e-learning user experience (e-LUX) for these specific users by enhancing the accessibility of content and container services. In particular, we propose preliminary solutions to tailor activities which can be more fruitful when performed in one's own ""native"" language, which for most deaf people, especially younger ones, is represented by national SL. © 2014 Springer International Publishing.",Deaf needs; e-learning; Sign Language; SignWriting; User Experience,E-learning; Transportation; Visual languages; Accessibility guidelines; Audio content; Deaf needs; Deaf students; Digital divide; Sign language; SignWriting; User experience; Human computer interaction,Conference Paper,Final,Scopus,2-s2.0-84903441819,Peter,,
"Kacorri H., Shinkawa K., Saito S.",35487798800;56178287000;35467617800;,Introducing game elements in crowdsourced video captioning by non-experts,2014,W4A 2014 - 11th Web for All Conference,,,2596713,,,,13.0,10.1145/2596695.2596713,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901328351&doi=10.1145%2f2596695.2596713&partnerID=40&md5=a7dc2b9a5f2a3a24cb03f69d0a92c782,"Video captioning can increase the accessibility of information for people who are deaf or hard-of-hearing and benefit second language learners and reading-deficient students. We propose a caption editing system that harvests crowdsourced work for the useful task of video captioning. To make the task an engaging activity, its interface incorporates game-like elements. Non-expert users submit their transcriptions for short video segments against a countdown timer, either in a ""type"" or ""fix"" mode, to score points. Transcriptions from multiple users are aligned and merged to form the final captions. Preliminary results with 42 participants and 578 short video segments show that the Word Error Rate of the merged captions with two users per segment improved from 20.7% in ASR to 16%. Finally, we discuss our work in progress to improve both the accuracy of the collected data and to increase the crowd engagement. Copyright 2014 ACM.",Crowdsourcing; Gamification; Transcription; Video captioning,Audition; Image segmentation; Crowdsourcing; Game-like elements; Gamification; Hard of hearings; Second language learners; Video captioning; Word error rate; Work in progress; Transcription,Conference Paper,Final,Scopus,2-s2.0-84901328351,Peter,,
"Lasecki W.S., Kushalnagar R., Bigham J.P.",54383728900;36142036500;16238221500;,Helping students keep up with real-time captions by pausing and highlighting,2014,W4A 2014 - 11th Web for All Conference,,,2596701,,,,21.0,10.1145/2596695.2596701,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901303048&doi=10.1145%2f2596695.2596701&partnerID=40&md5=3ca5463beb9a66dee0689446280ed685,"We explore methods for improving the readability of real-time captions by allowing users to more easily switch their gaze between multiple visual information sources. Real-time captioning provides deaf and hard of hearing (DHH) users with access to spoken content during live events, and the web has allowed these services to be provided via remotely-located captioning services, and for web content itself. However, despite caption benefits, spoken language reading rates often result in DHH users falling behind spoken content, especially when the audio is paired with visual references. This is particularly true in classroom settings, where multi-modal content is the norm, and captions are often poorly positioned in the room, relative to speakers. Additionally, this accommodation can benefit other students who face temporary or ""situational"" disabilities such as listening to unfamiliar speech accents, or if a student is in a location with poor acoustics. In this paper, we explore pausing and highlighting as a means of helping DHH students keep up with live classroom content by helping them track their place when reading text involving visual references. Our experiments show that by providing users with a tool to more easily track their place in a transcript while viewing live video, it is possible for them to follow visual content that might otherwise have been missed. Both pausing and highlighting have a positive impact on students' scores on comprehension tests, but highlighting is preferred to pausing, and yields nearly twice as large of an improvement. We then discuss several issues with captioning that we observed during our design process and user study, and then suggest future work that builds on these insights. Copyright 2014 ACM.",Accessibility; Caption readability; Human factors; Inclusive classrooms; Real-time captioning,Audition; Human engineering; Accessibility; Caption readability; Classroom settings; Inclusive classrooms; Real-time captioning; Spoken languages; Visual information; Visual reference; Students,Conference Paper,Final,Scopus,2-s2.0-84901303048,Peter,,
"Torrente J., del Blanco Á., Serrano-Laguna A., Vallejo-Pinto J.Á., Moreno-Ger P., Fernández-Manjón B.",24482065900;26657806000;56685528300;49561949700;8951372100;55884700700;,Towards a low cost adaptation of educational games for people with disabilities,2014,Computer Science and Information Systems,11,1,,369,391,,30.0,10.2298/CSIS121209013T,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893412649&doi=10.2298%2fCSIS121209013T&partnerID=40&md5=4515a480cd8f12d1dbf9d25d963f1695,"In this paper we analyze how to increase the level of accessibility in videogames by adding support for it in game authoring software. This approach can reduce the effort required to make a game accessible for people with disabilities, resulting in significant savings. A case study is presented to support the approach based on the eAdventure educational game authoring platform, which allows semi-automatic adaptation of the games. The game, My First Day At Work, was made accessible for students with different disability profiles, mainly blindness, low vision and limited mobility, although hearing and cognitive disabilities are also considered. Results show that the effort needed to make the games accessible is moderate in comparison to the total effort dedicated to game development. Although the specific solutions proposed are optimized for educational games, they could be generalized to other game frameworks and purposes (e.g. entertainment, advertising, etc.).",Accessibility; Educational games; Serious games; Universal design,,Article,Final,Scopus,2-s2.0-84893412649,Peter,,
"Kushalnagar R.S., Lasecki W.S., Bigham J.P.",36142036500;54383728900;16238221500;,Accessibility evaluation of classroom captions,2014,ACM Transactions on Accessible Computing,5,3,2543578,,,,44.0,10.1145/2543578,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893353120&doi=10.1145%2f2543578&partnerID=40&md5=935bfd646215532dc015ac07d85e9826,"Real-time captioning enables deaf and hard of hearing (DHH) people to follow classroom lectures and other aural speech by converting it into visual text with less than a five second delay. Keeping the delay short allows end-users to follow and participate in conversations. This article focuses on the fundamental problem that makes real-time captioning difficult: sequential keyboard typing is much slower than speaking. We first surveyed the audio characteristics of 240 one-hour-long captioned lectures on YouTube, such as speed and duration of speaking bursts. We then analyzed how these characteristics impact caption generation and readability, considering specifically our human-powered collaborative captioning approach. We note that most of these characteristics are also present in more general domains. For our caption comparison evaluation, we transcribed a classroom lecture in real-time using all three captioning approaches. We recruited 48 participants (24 DHH) to watch these classroom transcripts in an eye-tracking laboratory. We presented these captions in a randomized, balanced order. We show that both hearing and DHH participants preferred and followed collaborative captions better than those generated by automatic speech recognition (ASR) or professionals due to the more consistent flow of the resulting captions. These results show the potential to reliably capture speech even during sudden bursts of speed, as well as for generating ""enhanced"" captions, unlike other human-powered captioning approaches. © 2014 ACM.",Crowdsourcing; Deaf; Hard of hearing; Real-time captioning,Accessibility evaluation; Automatic speech recognition; Classroom lecture; Crowdsourcing; Deaf; Hard of hearings; Human-powered; Real-time captioning; Computer applications; Human computer interaction; Audition,Article,Final,Scopus,2-s2.0-84893353120,Peter,,
"Debevc M., Stjepanovič Z., Holzinger A.",56816724400;7003286032;23396282000;,Development and evaluation of an e-learning course for deaf and hard of hearing based on the advanced Adapted Pedagogical Index method,2014,Interactive Learning Environments,22,1,,35,50,,13.0,10.1080/10494820.2011.641673,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892995784&doi=10.1080%2f10494820.2011.641673&partnerID=40&md5=bc80e55e4250e8273308cf2af1855779,"Web-based and adapted e-learning materials provide alternative methods of learning to those used in a traditional classroom. Within the study described in this article, deaf and hard of hearing people used an adaptive e-learning environment to improve their computer literacy. This environment included streaming video with sign language interpreter video and subtitles. The courses were based on the learning management system Moodle, which also includes sign language streaming videos and subtitles. A different approach is required when adapting e-learning courses for the deaf and hard of hearing: new guidelines must be developed concerning the loading and display of video material. This is shown in the example of the e-learning course, ECDL (European Computer Driving Licence). The usability of the e-learning course is analyzed and confirmed using two methods: first, the Software Usability Measurement Inventory (SUMI) evaluation method, and second, the Adapted Pedagogical Index (AdaPI), which was developed as part of this study, and gives an index to measure the pedagogical effectiveness of e-learning courses adapted for people with disabilities. With 116 participants, of whom 22 are deaf or hard of hearing, the e-learning course for the target group has been found suitable and appropriate according to both evaluation methods. © 2012 © 2012 Taylor & Francis.",deaf education; distance learning; learning management system; pedagogical effectiveness; sign language; usability; video streaming; web-based education,,Article,Final,Scopus,2-s2.0-84892995784,Peter,,
Youngblood N.E.,36994960000;,Integrating usability and accessibility into the interactive media and communication curriculum,2013,Global Media Journal,12,23,,,,,3.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888413001&partnerID=40&md5=274394bab12502cd8b55746a7a089058,"One of the challenges of developing an interactive media curriculum is the need to balance production skills, such as coding and using development tools, with visual design and design best practices, such as usability and accessibility. Usability, or how easy it is to use a site, is critical because if a site is difficult to use, users will go elsewhere for the information or service. In addition, poor usability has been demonstrated to undermine overall site credibility. Accessibility is essentially how usable a site is by users with disabilities such as impairments in vision, hearing, cognition, and motor skills. While students need to understand accessibility on a legal level, particularly given the number of lawsuits related to website accessibility, they also need to understand the issue on an ethical level. This article discusses the importance of usability and accessibility and how to integrate these ideas into both interactive media creative classes as well as lecture courses such as media and society and media law courses, and it includes a discussion of sample exercises, sample assignments, and recommended resources.",Accessibility; Curriculum; Teaching; Usability,,Article,Final,Scopus,2-s2.0-84888413001,Peter,,
Seale J.K.,7103266797;,"E-learning and disability in higher education: Accessibility research and practice, second edition",2013,"E-Learning and Disability in Higher Education: Accessibility Research and Practice, Second Edition",,,,1,268,,23.0,10.4324/9780203095942,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905516482&doi=10.4324%2f9780203095942&partnerID=40&md5=f0cf6687a5909030679a1694be423b22,"Most people working within the higher education sector understand the importance of making e-learning accessible to students with disabilities, yet it is not always clear exactly how this should be accomplished. E-Learning and Disability in Higher Education evaluates current accessibility practice and critiques the extent to which 'best' practices can be confidently identified and disseminated. This second edition has been fully updated and includes a focus on research that seeks to give 'voice' to disabled students in a way that provides an indispensible insight into their relationship with technologies and the institutions in which they study. Examining the social, educational, and political background behind making online learning accessible in higher and further education, E-Learning and Disability in Higher Education considers the roles and perspectives of the key stake-holders involved in e-learning: lecturers, professors, instructional designers, learning technologists, student support services, staff developers, and senior managers and administrators. © 2014 Taylor & Francis. All rights reserved.",,,Book,Final,Scopus,2-s2.0-84905516482,Peter,,
"Krejtz I., Szarkowska A., Krejtz K.",54395481500;54416458200;55258716700;,The effects of shot changes on eye movements in subtitling,2013,Journal of Eye Movement Research,6,5,3,,,,31.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894493790&partnerID=40&md5=d2a993b762714cbd49fcc2de21652957,"In this paper we address the question whether shot changes trigger the re-reading of subtitles. Although it has been accepted in the professional literature on subtitling that subtitles should not be displayed over shot changes as they induce subtitle re-reading, support for this claim in eye movement studies is difficult to find. In this study we examined eye movement patterns of 71 participants watching news and documentary clips. We analysed subject hit count, number of fixations, first fixation duration, fixation time percent and transition matrix before, during and after shot changes in subtitles displayed over a shot change. Results of our study show that most viewers do not re-read subtitles crossing shot changes.",Deaf and hard of hearing viewers; Eye movements; Shot changes; Subtitling; Transition matrix,,Article,Final,Scopus,2-s2.0-84894493790,Peter,,
"Kacorri H., Harper A., Huenerfauth M.",35487798800;55926567500;12240800100;,Comparing native signers' perception of American Sign Language animations and videos via eye tracking,2013,"Proceedings of the 15th International ACM SIGACCESS Conference on Computers and Accessibility, ASSETS 2013",,,9,,,,7.0,10.1145/2513383.2513441,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887458139&doi=10.1145%2f2513383.2513441&partnerID=40&md5=c02db48d9b541be5103299cd6a35e1c3,"Animations of American Sign Language (ASL) have accessibility benefits for signers with lower written-language literacy. Our lab has conducted prior evaluations of synthesized ASL animations: asking native signers to watch different versions of animations and answer comprehension and subjective questions about them. Seeking an alternative method of measuring users' reactions to animations, we are now investigating the use of eye tracking to understand how users perceive our stimuli. This study quantifies how the eye gaze of native signers varies when they view: videos of a human ASL signer or synthesized animations of ASL (of different levels of quality). We found that, when viewing videos, signers spend more time looking at the face and less frequently move their gaze between the face and body of the signer. We also found correlations between these two eye-tracking metrics and participants' responses to subjective evaluations of animation-quality. This paper provides methodological guidance for how to design user studies evaluating sign language animations that include eye tracking, and it suggests how certain eye-tracking metrics could be used as an alternative or complimentary form of measurement in evaluation studies of sign language animation.",Accessibility technology for people who are deaf; American Sign Language; Animation; Eye tracking; User study,Accessibility technology for people who are deaf; American sign language; Evaluation study; Eye-gaze; Eye-tracking; Sign language; Subjective evaluations; User study; Animation; Quality control,Conference Paper,Final,Scopus,2-s2.0-84887458139,Peter,,
"Zekveld A.A., George E.L.J., Houtgast T., Kramer S.E.",8641362500;14831336800;7003507966;7401609154;,Cognitive abilities relate to self-reported hearing disability,2013,"Journal of Speech, Language, and Hearing Research",56,5,,1364,1372,,24.0,10.1044/1092-4388(2013/12-0268),https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886659200&doi=10.1044%2f1092-4388%282013%2f12-0268%29&partnerID=40&md5=f70fe0de116646f33bde6cfe3886f231,"Purpose: In this explorative study, the authors investigated the relationship between auditory and cognitive abilities and self-reported hearing disability. Method: Thirty-two adults with mild to moderate hearing loss completed the Amsterdam Inventory for Auditory Disability and Handicap (AIADH; Kramer, Kapteyn, Festen, & Tobi, 1996) and performed the Text Reception Threshold (TRT; Zekveld, George, Kramer, Goverts, & Houtgast, 2007) test as well as tests of spatial working memory (SWM) and visual sustained attention. Regression analyses examined the predictive value of age, hearing thresholds (pure-tone averages [PTAs]), speech perception in noise (speech reception thresholds in noise [SRTNs]), and the cognitive tests for the 5 AIADH factors. Results: Besides the variance explained by age, PTA, and SRTN, cognitive abilities were related to each hearing factor. The reported difficulties with sound detection and speech perception in quiet were less severe for participants with higher age, lower PTAs, and better TRTs. Fewer sound localization and speech perception in noise problems were reported by participants with better SRTNs and smaller SWM. Fewer sound discrimination difficulties were reported by subjects with better SRTNs and TRTs and smaller SWM. Conclusions: The results suggest a general role of the ability to read partly masked text in subjective hearing. Large working memory was associated with more reported hearing difficulties. This study shows that besides auditory variables and age, cognitive abilities are related to self-reported hearing disability. © American Speech-Language-Hearing Association.",Cognition; Hearing loss; Speech perception; Speech recognition,"aged; article; attention; auditory stimulation; auditory threshold; cognition; female; hearing impairment; human; male; methodology; middle aged; noise; pathophysiology; patient; perception; perception deafness; physiology; psychological aspect; reading; regression analysis; self report; short term memory; speech audiometry; speech discrimination; speech perception; very elderly; cognition; hearing loss; speech perception; speech recognition; Acoustic Stimulation; Aged; Aged, 80 and over; Attention; Auditory Threshold; Cognition; Female; Hearing Loss, Sensorineural; Humans; Male; Memory, Short-Term; Middle Aged; Noise; Perceptual Masking; Persons With Hearing Impairments; Reading; Regression Analysis; Self Report; Speech Perception; Speech Reception Threshold Test",Article,Final,Scopus,2-s2.0-84886659200,Peter,,
"Elliot L.B., Rubin B., DeCaro J.J., William Clymer E., Earp K., Fish M.D.",7003340239;57158630000;6603549343;57160205600;57159988500;57159158900;,Creating a virtual academic community for STEM students,2013,Journal of Applied Research in Higher Education,5,2,,173,188,,6.0,10.1108/JARHE-11-2012-0051,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960125224&doi=10.1108%2fJARHE-11-2012-0051&partnerID=40&md5=ae9d65e61da5b1f2aac9adff36950365,"Purpose – The purpose of this paper is to describe synchronous, remote tutoring for the Deaf STEM Community Alliance's virtual academic community (VAC). The alliance addresses critical barriers for students who are deaf or hard of hearing (D/HH) in postsecondary science, technology, engineering, and mathematics (STEM) majors. Design/methodology/approach – A mixed-method approach (qualitative content analysis and descriptive statistics) documents project activities. Findings – Google+ Hangouts was used for remote tutoring. Participants completed 57 tutoring sessions. Participants found tutoring beneficial, especially for its convenience. Technical assistance and feedback systems were created to support participants. Grade point averages (GPA) and retention remained stable. Research limitations/implications – Research on this project continues. Small sample size is a limitation of the study. Ongoing research investigates how remote technology and social media impact learning for students who are D/HH. Practical implications – Scholarship on social media for educational purposes is minimal. While specifics of particular social media platforms vary, recruitment, technical assistance, and establishing feedback mechanisms are common issues for VACs. Outcomes from this study will be used to improve this VAC and create documentation for replication. Social implications – The Deaf STEM Community Alliance provides supportive resources to underrepresented students in STEM majors. Improved GPA and retention in STEM majors will generate more individuals qualified for STEM careers. Research on VACs creates opportunities to understand how technology and networked communities change knowledge and learning. Originality/value – The Deaf STEM Community Alliance is a unique project for postsecondary students in STEM fields who are D/HH. The information is valuable to educators interested in using social media for instruction. © 2013, © Emerald Group Publishing Limited.",Deaf or hard of hearing students; Deaf people; Postsecondary STEM education; Remote tutoring; Social media; Synchronous tutoring,,Article,Final,Scopus,2-s2.0-84960125224,Peter,,
"Sacks L., Nakaji M., Harry K.M., Oen M., Malcarne V.L., Sadler G.R.",57193203833;25522295300;48861419000;55774245600;6701495867;7005152183;,Testicular cancer knowledge among deaf and hearing men,2013,Journal of Cancer Education,28,3,,503,508,,18.0,10.1007/s13187-013-0493-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893677607&doi=10.1007%2fs13187-013-0493-x&partnerID=40&md5=865b417a6f9110dbe004a166f7f6e467,"Testicular cancer typically affects young and middle-aged men. An educational video about prostate and testicular cancer was created in American Sign Language, with English open captioning and voice overlay, so that it could be viewed by audiences of diverse ages and hearing characteristics. This study recruited young Deaf (n=85) and hearing (n=90) adult males to help evaluate the educational value of the testicular cancer portion of this video. Participants completed surveys about their general, testicular, and total cancer knowledge before and after viewing the video. Although hearing men had higher pre-test scores than Deaf men, both Deaf and hearing men demonstrated significant increases in General, Testicular, and Total Cancer Knowledge scores after viewing the intervention video. Overall, results demonstrate the value of the video to Deaf and hearing men. © Springer Science+Business Media New York 2013.",American Sign Language; Deaf; Prostate cancer knowledge; Testicular cancer knowledge,"adult; article; attitude to health; case control study; communication disorder; comparative study; deaf education; follow up; health care delivery; health education; human; male; patient; prognosis; questionnaire; sign language; testis tumor; videorecording; young adult; Testicular Neoplasms; Adult; Case-Control Studies; Communication Barriers; Education of Hearing Disabled; Follow-Up Studies; Health Education; Health Knowledge, Attitudes, Practice; Health Services Accessibility; Humans; Male; Persons With Hearing Impairments; Prognosis; Questionnaires; Sign Language; Testicular Neoplasms; Video Recording; Young Adult; Adult; Case-Control Studies; Communication Barriers; Education of Hearing Disabled; Follow-Up Studies; Health Education; Health Knowledge, Attitudes, Practice; Health Services Accessibility; Humans; Male; Persons With Hearing Impairments; Prognosis; Questionnaires; Sign Language; Testicular Neoplasms; Video Recording; Young Adult",Article,Final,Scopus,2-s2.0-84893677607,Peter,,
"Foulsham T., Sanderson L.A.",8983741300;55882470600;,Look who's talking? Sound changes gaze behaviour in a dynamic social scene,2013,Visual Cognition,21,7,,922,944,,22.0,10.1080/13506285.2013.849785,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889239104&doi=10.1080%2f13506285.2013.849785&partnerID=40&md5=a3f2a61e4af20a49bb078ddbb4383925,"Humans often look at other people in natural scenes, and previous research has shown that these looks follow the conversation and that they are sensitive to sound in audiovisual speech perception. In the present experiment, participants viewed video clips of four people involved in a discussion. By removing the sound, we asked whether auditory information would affect when speakers were fixated, how fixations between different observers were synchronized, and whether the eyes or mouth were looked at most often. The results showed that sound changed the timing of looks-by alerting observers to changes in conversation and attracting attention to the speaker. Clips with sound also led to greater attentional synchrony, with more observers fixating the same regions at the same time. However, looks towards the eyes of the people continued to dominate and were unaffected by removing the sound. These findings provide a rich example of multimodal social attention. © 2013 Taylor & Francis.",Eye movements; Scene perception; Social attention; Speech perception,adult; article; binocular convergence; conversation; eye fixation; eye position; eye tracking; facies; female; gaze; human; lip reading; male; normal human; priority journal; saccadic eye movement; selective attention; sound; speech; speech perception; videorecording,Article,Final,Scopus,2-s2.0-84889239104,Peter,,
"Szarkowska A., Zbikowska J., Krejtz I.",54416458200;55602697700;54395481500;,Subtitling for the deaf and hard of hearing in multilingual films,2013,International Journal of Multilingualism,10,3,,292,312,,18.0,10.1080/14790718.2013.766195,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880040303&doi=10.1080%2f14790718.2013.766195&partnerID=40&md5=74881ec94b38757bd9d0b7054f75df58,"In this article, we examine different ways in which multilingualism can be manifested in subtitling for the deaf and hard of hearing (SDH). First, we propose a set of strategies to tackle multilingualism in SDH. We then present the results of an online survey conducted among deaf and hard of hearing Poles (N=135) on their preferences regarding the SDH strategies. Our results show that participants prefer more informative strategies, such as vehicular matching and explicit attribution, rather than less informative ones, such as linguistic homogenisation. The results also indicate that current SDH practices could be improved by using a more varied set of strategies, especially by including foreign language text in the subtitles, so that multilingualism present in films is better reflected in SDH. © 2013 Taylor & Francis.",closed captions; film; hearing impaired; multilingualism; subtitling; subtitling for the deaf and hard of hearing,,Article,Final,Scopus,2-s2.0-84880040303,Peter,,
"Cambra C., Silvestre N., Leal A.",7801358308;15137439300;7004298024;,Interpretation of an audiovisual documentary by deaf and hearing adolescents: The importance of subtitling,2013,"Revista de Logopedia, Foniatria y Audiologia",33,3,,99,108,,1.0,10.1016/j.rlfa.2012.12.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881547825&doi=10.1016%2fj.rlfa.2012.12.003&partnerID=40&md5=509f5b9f77411f08258d46e8f582d3ee,"This study investigated types of expository texts, which have been less explored than narrative texts. The main objective was to analyze the interpretation of an audiovisual documentary by 2. groups of adolescents, one group of 20. hearing adolescents and another group of 20. deaf adolescents aged 12 to 19. years, and to assess the contribution of subtitles to the interpretations of the group with hearing loss. The documentary contained material on the social sciences, geography and history curricula required by compulsory secondary education. After viewing the documentary, the participants were asked to list its contents. The results showed significant differences in the interpretation of the documentary, both in the lower number of contents noted by deaf adolescents and in the type of content that most attracted their attention. These differences disappeared when the deaf adolescents viewed the film with subtitles. © 2012 AELFA.",Adolescence; Audiovisual documentary; Deafness; Expository text; Subtitles,,Article,Final,Scopus,2-s2.0-84881547825,Peter,,
"Lasecki W.S., Miller C.D., Bigham J.P.",54383728900;57198933938;16238221500;,Warping time for more effective real-time crowdsourcing,2013,Conference on Human Factors in Computing Systems - Proceedings,,,,2033,2036,,42.0,10.1145/2470654.2466269,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877983946&doi=10.1145%2f2470654.2466269&partnerID=40&md5=bfc3d41a7177fd9366bf8ed7feb0e92f,"In this paper, we introduce the idea of ""warping time"" to improve crowd performance on the difficult task of captioning speech in real-time. Prior work has shown that the crowd can collectively caption speech in real-time by merging the partial results of multiple workers. Because non-expert workers cannot keep up with natural speaking rates, the task is frustrating and prone to errors as workers buffer what they hear to type later. The TimeWarp approach automatically increases and decreases the speed of speech playback systematically across individual workers who caption only the periods played at reduced speed. Studies with 139 remote crowd workers and 24 local participants show that this approach improves median coverage (14.8%), precision (11.2%), and per-word latency (19.1%). Warping time may also help crowds outperform individuals on other difficult real-time performance tasks. Copyright © 2013 ACM.",Captioning; Human computation; Real-time crowdsourcing,Captioning; Crowdsourcing; Human computation; Natural speaking; Partial results; Real time performance; Human engineering; Human computer interaction,Conference Paper,Final,Scopus,2-s2.0-84877983946,Peter,,
"Rajendran D.J., Duchowski A.T., Orero P., Martínez J., Romero-Fresco P.",55587907600;6701824388;24921771100;55587631600;36675476200;,Effects of text chunking on subtitling: A quantitative and qualitative examination,2013,Perspectives: Studies in Translatology,21,1,,5,21,,40.0,10.1080/0907676X.2012.722651,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873691299&doi=10.1080%2f0907676X.2012.722651&partnerID=40&md5=fe0ba3e0e8c8d39655bb357cc5d540ed,"Our work serves as an assay of the visual impact of text chunking on live (respoken) subtitles. We evaluate subtitles constructed with different chunking methods to determine whether segmentation influences comprehension or otherwise affects the viewing experience. Disparities in hearing participants' recorded eye movements over four styles of subtitling suggest that chunking reduces the amount of time spent reading subtitles. © 2013 Copyright Taylor and Francis Group, LLC.",eye tracking; human factors; subtitling,,Article,Final,Scopus,2-s2.0-84873691299,Peter,,
"Ammirante P., Russo F.A., Good A., Fels D.I.",35193448600;8835880000;55559439200;57218572233;,Feeling Voices,2013,PLoS ONE,8,1,e53585,,,,13.0,10.1371/journal.pone.0053585,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872441993&doi=10.1371%2fjournal.pone.0053585&partnerID=40&md5=143290543579170192aca0f94ea7af14,"Two experiments investigated deaf individuals' ability to discriminate between same-sex talkers based on vibrotactile stimulation alone. Nineteen participants made same/different judgments on pairs of utterances presented to the lower back through voice coils embedded in a conforming chair. Discrimination of stimuli matched for F0, duration, and perceived magnitude was successful for pairs of spoken sentences in Experiment 1 (median percent correct = 83%) and pairs of vowel utterances in Experiment 2 (median percent correct = 75%). Greater difference in spectral tilt between ""different"" pairs strongly predicted their discriminability in both experiments. The current findings support the hypothesis that discrimination of complex vibrotactile stimuli involves the cortical integration of spectral information filtered through frequency-tuned skin receptors. © 2013 Ammirante et al.",,acoustics; adult; article; auditory stimulation; brain cortex; controlled study; decision making; female; frequency analysis; hearing impairment; human; human experiment; male; normal human; sensory stimulation; skin receptor; speech discrimination; speech perception; stimulus response; vibrotactile stimulation; Acoustic Stimulation; Adult; Female; Humans; Male; Middle Aged; Sound Spectrography; Speech Perception; Voice; Young Adult,Article,Final,Scopus,2-s2.0-84872441993,Peter,,
"Naim I., Gildea D., Lasecki W., Bigham J.P.",36629064700;6603603942;54383728900;16238221500;,Text Alignment for Real-Time Crowd Captioning,2013,"Proceedings of the 2nd Workshop on Computational Linguistics for Literature, CLfL 2013 at the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2013",,,,201,210,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121734817&partnerID=40&md5=937cf11a1f18a57c366e284d704e8ece,"The primary way of providing real-time captioning for deaf and hard of hearing people is to employ expensive professional stenographers who can type as fast as natural speaking rates. Recent work has shown that a feasible alternative is to combine the partial captions of ordinary typists, each of whom types part of what they hear. In this paper, we describe an improved method for combining partial captions into a final output based on weighted A∗ search and multiple sequence alignment (MSA). In contrast to prior work, our method allows the tradeoff between accuracy and speed to be tuned, and provides formal error bounds. Our method outperforms the current state-of-the-art on Word Error Rate (WER) (29.6%), BLEU Score (41.4%), and F-measure (36.9%). The end goal is for these captions to be used by people, and so we also compare how these metrics correlate with the judgments of 50 study participants, which may assist others looking to make further progress on this problem. © 2013 Association for Computational Linguistics.",,Audition; Speech recognition; 'current; Error bound; Feasible alternatives; Hard of hearings; Multiple sequence alignments; Natural speaking; Real- time; Speaking rate; State of the art; Text alignments; Error analysis,Conference Paper,Final,Scopus,2-s2.0-85121734817,Peter,,
"O'Hagan M., Mangiron C.",18233953800;55587445400;,Game localization,2013,Benjamins Translation Library,106,,,1,388,,1.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105662353&partnerID=40&md5=1a1d0039e6046cad0b380e674c3dbea2,[No abstract available],,,Book Chapter,Final,Scopus,2-s2.0-85105662353,Peter,,
Gass S.M.,7005403470;,"Second language acquisition: An introductory course, fourth edition",2013,"Second Language Acquisition: An Introductory Course, Fourth Edition",,,,1,623,,14.0,10.4324/9780203137093,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085446093&doi=10.4324%2f9780203137093&partnerID=40&md5=369dd8f1da7d1a367ace828f35a7b7bf,"Now in a fourth edition, this bestselling introductory textbook remains the cornerstone volume for the study of second language acquisition (SLA). Its chapters have been fully updated, and reorganized where appropriate, to provide a comprehensive yet accessible overview of the field and its related disciplines. To reflect current developments, new sections on using learner corpora, semantics and morphosyntax (within formal approaches to SLA), sociocultural approaches, gesture, priming research, and chaos theory have been added. Students will also find expanded discussions of heritage language learning, bilingualism, pragmatics, and much more. The redesigned fourth edition of Second Language Acquisition retains the features that students found useful in the current edition but also provides new pedagogical tools that encourage students to reflect upon the experiences of second language learners. As with previous editions, discussion questions and problems at the end of each chapter help students apply their knowledge, and a glossary defines and reinforces must-know terminology. This clearly-written, comprehensive, and current textbook, by expert Sue Gass, is the ideal textbook for the introductory SLA course in second language studies, applied linguistics, linguistics, TESOL, and language education programs. © 2013 Taylor & Francis.",,,Book,Final,Scopus,2-s2.0-85085446093,Peter,,
"Cambra C., Leal A., Silvestre N.",7801358308;7004298024;15137439300;,The interpretation and visual attention of hearing impaired children when watching a subtitled cartoon1,2013,Journal of Specialised Translation,,20,,134,146,,7.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937677512&partnerID=40&md5=95f5fad49947d514a356d1a93b126179,"The article analyses how hearing impaired children interpret a subtitled cartoon, taking into account their visual attention to both sources of information (the images and the subtitles), and the auditory information from the oral language. The sample was made up of eleven children, aged seven to eleven years old (second to fifth grade of elementary school), who attend ordinary mainstream schools in Barcelona in the oral mode. A narrative cartoon with subtitles in Catalan was used in the study. The eye movement of the subjects was also analysed using an eye-tracker as the subjects watched the audiovisual story. After seeing it, they were asked to orally retell the cartoon story. The results reveal the importance of having time to view the images to interpret the story well. The eye-tracker shows that the hearing impaired children who participated in the study maintained attention on the frames with subtitles, with the exception of the participants in second grade with lower reading skills who did not pay attention to the subtitles towards the end of the cartoon when the conflict of the story was resolved. © 2013 University of Roehampton. All rights reserved.",Accessibility; Cartoon programme; Eye-tracker; Hearing impairment; Subtitles; Television,,Review,Final,Scopus,2-s2.0-84937677512,Peter,,
Rost M.,56409591400;,"Teaching and researching listening, second edition",2013,"Teaching and Researching Listening, Second Edition",,,,1,407,,8.0,10.4324/9781315833705,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909145238&doi=10.4324%2f9781315833705&partnerID=40&md5=146e53e5ceac405a14b11c6588c1c578,"Teaching and Researching Listening provides a focused, state-of-the-art treatment of the linguistic, psycholinguistic and pragmatic processes that are involved in oral language use, and shows how these processes influence listening in a range of practical contexts. Through understanding the interaction between these processes, language educators and researchers can develop more robust research methods and more effective classroom language teaching approaches.In this fully revised and updated second edition, the book: · examines a full range of teaching methods and research initiatives related to listening · gives definitions of key concepts in neurolinguistics and psycholinguistics · provides a clear agenda for implementing listening strategies and designing tests · offers an abundance of resources for immediate use for teaching and research.Featuring insightful quotes and concept boxes, chapter overviews and summaries to guide the reader, Teaching and Researching Listening will engage and inform teachers, teacher trainers and researchers investigating communicative language use. © 2002, 2011, Taylor & Francis.",,,Book,Final,Scopus,2-s2.0-84909145238,Peter,,
"Naim I., Gildea D., Lasecki W., Bigham J.P.",36629064700;6603603942;54383728900;16238221500;,Text alignment for real-time crowd captioning,2013,"NAACL HLT 2013 - 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Main Conference",,,,201,210,,17.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901371496&partnerID=40&md5=61d0e50a7a2171db1dbc15e034129a5d,"The primary way of providing real-time captioning for deaf and hard of hearing people is to employ expensive professional stenographers who can type as fast as natural speaking rates. Recent work has shown that a feasible alternative is to combine the partial captions of ordinary typists, each of whom types part of what they hear. In this paper, we describe an improved method for combining partial captions into a final output based on weighted A∗ search and multiple sequence alignment (MSA). In contrast to prior work, our method allows the tradeoff between accuracy and speed to be tuned, and provides formal error bounds. Our method outperforms the current state-of-The-Art onWord Error Rate (WER) (29.6%), BLEU Score (41.4%), and F-measure (36.9%). The end goal is for these captions to be used by people, and so we also compare how these metrics correlate with the judgments of 50 study participants, which may assist others looking to make further progress on this problem. © 2013 Association for Computational Linguistics.",,Audition; Error analysis; Bleu scores; Error bound; Feasible alternatives; Hard of hearings; Multiple sequence alignments; Natural speaking; State of the art; Text alignments; Computational linguistics,Conference Paper,Final,Scopus,2-s2.0-84901371496,Peter,,
"Yeratziotis G., Van Greunen D.",56031333800;10046346400;,Making ICT accessible for the deaf,2013,"2013 IST-Africa Conference and Exhibition, IST-Africa 2013",,,6701722,,,,17.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893807810&partnerID=40&md5=275e2b0ccd311129ebe2ffbd96e9f202,"Access to information and the ability to share information online, must not be a privilege only for the non-disabled. Making ICTs accessible for the disabled must be a priority as it can contribute toward reducing the digital divide and providing e-inclusion. To accomplish this, one must keep in mind not only disabled people's disabilities but also their abilities. Topics such as user interface design for the Deaf, e-learning multimedia, sign language and literacy problems that hearing impaired are faced with, are discussed in this paper. The primary purpose of the paper however is to identify appropriate user experience and user interface design guidelines for the design of mobile applications for Deaf people. © 2013 The Authors.",Accessibility; Deaf Community in South Africa; Disability; Early Childhood Policies; Hearing Loss; Interface Design; Sign Language,Audition; E-learning; Exhibitions; User interfaces; Accessibility; Disability; Early childhoods; Hearing loss; Interface designs; Sign language; South Africa; Handicapped persons,Conference Paper,Final,Scopus,2-s2.0-84893807810,Peter,,
"Moreland C.J., Latimore D., Sen A., Arato N., Zazove P.",36572291300;55535858600;55443624400;55535523800;6701847250;,Deafness among physicians and trainees: A national survey,2013,Academic Medicine,88,2,,224,232,,28.0,10.1097/ACM.0b013e31827c0d60,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873375482&doi=10.1097%2fACM.0b013e31827c0d60&partnerID=40&md5=0ec7c4fbcbd03e0312e7ea78aadf1930,"Purpose: To describe the characteristics of and accommodations used by the deaf and hard-of-hearing (DHoH) physician and trainee population and examine whether these individuals are more likely to care for DHoH patients. METHOD: Multipronged snowball sampling identified 86 potential DHoH physician and trainee participants. In July to September 2010, a Web-based survey investigated accommodations used by survey respondents. The authors analyzed participants' demographics, accommodation and career satisfaction, sense of institutional support, likelihood of recommending medicine as a career, and current/anticipated DHoH patient population size. RESULTS: The response rate was 65% (56 respondents; 31 trainees and 25 practicing physicians). Modified stethoscopes were the most frequently used accommodation (n = 50; 89%); other accommodations included auditory equipment, note-taking, computer-assisted real-time captioning, signed interpretation, and oral interpretation. Most respondents reported that their accommodations met their needs well, although 2 spent up to 10 hours weekly arranging accommodations. Of 25 physicians, 17 reported primary care specialties; 7 of 31 trainees planned to enter primary care specialties. Over 20% of trainees anticipated working with DHoH patients, whereas physicians on average spent 10% of their time with DHoH patients. Physicians' accommodation satisfaction was positively associated with career satisfaction and recommending medicine as a career. CONCLUSIONS: DHoH physicians and trainees seemed satisfied with frequent, multimodal accommodations from employers and educators. These results may assist organizations in planning accommodation provisions. Because DHoH physicians and trainees seem interested in primary care and serving DHoH patients, recruiting and training DHoH physicians has implications for the care of this underserved population.",,,Review,Final,Scopus,2-s2.0-84873375482,Peter,,
"Forman I.R., Fletcher B., Hartley J., Rippon B., Wilson A.",6603526154;55497065100;57197977216;55497998800;55458758700;,Blue herd: Automated captioning for videoconferences,2012,ASSETS'12 - Proceedings of the 14th International ACM SIGACCESS Conference on Computers and Accessibility,,,,227,228,,3.0,10.1145/2384916.2384966,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869774019&doi=10.1145%2f2384916.2384966&partnerID=40&md5=24f492f8054bc5155e4116914c7b4152,"Blue Herd is a project in IBM Research to investigate automated captioning for videoconferences. Today videoconferences are held among meeting participants connected with a variety of devices: personal computers, mobile devices, and multi-participant meeting rooms. Blue Herd is charged with studying automated real-time captioning in that context. This poster explains the system that was developed for personal computers and describes our experiments to include mobile devices and multi-participant meeting rooms.",Accessibility; Automated captioning; Speech recognition; Transcriptions,Accessibility; Automated captioning; Mobile devices; Personal computers; Speech recognition; Transcription; Automation,Conference Paper,Final,Scopus,2-s2.0-84869774019,Peter,,
"Debevc M., Kožuh I., Kosec P., Rotovnik M., Holzinger A.",56816724400;54412464700;23397543000;35280916500;23396282000;,Sign language multimedia based interaction for aurally handicapped people,2012,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),7383 LNCS,PART 2,,213,220,,7.0,10.1007/978-3-642-31534-3_33,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864823109&doi=10.1007%2f978-3-642-31534-3_33&partnerID=40&md5=b0d496b7de01a46ed574685dfb6fb41b,"People with hearing disabilities still do not have a satisfactory access to Internet services. Since sign language is the mother tongue of deaf people, and 80% of this social group cannot successfully understand the written content, different ways of using sign language to deliver information via the Internet should be considered. In this paper, we provide a technical overview of solutions to this problem that we have designed and tested in recent years, along with the evaluation results and users' experience reports. The solutions discussed prioritize sign language on the Internet for the deaf and hard of hearing using a multimodal approach to delivering information, including video, audio and captions. © 2012 Springer-Verlag.",deaf; hard of hearing; sign language; sign language glossary; SLI module; VELAP,deaf; Evaluation results; Experience report; Internet services; Multi-modal approach; Sign language; SLI module; Social groups; VELAP; Audition; Internet; Handicapped persons,Conference Paper,Final,Scopus,2-s2.0-84864823109,Peter,,
"Rodriguez-Alsina A., Talavera G., Orero P., Carrabina J.",36185982300;18038945600;24921771100;6602465710;,Subtitle synchronization across multiple screens and devices,2012,Sensors (Switzerland),12,7,,8710,8731,,11.0,10.3390/s120708710,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864353967&doi=10.3390%2fs120708710&partnerID=40&md5=7751d5fcac3eb437c268a7621ca99923,"Ambient Intelligence is a new paradigm in which environments are sensitive and responsive to the presence of people. This is having an increasing importance in multimedia applications, which frequently rely on sensors to provide useful information to the user. In this context, multimedia applications must adapt and personalize both content and interfaces in order to reach acceptable levels of context-specific quality of service for the user, and enable the content to be available anywhere and at any time. The next step is to make content available to everybody in order to overcome the existing access barriers to content for users with specific needs, or else to adapt to different platforms, hence making content fully usable and accessible. Appropriate access to video content, for instance, is not always possible due to the technical limitations of traditional video packaging, transmission and presentation. This restricts the flexibility of subtitles and audio-descriptions to be adapted to different devices, contexts and users. New Web standards built around HTML5 enable more featured applications with better adaptation and personalization facilities, and thus would seem more suitable for accessible AmI environments. This work presents a video subtitling system that enables the customization, adaptation and synchronization of subtitles across different devices and multiple screens. The benefits of HTML5 applications for building the solution are analyzed along with their current platform support. Moreover, examples of the use of the application in three different cases are presented. Finally, the user experience of the solution is evaluated. © 2012 by the authors; licensee MDPI, Basel, Switzerland.",Accessibility; Ambient Intelligence; HTML5; Multimedia synchronization; SMIL; Subtitling,,Article,Final,Scopus,2-s2.0-84864353967,Peter,,
"Jiang X., Zhou X.",55724152800;36116357700;,Multiple semantic processes at different levels of syntactic hierarchy: Does the higher-level process proceed in face of a lower-level failure?,2012,Neuropsychologia,50,8,,1918,1928,,15.0,10.1016/j.neuropsychologia.2012.04.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862840109&doi=10.1016%2fj.neuropsychologia.2012.04.016&partnerID=40&md5=f7ae21890b386f5f9dca160cdf8fc035,"Humans have special abilities in processing hierarchical, recursive structures. Here we investigated how an upcoming word embedded in a hierarchical structure is semantically integrated into the prior representation during sentence comprehension. Participants read Chinese sentences with a complex verb argument structure ""subject noun+verb+numeral+classifier+object noun"", in which the object noun was constrained by the classifier in a local structure and by the verb in a higher-level structure. The semantic congruence between the classifier and the noun, between the verb and the noun, and between the verb and the classifier was manipulated individually or simultaneously to create a local mismatch (i.e., with classifier-noun mismatch), a sequential mismatch (with verb-classifier and classifier-noun mismatches) or a triple mismatch (with verb-classifier, classifier-noun, and verb-classifier mismatches) condition. Event-related potentials locked to the object noun showed increased N400 and late negativity responses over the local mismatch, the sequential mismatch and the triple mismatch conditions. The local mismatch additionally elicited a posterior positivity effect on the object noun. The verb-classifier mismatch elicited a right N400-like effect followed by a posterior positivity (P600) effect on the classifier. The N400 effects demonstrate that the semantic process at a higher syntactic level can proceed in face of the failure of semantic processes at lower levels when no structural re-interpretation is available, and that the semantic congruence between earlier sentence constituents can affect the integration of the upcoming word in the hierarchical structure. The P600 effects suggest the immediate triggering of a co-ordination process across syntactic levels whereas the late anterior negativity effects suggest the initiation of a second-pass semantic re-interpretation process. © 2012 Elsevier Ltd.",Coordination process; ERP; Hierarchical structure; Late negativity; Local priority; N400; P600; Semantic process,adult; article; classifier; cognition; comprehension; controlled study; coordination; event related potential; evoked response; female; human; local mismatch; male; mismatch negativity; semantics; sequential mismatch; triple mismatch; Adolescent; Adult; Comprehension; Electroencephalography; Evoked Potentials; Female; Humans; Male; Reading; Semantics,Article,Final,Scopus,2-s2.0-84862840109,Peter,,
"Baldwin E.E., Boudreault P., Fox M., Sinsheimer J.S., Palmer C.G.S.",15834226500;14049630100;56574660300;7006292467;57191587872;,Effect of pre-test genetic counseling for deaf adults on knowledge of genetic testing,2012,Journal of Genetic Counseling,21,2,,256,272,,17.0,10.1007/s10897-011-9398-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863613382&doi=10.1007%2fs10897-011-9398-1&partnerID=40&md5=9df59d073a0d0954af4d8d31fc1f61a2,"Empirical data on genetic counseling outcomes in the deaf population are needed to better serve this population. This study was an examination of genetics knowledge before and after culturally and linguistically appropriate pre-test genetic counseling in a diverse deaf adult sample. Individuals ≥18 years old with early-onset sensorineural deafness were offered connexin-26/30 testing and genetic counseling. Participants completed questionnaires containing 10 genetics knowledge items at baseline and following pre-test genetic counseling. The effects of genetic counseling, prior beliefs about etiology, and participant's preferred language on genetics knowledge scores were assessed (n=244). Pre-test genetic counseling (p=.0007), language (p<.0001), prior beliefs (p<.0001), and the interaction between counseling and beliefs (p=.035) were predictors of genetics knowledge. American Sign Language (ASL)-users and participants with ""non-genetic/unknown"" prior beliefs had lower knowledge scores than English-users and participants with ""genetic"" prior beliefs, respectively. Genetics knowledge improved after genetic counseling regardless of participants' language; knowledge change was greater for the ""non-genetic/unknown"" beliefs group than the ""genetic"" beliefs group. ASL-users' lower knowledge scores are consistent with evidence that ethnic and cultural minority groups have less genetics knowledge, perhaps from exposure and access disparities. Culturally and linguistically appropriate pre-test genetic counseling significantly improved deaf individuals' genetics knowledge. Assessing deaf individuals' prior beliefs is important for enhancing genetics knowledge. © The Author(s) 2011.",American Sign Language; Connexin-26; Deaf; Deafness; Genetic counseling; Genetic testing; Genetics knowledge; GJB2; Minority,"connexin 26; gap junction protein; GJB6 protein, human; adult; article; female; genetic screening; genetics; hearing impairment; human; male; middle aged; psychological aspect; Adult; Connexins; Deafness; Female; Genetic Testing; Humans; Male; Middle Aged",Article,Final,Scopus,2-s2.0-84863613382,Peter,,
"Koelewijn T., Zekveld A.A., Festen J.M., Kramer S.E.",22985390400;8641362500;7005417075;7401609154;,Pupil dilation uncovers extra listening effort in the presence of a single-talker masker,2012,Ear and Hearing,33,2,,291,300,,137.0,10.1097/AUD.0b013e3182310019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857915571&doi=10.1097%2fAUD.0b013e3182310019&partnerID=40&md5=4436f4f6c7d4fb7d88e7f96c2b3d25ae,"Objectives: Recent research has demonstrated that pupil dilation, a measure of mental effort (cognitive processing load), is sensitive to differences in speech intelligibility. The present study extends this outcome by examining the effects of masker type and age on the speech reception threshold (SRT) and mental effort. Design: In young and middle-aged adults, pupil dilation was measured while they performed an SRT task, in which spoken sentences were presented in stationary noise, fluctuating noise, or together with a singletalker masker. The masker levels were adjusted to achieve 50% or 84% sentence intelligibility. Results: The results show better SRTs for fluctuating noise and a single-talker masker compared with stationary noise, which replicates results of previous studies. The peak pupil dilation, reflecting mental effort, was larger in the single-interfering speaker condition compared with the other masker conditions. Remarkably, in contrast to the thresholds, no differences in peak dilation were observed between fluctuating noise and stationary noise. This effect was independent of the intelligibility level and age. Conclusions: To maintain similar intelligibility levels, participants needed more mental effort for speech perception in the presence of a single-talker masker and then with the two other types of maskers. This suggests an additive interfering effect of speech information from the single-talker masker. The dissociation between these performance and mental effort measures underlines the importance of including measurements of pupil dilation as an independent index of mental effort during speech processing in different types of noisy environments and at different intelligibility levels. Copyright © 2012 by Lippincott Williams &Wilkins.",,"adolescent; adult; article; auditory stimulation; auditory threshold; female; hearing; human; male; mental function; methodology; middle aged; noise; perception; physiology; psychophysics; pupil reflex; pure tone audiometry; speech intelligibility; speech perception; Acoustic Stimulation; Adolescent; Adult; Audiometry, Pure-Tone; Auditory Threshold; Female; Hearing; Humans; Male; Mental Processes; Middle Aged; Noise; Perceptual Masking; Psychoacoustics; Reflex, Pupillary; Speech Intelligibility; Speech Perception; Young Adult",Article,Final,Scopus,2-s2.0-84857915571,Peter,,
"Carberry S., Schwartz S.E., McCoy K., Demir S., Wu P., Greenbacker C., Chester D., Schwartz E., Oliver D., Moraes P.",7004588341;55860925600;55408819600;14044928200;57190867646;55081970100;26643146000;35596559600;57196888148;24768302900;,Access to multimodal articles for individuals with sight impairments,2012,ACM Transactions on Interactive Intelligent Systems,2,4,21,,,,13.0,10.1145/2395123.2395126,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983574296&doi=10.1145%2f2395123.2395126&partnerID=40&md5=5cce3b74fc108a5931fe4d8113cfcd7c,"Although intelligent interactive systems have been the focus of many research efforts, very few have addressed systems for individuals with disabilities. This article presents our methodology for an intelligent interactive system that provides individuals with sight impairments with access to the content of information graphics (such as bar charts and line graphs) in popular media. The article describes the methodology underlying the systems intelligent behavior, its interface for interacting with users, examples processed by the implemented system, and evaluation studies both of the methodology and the effectiveness of the overall system. This research advances universal access to electronic documents. © 2012 ACM.",Accessibility; Blind individuals; Human-computer interaction; Intelligent systems; Multimodal,Computer graphics; Human computer interaction; Intelligent systems; Accessibility; Blind individuals; Electronic document; Information graphics; Intelligent behavior; Intelligent interactive systems; Multi-modal; Research advances; Graph theory,Article,Final,Scopus,2-s2.0-84983574296,Peter,,
"Lasecki W.S., Miller C.D., Sadilek A., Abumoussa A., Borrello D., Kushalnagar R., Bigham J.P.",54383728900;57198933938;36601324100;55481887500;55445832300;36142036500;16238221500;,Real-time captioning by groups of non-experts,2012,UIST'12 - Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology,,,,23,33,,132.0,10.1145/2380116.2380122,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869046812&doi=10.1145%2f2380116.2380122&partnerID=40&md5=a16fca9ce898f4a9240a47fc4aacaf45,"Real-time captioning provides deaf and hard of hearing people immediate access to spoken language and enables participation in dialogue with others. Low latency is critical because it allows speech to be paired with relevant visual cues. Currently, the only reliable source of real-time captions are expensive stenographers who must be recruited in advance and who are trained to use specialized keyboards. Automatic speech recognition (ASR) is less expensive and available on-demand, but its low accuracy, high noise sensitivity, and need for training beforehand render it unusable in real-world situations. In this paper, we introduce a new approach in which groups of non-expert captionists (people who can hear and type) collectively caption speech in real-time on-demand. We present LEGION:SCRIBE, an end-to-end system that allows deaf people to request captions at any time. We introduce an algorithm for merging partial captions into a single output stream in real-time, and a captioning interface designed to encourage coverage of the entire audio stream. Evaluation with 20 local participants and 18 crowd workers shows that non-experts can provide an effective solution for captioning, accurately covering an average of 93.2% of an audio stream with only 10 workers and an average per-word latency of 2.9 seconds. More generally, our model in which multiple workers contribute partial inputs that are automatically merged in real-time may be extended to allow dynamic groups to surpass constituent individuals (even experts) on a variety of human performance tasks. Copyright 2012 ACM.",Captioning; Crowdsourcing; Deaf; Hard of hearing; Real-time; Text alignment; Transcription,Audition; Crowdsourcing; Speech recognition; Transcription; Captioning; Deaf; Hard of hearings; Real time; Text alignments; User interfaces,Conference Paper,Final,Scopus,2-s2.0-84869046812,Peter,,
Stuckey M.E.,6701812382;,"FDR, the rhetoric of vision, and the creation of a national synoptic state",2012,Quarterly Journal of Speech,98,3,,297,319,,7.0,10.1080/00335630.2012.691172,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864016321&doi=10.1080%2f00335630.2012.691172&partnerID=40&md5=6240291665c47b8621b0eba2170c3663,"Throughout his administration, FDR engaged in a complex set of arguments that worked together to defend democracy in general as a viable form of government; American democracy as the highest expression of democratic government; the primacy of the federal government as the most efficient and effective locus of democratic power; and the executive office as the culmination of the form, efficiency, and locus of that power. My specific concern here is with one form those arguments took, the visual metaphors that permeate FDR's rhetoric. Visuality in FDR's rhetoric is especially intriguing because of the way it interacted with the prevailing political culture in order to underwrite radical shifts in political power by helping FDR persuade the mass public to accept a synoptic view of nationalism and governmental responsibility. These changes have implications for presidents, presidential candidates, and for the citizens whose support they seek. © 2012 National Communication Association.",,,Article,Final,Scopus,2-s2.0-84864016321,Peter,,
"Jacobs P.G., Brown P.M., Paatsch L.",36089058700;55454858700;6507856287;,Social and professional participation of individuals who are deaf: Utilizing the psychosocial potential maximization framework,2012,Volta Review,112,1,,37,62,,8.0,10.17955/tvr.112.1.693,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861808919&doi=10.17955%2ftvr.112.1.693&partnerID=40&md5=4eaa2777c3b982d58a7687a08c66b48d,"This article documents a strength-based understanding of how individuals who are deaf maximize their social and professional potential. This exploratory study was conducted with 49 adult participants who are deaf (n = 30) and who have typical hearing (n = 19) residing in America, Australia, England, and South Africa. The findings support a systematic and comprehensive framework of proactive psychosocial attributes and tactics. Statistical analysis showed no significant differences between the groups across four variables of psychosocial competencies. Qualitative data further suggests that participants who are deaf maximize their potential using two types of proactive psychosocial attributes and tactics: 1) skills that individuals with typical hearing use and 2) specialized skills for identifying, circumventing, or mastering deafness-related difficulties. These findings contribute to understandings of how individuals who are deaf can achieve social and professional success.",,,Article,Final,Scopus,2-s2.0-84861808919,Peter,,
"Collins K., Taillon P.J.",56211826000;8100093900;,Visualized sound effect icons for improved multimedia accessibility: A pilot study,2012,Entertainment Computing,3,1,,11,17,,5.0,10.1016/j.entcom.2011.09.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857142676&doi=10.1016%2fj.entcom.2011.09.002&partnerID=40&md5=461deddad0f6b53fb1e96421812c26bf,"Sound effects are often used to communicate important information in multimedia such as video games. For instance, they may tell the player that a character has just snuck up on them, is firing at them, or is about to paddle over a waterfall. Nevertheless, there are times when playing sound may be inappropriate, may be inaudible, may become fatiguing and/or may be inaccessible for hard of hearing and deaf users. Therefore, an alternative to sound that can relay the same information would be beneficial to many users. The majority of studies into alternative presentations of sound for these purposes have focused on dialogue at the expense of music and sound effects. The paper introduces a pilot study of "" SoundSign"" , a prototype symbolic representation of sound effects for multimedia, using an innovative icon and compass that indicates direction, sound cue and proximity. Users who have disabled the sound, are hearing-impaired or are otherwise unable to hear sound will still get the information needed. A description of SoundSign and the results of a usability test are presented. © 2011.",Accessibility; Audio; Games; Multimedia; Usability; User interfaces,User interfaces; Accessibility; Audio; Games; Multimedia; Usability; Audition,Article,Final,Scopus,2-s2.0-84857142676,Peter,,
"Coutinho F., Prates R.O., Chaimowicz L.",57195282178;6506422045;6602923971;,An analysis of information conveyed through audio in an FPS game and its impact on deaf players experience,2011,"Brazilian Symposium on Games and Digital Entertainment, SBGAMES",,,6363218,53,62,,11.0,10.1109/SBGAMES.2011.16,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872066311&doi=10.1109%2fSBGAMES.2011.16&partnerID=40&md5=8366046e43180491039fd4b12e55dc6c,"Mainstream games usually lack support for accessibility to deaf and hard of hearing people. The popular FPS game Half-Life 2 is an exception, in that it provides well constructed closed captions to players. In this paper, we performed a semiotic inspection on Half-Life 2, seeking to identify which strategies were used to convey information through audio. We also evaluated how the loss of information in each of them may impact players' experience. Our findings reveal that six different strategies are used and how they may compromise player experience. © 2011 IEEE.",accessibility; deaf; games; hard of hearing; human-computer interaction; semiotic inspection,accessibility; deaf; games; Player experience; Human computer interaction; Semiotics; Audition,Conference Paper,Final,Scopus,2-s2.0-84872066311,Peter,,
"Bigham J.P., Ladner R.E., Borodin Y.",16238221500;7005099015;16238262300;,The design of human-powered access technology,2011,ASSETS'11: Proceedings of the 13th International ACM SIGACCESS Conference on Computers and Accessibility,,,,3,10,,47.0,10.1145/2049536.2049540,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855209806&doi=10.1145%2f2049536.2049540&partnerID=40&md5=dd7b0eeaa1b3315938b42e15a8553e9c,"People with disabilities have always overcome accessibility problems by enlisting people in their community to help. The Internet has broadened the available community and made it easier to get on-demand assistance remotely. In particular, the past few years have seen the development of technology in both research and industry that uses human power to overcome technical problems too difficult to solve automatically. In this paper, we frame recent developments in human computation in the historical context of accessibility, and outline a framework for discussing new advances in human-powered access technology. Specifically, we present a set of 13 design principles for human-powered access technology motivated both by historical context and current technological developments. We then demonstrate the utility of these principles by using them to compare several existing human-powered access technologies. The power of identifying the 13 principles is that they will inspire new ways of thinking about human-powered access technologies. © 2011 ACM.",access technology; crowdsourcing; human computation,Access technology; Accessibility problems; Crowdsourcing; Design Principles; Human computation; Human power; People with disabilities; Technical problem; Technological development; Ways of thinking; Handicapped persons; Industrial research; Technology,Conference Paper,Final,Scopus,2-s2.0-81855209806,Peter,,
"Soliman A.S., Mullan P.B., O'Brien K.S., Thaivalappil S., Chamberlain R.M.",15738602000;7003300976;54407472600;36599455500;57206510922;,Career development needs assessment in cancer prevention and control: Focus on research in minority and international settings,2011,Journal of Cancer Education,26,3,,409,419,,3.0,10.1007/s13187-011-0243-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80855148223&doi=10.1007%2fs13187-011-0243-x&partnerID=40&md5=f889a1910e03ebc59c185eee28a99598,"This study was conducted as a needs assessment to inform the development of an educational program designed to provide mentorship and skills supporting careers in cancer research, with a focus on domestic minority populations and international settings. The objectives were to determine: (1) the level of interest among trainees in careers in cancer research and (2) preferences and constraints constituted by potential components, features, and duration of the proposed extramural training program. The target populations were participants and directors of federal training programs in cancer research, specifically (1) trainees in the NCI-K01, K07, and K08 programs, as well as the Department of Defense (DoD) Breast and Prostate Control Programs and (2) PIs of NCI R25 training programs and federally designated Comprehensive Cancer Centers. We developed, piloted, and administered electronically a survey to elicit perspectives of trainees' career development needs and preferences. Response rates from each training group exceeded 65%, with the exception of the K08 trainees (49%). The proportion of cancer research trainees who are interested in careers that include research on US minority groups was 70% of K01 trainees, 72% of K07 trainees, 45% of K08 trainees, and 75% of DoD trainees. A substantial percent of these trainees indicated their plans also include cancer research in international settings: 60% of K01s; 50% of K07s, 42% of K08s, and 87% of DoD trainees. Trainees identified substantial interest in a program that would provide the following: mentoring, manuscript writing skills, collaborative research in special populations, financial support, and focused modular courses. This study offers encouraging evidence of interest which focused in extramural education to augment skills facilitating cancer-related research in special populations. © Springer 2011.",Cancer education; Disparities; Extramural training; Mentoring; Special populations,article; decision making; education; health; human; medical research; minority health; needs assessment; neoplasm; teacher; Biomedical Research; Career Choice; Humans; Mentors; Minority Health; Needs Assessment; Neoplasms; World Health,Article,Final,Scopus,2-s2.0-80855148223,Peter,,
"Balteş F.R., Avram J., Miclea M., Miu A.C.",36024943600;36024869400;6602147540;6701771752;,"Emotions induced by operatic music: Psychophysiological effects of music, plot, and acting. A scientist's tribute to Maria Callas",2011,Brain and Cognition,76,1,,146,157,,46.0,10.1016/j.bandc.2011.01.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955125131&doi=10.1016%2fj.bandc.2011.01.012&partnerID=40&md5=7d889190ef0da33e39a55d4530e2562b,"Operatic music involves both singing and acting (as well as rich audiovisual background arising from the orchestra and elaborate scenery and costumes) that multiply the mechanisms by which emotions are induced in listeners. The present study investigated the effects of music, plot, and acting performance on emotions induced by opera. There were three experimental conditions: (1) participants listened to a musically complex and dramatically coherent excerpt from Tosca; (2) they read a summary of the plot and listened to the same musical excerpt again; and (3) they re-listened to music while they watched the subtitled film of this acting performance. In addition, a control condition was included, in which an independent sample of participants succesively listened three times to the same musical excerpt. We measured subjective changes using both dimensional, and specific music-induced emotion questionnaires. Cardiovascular, electrodermal, and respiratory responses were also recorded, and the participants kept track of their musical chills. Music listening alone elicited positive emotion and autonomic arousal, seen in faster heart rate, but slower respiration rate and reduced skin conductance. Knowing the (sad) plot while listening to the music a second time reduced positive emotions (peacefulness, joyful activation), and increased negative ones (sadness), while high autonomic arousal was maintained. Watching the acting performance increased emotional arousal and changed its valence again (from less positive/sad to transcendent), in the context of continued high autonomic arousal. The repeated exposure to music did not by itself induce this pattern of modifications. These results indicate that the multiple musical and dramatic means involved in operatic performance specifically contribute to the genesis of music-induced emotions and their physiological correlates. © 2011 Elsevier Inc.",Music-induced emotions; Operatic music; Physiological differentiation of emotions,adult; affect; arousal; article; breathing rate; controlled study; electrodermal response; emotionality; female; heart rate variability; human; human experiment; male; mood; music therapy; normal human; positive feedback; priority journal; psychophysiology; sensory stimulation; skin conductance; sympathetic function; task performance; vagus tone; Arousal; Auditory Perception; Autonomic Nervous System; Emotions; Female; Galvanic Skin Response; Heart Rate; Humans; Male; Music; Questionnaires; Young Adult,Article,Final,Scopus,2-s2.0-79955125131,Peter,,
"Ellis K., Kent M.",36496810100;56224876400;,Disability and new media,2011,Disability and New Media,,,,1,172,,130.0,10.4324/9780203831915,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909326663&doi=10.4324%2f9780203831915&partnerID=40&md5=90480986c44ef5f358a5cc4861607261,"Disability and New Media examines how digital design is triggering disability when it could be a solution. Video and animation now play a prominent role in the World Wide Web and new types of protocols have been developed to accommodate this increasing complexity. However, as this has happened, the potential for individual users to control how the content is displayed has been diminished. Accessibility choices are often portrayed as merely technical decisions but they are highly political and betray a disturbing trend of ableist assumption that serve to exclude people with disability. It has been argued that the Internet will not be fully accessible until disability is considered a cultural identity in the same way that class, gender and sexuality are. Kent and Ellis build on this notion using more recent Web 2.0 phenomena, social networking sites, virtual worlds and file sharing. Many of the studies on disability and the web have focused on the early web, prior to the development of social networking applications such as Facebook, YouTube and Second Life. This book discusses an array of such applications that have grown within and alongside Web 2.0, and analyzes how they both prevent and embrace the inclusion of people with disability. © 2011 Taylor & Francis. All rights reserved.",,,Book,Final,Scopus,2-s2.0-84909326663,Peter,,
"Vlaming M.S.M.G., Kollmeier B., Dreschler W.A., Martin R., Wouters J., Grover B., Mohammadh Y., Mohammadh T.",6602649424;7006746726;7003763918;55689404200;7102732105;57515385100;37003000800;37002861300;,Hearcom: Hearing in the communication society,2011,Acta Acustica united with Acustica,97,2,,175,192,,42.0,10.3813/AAA.918397,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952171609&doi=10.3813%2fAAA.918397&partnerID=40&md5=c0a188b4481c974c668bbe989d6a8801,"A group of 28 research partners joined the EU-funded project HearCom with the overall aim to improve hearing communication. One of the main achievements has been the provision of advanced hearing screening tests by telephone and Internet. Next to that it was aimed to harmonize hearing diagnostic tests for European languages. For this the concept of an Auditory Profile was defined on which a number of diagnostic hearing tests were developed in several languages. As hearing problems are also a result of adverse acoustical circumstances such as for room acoustics and telecom systems, these effects have been studied, modelled and evaluated for hearing impaired persons. In the area of rehabilitation a large scale comparison study was performed on signal enhancement techniques for hearing devices. Both objective and subjective benefits were found for specific listening conditions in relation to a chosen signal processing method. As modern technology may assist on hearing and communication it was studied how the use of automatic speech transcription or the use of handheld communication devices may help people with hearing problems. It is shown that communication benefits can be obtained, but that the benefit is limited in practice as processing power of today's handheld devices is still insufficient. An overview is given on the HearCom portal with sections for screening diagnostics, hearing information for the public and professionals, and a new HearCompanion service that provides step-by-step support for the hearing rehabilitation process. © S. Hirzel Verlag.",,Communication device; Comparison study; Diagnostic tests; European languages; Hand held device; Handhelds; Hearing devices; Hearing screening; Hearing-impaired persons; Modern technologies; Processing power; Room acoustics; Signal enhancement; Speech transcriptions; Step-by-step; Telecom systems; Architectural acoustics; Handicapped persons; Processing; Signal processing; Speech communication; Speech transmission; Transcription; Audition,Review,Final,Scopus,2-s2.0-79952171609,Peter,,
"Yuan B., Folmer E., Harris Jr. F.C.",25029168800;9840498500;57213805178;,Game accessibility: A survey,2011,Universal Access in the Information Society,10,1,,81,100,,206.0,10.1007/s10209-010-0189-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952107696&doi=10.1007%2fs10209-010-0189-5&partnerID=40&md5=8e8beae2f63e38f04a624c9a5739f9f3,"Over the last three decades, video games have evolved from a pastime into a force of change that is transforming the way people perceive, learn about, and interact with the world around them. In addition to entertainment, games are increasingly used for other purposes such as education or health. Despite this increased interest, a significant number of people encounter barriers when playing games due to a disability. Accessibility problems may include the following: (1) not being able to receive feedback; (2) not being able to determine in-game responses; (3) not being able to provide input using conventional input devices. This paper surveys the current state-of-the-art in research and practice in the accessibility of video games and points out relevant areas for future research. A generalized game interaction model shows how a disability affects ones ability to play games. Estimates are provided on the total number of people in the United States whose ability to play games is affected by a disability. A large number of accessible games are surveyed for different types of impairments, across several game genres, from which a number of high- and low-level accessibility strategies are distilled for game developers to inform their design. © 2010 Springer-Verlag.",Disability; Game accessibility; Impairment; Strategy,Accessibility problems; Disability; Game accessibility; Impairment; Input devices; Interaction model; Strategy; Video game; Handicapped persons; Surveys; Human computer interaction,Article,Final,Scopus,2-s2.0-79952107696,Peter,,
Varney E.,36708819900;,Disability and information technology: A comparative study in media regulation,2011,Disability and Information Technology: A Comparative Study in Media Regulation,,,,1,288,,4.0,10.1017/CBO9781139017947,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928077348&doi=10.1017%2fCBO9781139017947&partnerID=40&md5=2adaafc05e701cb959463f8d7eade92a,"Disability and Information Technology examines the extent to which regulatory frameworks for information and communication technologies (ICTs) safeguard the rights of persons with disabilities as citizenship rights. The book adopts a comparative approach focused on four case studies: Canada, the European Union, the United Kingdom, and the United States. The discussion focuses on the tension between social and economic values in the regulation of ICTs and calls for a regulatory approach based on a framework of principles that reflects citizenship values such as equality and dignity. The analysis identifies common challenges encountered in the jurisdictions examined and points toward the rights-based approach advanced by the UN Convention on the Rights of Persons with Disabilities as a benchmark in protecting the rights of persons with disabilities to have equal access to information. The research draws on a wealth of resources, including legislation, cases, interviews conducted at organisations representing persons with disabilities, consultation documents and responses from organisations representing persons with disabilities. © Eliza Varney 2013.",,,Book,Final,Scopus,2-s2.0-84928077348,Peter,,
"Szarkowska A., Krejtz I., Klyszejko Z., Wieczorek A.",54416458200;54395481500;57199568233;54416654800;,"Verbatim, standard, or edited? reading patterns of different captioning styles among deaf, hard of hearing, and hearing viewers",2011,American Annals of the Deaf,156,4,,363,378,,50.0,10.1353/aad.2011.0039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-81355153491&doi=10.1353%2faad.2011.0039&partnerID=40&md5=03959dd301558b027eb84f3727116d7c,"One of the most frequently recurring themes in captioning is whether captions should be edited or verbatim. The authors report on the results of an eye-tracking study of captioning for deaf and hard of hearing viewers reading different types of captions. By examining eye movement patterns when these viewers were watching clips with verbatim, standard, and edited captions, the authors tested whether the three different caption styles were read differently by the study participants (N = 40): 9 deaf, 21 hard of hearing, and 10 hearing individuals. Interesting interaction effects for the proportion of dwell time and fixation count were observed. In terms of group differences, deaf participants differed from the other two groups only in the case of verbatim captions. The results are discussed with reference to classical reading studies, audiovisual translation, and a new concept of viewing speed.",,,Article,Final,Scopus,2-s2.0-81355153491,Peter,,
"Wentz B., Jaeger P.T., Lazar J.",35486438400;7102351010;7007005493;,Retrofitting accessibility: The legal inequality of after-the-fact online access for persons with disabilities in the United States,2011,First Monday,16,11,,,,,55.0,10.5210/fm.v16i11.3666,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80855156907&doi=10.5210%2ffm.v16i11.3666&partnerID=40&md5=256567aafa6b18f10c6bd55011d6c34c,"Despite the significant advantages that access to information and communication technology has made to many of our lives, the related benefits, opportunities and even equalizing effect of this technology are often not accessible or only partially accessible to a growing portion of the global population. Current disability rights laws which are supposed to exist for the protection and well-being of individuals with disabilities are often too close to the heart of the problem, as they can actually promote a separate but unequal online environment. If current U.S. laws were revised to encourage born-accessible technology and there was consistent enforcement of such laws, the online experience of millions of individuals with disabilities could be drastically improved. This article examines the aspects of the current laws that perpetuate a separate but unequal online environment, discussing past and current examples of such inequity. It also contrasts the structure of current U.S. disability rights laws with other civil rights legislation and offers a set of policy recommendations that could have a positive mpact on accessibility. © 2011, First Monday.",,,Article,Final,Scopus,2-s2.0-80855156907,Peter,,
"Yoon J.-O., Kim M.",53882220900;55686325500;,"The effects of captions on deaf students' content comprehension, cognitive load, and motivation in online learning",2011,American Annals of the Deaf,156,3,,283,289,,20.0,10.1353/aad.2011.0026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052578317&doi=10.1353%2faad.2011.0026&partnerID=40&md5=6ed7ced588d83918c623c6292e582a62,"The authors examined the effects of captions on deaf students' content comprehension, cognitive load, and motivation in online learning. The participants in the study were 62 deaf adult students who had limited reading comprehension skills and used sign language as a first language. Participants were randomly assigned to either the control group or the experimental group. The independent variable was the presence of captions, and the dependent variables were content comprehension, cognitive load, and motivation. The study applied a posttest-only control group design. The results of the experiment indicated a significant difference (t = -2.16, p <.05) in content comprehension but no statistically significant difference in cognitive load and motivation between the two groups. These results led to suggestions for improvements in learning materials for deaf individuals.",,,Article,Final,Scopus,2-s2.0-80052578317,Peter,,
Wang Y.,15764048200;,Inquiry-based science instruction and performance literacy for students who are deaf or hard of hearing,2011,American Annals of the Deaf,156,3,,239,254,,14.0,10.1353/aad.2011.0031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052573960&doi=10.1353%2faad.2011.0031&partnerID=40&md5=de5bdb383b632ecf2b936a0dd17ca4ef,"Deaf and hard of hearing students, who cannot successfully access and utilize information in print, experience various difficulties in conventional science instruction, which heavily relies on lectures and textbooks. The purpose of the present review is threefold. First, an overview of inquiry-based science instruction reform, including the sociohistorical forces behind the movement, is presented. Then, the author examines the empirical research on science education for students who are deaf or hard of hearing from the 1970s to the present and identifies and rates inquiry-based practice. After discussing the difficulty of using science texts with deaf and hard of hearing students, the author introduces a conceptual framework that integrates inquirybased instruction and the construct of performance literacy. She suggests that this integration should enable students who are deaf or hard of hearing to access the general education curriculum.",,,Review,Final,Scopus,2-s2.0-80052573960,Peter,,
"Vy Q.V., Fels D.I.",23026041900;57218572233;,Enhanced captioning: Speaker identification: Text vs. images,2011,Telecommunications Journal of Australia,61,2,,,,,,10.7790/tja.v61i2.209,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957644591&doi=10.7790%2ftja.v61i2.209&partnerID=40&md5=7a4e8f16b4b1c666843d9c5cce86c5d8,"The enhanced captioning system was developed using a participatory design methodology to improve speaker identification using avatars. These avatars consisted of the speaker's name and image surrounded by a coloured border matching their clothing. An evaluation was conducted using eye-tracking with viewers who are deaf and hard-or-hearing. The major findings indicate that using an avatar image as a method for identifying who is speaking is not only feasible but may require the least attention from viewers to process. Further, differences between hard-of-hearing and deaf participants are reported.",,,Article,Final,Scopus,2-s2.0-79957644591,Peter,,
"Fels D.I., Gerdzhev M., Ho J., Hibbard E.",57218572233;35102096600;36701387000;35102368600;,Usability and use of SLS: Caption,2010,ASSETS'10 - Proceedings of the 12th International ACM SIGACCESS Conference on Computers and Accessibility,,,,291,292,,,10.1145/1878803.1878876,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650610821&doi=10.1145%2f1878803.1878876&partnerID=40&md5=d581f92c6539e88315480a7ee12cc4aa,"SLS: Caption provides captioning functionality for deaf and hearing users to provide captions to video content (including sign language content). Users are able to enter and modify text as well as adjust its font, colour, location and background opacity. An initial user study with hearing users showed that SLS: Caption was easy to learn and use. However, users seem reluctant to produce captions for their own video material; this was likely due to the task complexity and time required to create captions regardless of the usability of the captioning tool.",Captioning; Captioning tools; Sign language online,Captioning; Captioning tools; Sign language; Task complexity; User study; Video contents; Video material; Audition,Conference Paper,Final,Scopus,2-s2.0-78650610821,Peter,,
Smith M.B.,56228487700;,Opening our eyes: The complexity of competing visual demands in interpreted classrooms,2010,Ethical Considerations in Educating Children Who Are Deaf or Hard of Hearing,,,,154,191,,2.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903240936&partnerID=40&md5=239967363e02d41a4b5e61d2db971907,"Signed languages are visual languages. The importance of this quality was emphasized almost one hundred years ago by George Veditz, a prominent leader in the Deaf community and former president of the National Association of the Deaf. Veditz (1912) delivered a passionate argument in support of American Sign Language even in the face of intense political pressures, punctuated by the 1880 decision in Milan, Italy, to ban the use of sign language in public schools. In this address he characterized Deaf people as ""first, last, and of all time the people of the eye."" The fundamentally visual nature of American Sign Language and the people who use it validated his case for the preservation of this language at a time when it seemed on the verge of eradication. Nearly a century later, some Deaf leaders are celebrating the process of discovering what it truly means to be Deaf (Ladd, 2003) and champion the essential aspect of vision at its core (Bahan, 2008; Lentz, 2007). American Sign Language has, in fact, not only survived but also attained wide recognition and support. The California Department of Education's vision for California's deaf and hard of hearing students states that each of these students will be provided with the means to ""develop age-appropriate communication skills, in his/her preferred mode of communication⋯ which will allow him/her to acquire the academic, social, emotional, and vocational skills needed for the establishment of social relationships, economic self-sufficiency, and the assumption of civic responsibility"" (2000, p. 1). In a report of the California Deaf and Hard-of-Hearing Education Advisory Task Force, American Sign Language is listed as one of the communication options that should be made available (1999). Although ASL is largely recognized today, for Deaf and hard of hearing learners in K-12 classrooms, the struggle has now become focused on their true inclusion in and access to a quality education. The typical classroom environment makes meeting their needs inherently and intensely complex. To date, schools have largely failed Deaf and hard of hearing students (Commission on Education of the Deaf [COED], 1988; California Deaf and Hard-of-Hearing Education Advisory Task Force, 1999; O'Connell, 2007). In mainstream education, students are expected to adapt to the regular classroom, whereby inclusion implies that the classroom and the teachers are supposed to adapt to meet the needs of the students (Stinson & Antia, 1999). ""Philosophically, inclusion implies more than mainstreaming. Inclusion refers to full membership in a regular classroom"" (Seal, 2004, p. 1). Many Deaf and hard of hearing students attend regular classrooms with interpreters, yet concerns about the efficacy of an interpretermediated education remain. There is no doubt about the moral imperative to create change in the dismal statistics on the educational outcomes of Deaf and hard of hearing students. In his 2007 address on the state of education in California, Jack O'Connell reported that only 8% of Deaf students and 15% of hard of hearing students attain a score of at least ""proficient"" on English language arts standards. Because school success is largely predictive of employment options, the stakes are incredibly high when making decisions about how to most effectively meet the needs of Deaf and hard of hearing students in inclusive classrooms. If the general assumption and legislative reality are that the provision of a qualified interpreter is a sufficient means for Deaf and hard of hearing students to be fully included in mainstream school contexts, then we have an obligation to determine the factors that need to be in place so that they are truly afforded the opportunity to achieve socially and academically to their greatest potential. Dean and Pollard (2006) have brought Niebuhr's moral philosophy of responsibility to the attention of professional interpreters. In 1963 Niebuhr described the moral person as one who responds appropriately to a given situation, a respon se that is characterized by a process of first asking oneself, ""not about what is good or right, but about what is fitting to do in the light of what is actually going on"" (West, 1965, p. 3). Professional ethics obligate practitioners to rely on their specialized knowledge and training to make decisions that take into account the likely outcomes of a chosen action (or intentional nonaction). Dean and Pollard (2006) suggest that interpreters must first consider the context in which the interpreted interaction takes place and the identification of the circumstances that require a response: ""An ethical response cannot be determined absent of knowledge regarding what it is that one is attempting to respond to, that is, what the situation or question is that has predicated the opportunity for a response"" (p. 121). They further elaborate that ""careful consideration and judgment regarding situational and human interaction factors are central to doing effective [interpreting] work"" (p. 259). Based on interviews with Deaf and hard of hearing students, Brown Kurz and Caldwell Langer (2004) have determined that ""a constellation of factors has to be properly aligned to achieve adequate access to education through an interpreter. Even if that alignment were achieved, these participants are quite aware that they still would not have equal access to education because of inherent alterations associated with the interpreting process"" (p. 11). This alarming tacit acceptance of a lack of equal access confirms the need for extensive investigation into the possibilities and pitfalls of an interpreted education. In inclusive teaching environments, effective and responsible educational interpreters must carefully consider a constellation of academic, contextual, and situational factors along with a myriad of student- and teacher-related factors in order to make decisions appropriate for each interpreted interaction. In fact, it is only upon consideration of multiple factors that real interpreting can occur (Turner, 2005). The question that remains is what factors interpreters should consider when making decisions that affect Deaf and hard of hearing students' school experiences. Concerns about barriers to access and inclusion (Antia & Kreimeyer, 2001; Brown Kurz & Caldwell Langer, 2004; Komesaroff & McLean, 2006; Lane, 1995; Mertens, 1990; Power & Hyde, 2002; Ramsey, 1997; Schildroth & Hotto, 1994; Winston, 1994, 2004), equivalence of interpretations, and amount of instruction comprehended by Deaf and hard of hearing students in comparison to their hearing peers (Harrington, 2000; Jacobs, 1977; Johnson, 1991; La Bue, 1998; Marschark, Sapere, Convertino, Seewagen, & Maltzen, 2004; Marschark, Sapere, Convertino, & Seewagen, 2005; Russell, 2006, 2008), as well as the roles, practices, and qualifications of educational interpreters (Hayes, 1992; Jones, 1993, 2004; Metzger & Fleetwood, 2004; Monikowski & Winston, 2003; Schick & Williams, 2004; Schick, Williams, & Bolster, 1999; Stewart & Kluwin, 1996; Stuckless, Avery, & Hurwitz, 1989; Taylor & Elliott, 1994) have been amply documented. Several researchers have expressed a specific concern about meeting the visual needs of Deaf and hard of hearing students in educational settings. Although it is impossible for Deaf and hard of hearing students to watch the interpreter and attend simultaneously to other visual stimuli, Winston (1994, 2004) has noted that educational settings are riddled with practices that result in just such conflicting agendas. Seal (2004) cautions that interpreting during art demonstrations or computer work ""poses another challenge for interpreters who must negotiate competing visual referents"" (p. 94) and mentions that multiple factors will likely complicate interpreters' decisions when teachers show captioned video. Competing demands for visual attention exist when a Deaf or hard of hearing student needs to attend visually to a signed interpretation while also locating and/or viewing another source of visual input. Classroom teachers depend on a multitude of visual aids (such as whiteboards, overhead transparencies, charts, and textbooks) and are accustomed to working with hearing students, who can look at visual input while simultaneously listening to instruction. Therefore, teachers typically keep talking while referring to other sources of important visual information, inadvertently creating difficulties for Deaf and hard of hearing students, who need to look at an interpreter to access classroom discourse. These students must have an opportunity to alternate between looking at other visual stimuli and attending visually to the interpreted communication. © 2010 by Gallaudet University.",,,Book Chapter,Final,Scopus,2-s2.0-84903240936,Peter,,
"Kushalnagar R.S., Pâris J.-F.",36142036500;56212865200;,Multiple view scalability of presentations distributed to heterogeneous devices,2010,"Proceedings of the 2010 International Symposium on Performance Evaluation of Computer and Telecommunication Systems, SPECTS'2010",,,5589004,7,13,,1.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649295794&partnerID=40&md5=19475f675d4b284ad7b06fa99685244c,"We present a novel approach of distributing video lectures by capturing multiple regions of interest from a high definition camera and then distributing via a peer-to-peer network to participants' viewing devices. We show this approach is scalable and supports a broader range of viewing devices for participants. Our approach is more flexible than relying on a traditional camera operator to capture a single high bandwidth video stream. In addition, the solution supports a more active learning and inclusive environment by enabling views of additional streams such as audio captioning, video descriptions or optical character recognition of overheads and whiteboards for sensory disabled participants using personal devices.",Mobile devices; Multiple video streams; Online learning; Peer-to-peer live streaming; Simulation,Active Learning; Heterogeneous devices; High bandwidth; High-definition cameras; Multiple regions; Multiple video streams; Multiple views; Online learning; Peer to peer; Personal devices; Simulation; Video streams; Viewing devices; White board; High definition; Live streaming; Cameras; Distributed computer systems; E-learning; Mobile devices; Multimedia services; Optical character recognition; Portable equipment; Telecommunication systems; Video streaming; Peer to peer networks; Video streaming,Conference Paper,Final,Scopus,2-s2.0-78649295794,Peter,,
"Perego E., del Missier F., Porta M., Mosconi M.",36477731700;57191451517;35100711800;6603725099;,The cognitive effectiveness of subtitle processing,2010,Media Psychology,13,3,,243,272,,131.0,10.1080/15213269.2010.502873,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956621020&doi=10.1080%2f15213269.2010.502873&partnerID=40&md5=bc9578a851fbf7870174a853e4fd430c,"In an experimental study, we analyzed the cognitive processing of a subtitled film excerpt by adopting a methodological approach based on the integration of a variety of measures: eye-movement data, word recognition, and visual scene recognition. We tested the hypothesis that the processing of subtitled films is cognitively effective: It leads to a good understanding of film content without requiring a significant tradeoff between image processing and text processing. Following indications in the psycholinguistic literature, we also tested the hypothesis that two-line subtitles whose segmentation is syntactically incoherent can have a disruptive effect on information processing and recognition performance. The results highlighted the effectiveness of subtitle processing: Regardless of the quality of line segmentation, participants had a good understanding of the film content, they achieved good levels of performance in both word and scene recognition, and no tradeoff between text and image processing was detected. Eye-movement analyses enabled a further characterization of cognitive processing during subtitled film viewing. This article discusses the theoretical implications of the findings for both subtitling and multiple-source communication and highlights their methodological and applied implications. © Taylor & Francis Group, LLC.",,,Article,Final,Scopus,2-s2.0-77956621020,Peter,,
"Sadler G.R., Lee H., Lim R.S., Fullerton J.",7005152183;54389275400;35083860200;57203237717;,Recruitment of hard-to-reach population subgroups via adaptations of the snowball sampling strategy,2010,Nursing and Health Sciences,12,3,,369,374,,612.0,10.1111/j.1442-2018.2010.00541.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955782187&doi=10.1111%2fj.1442-2018.2010.00541.x&partnerID=40&md5=6422e07aecf7fea5e278f36c26137801,"Nurse researchers and educators often engage in outreach to narrowly defined populations. This article offers examples of how variations on the snowball sampling recruitment strategy can be applied in the creation of culturally appropriate, community-based information dissemination efforts related to recruitment to health education programs and research studies. Examples from the primary author's program of research are provided to demonstrate how adaptations of snowball sampling can be used effectively in the recruitment of members of traditionally underserved or vulnerable populations. The adaptation of snowball sampling techniques, as described in this article, helped the authors to gain access to each of the more-vulnerable population groups of interest. The use of culturally sensitive recruitment strategies is both appropriate and effective in enlisting the involvement of members of vulnerable populations. Adaptations of snowball sampling strategies should be considered when recruiting participants for education programs or for research studies when the recruitment of a population-based sample is not essential. © 2010 Blackwell Publishing Asia Pty Ltd.",Diversity; Hard-to-reach populations; Non-probability sampling; Recruitment; Research strategies; Snowball sampling,article; epidemiology; human; methodology; nursing research; patient selection; research subject; Humans; Nursing Research; Patient Selection; Research Design; Research Subjects; Sampling Studies,Article,Final,Scopus,2-s2.0-77955782187,Peter,,
"Zekveld A.A., Kramer S.E., Festen J.M.",8641362500;7401609154;7005417075;,Pupil response as an indication of effortful listening: The influence of sentence intelligibility,2010,Ear and Hearing,31,4,,480,490,,249.0,10.1097/AUD.0b013e3181d4f251,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955784292&doi=10.1097%2fAUD.0b013e3181d4f251&partnerID=40&md5=33fd44d10df231be6bda932550f6ce6d,"Objectives: The aim of this study was to evaluate the influence of sentence intelligibility on the pupil dilation response during listening. Task-induced pupil-dilation reflects explicit effortful processing load. Therefore, pupillometry can be used to examine the listening effort during speech perception in difficult listening conditions. We expected to find increasing pupil dilation as a function of decreasing speech intelligibility. Design: Thirty-eight young participants (mean age = 23 yrs, SD = 3.2 yrs) with normal hearing were included. They performed three speech reception threshold (SRT) tests in which they listened to sentences in stationary noise. A one-up-one-down, two-up-one-down, or four-up-one-down adaptive procedure was applied, resulting in the correct rehearsal of 50, 71, or 84% of the sentences (SRT50%, SRT71%, and SRT84%, respectively). We examined the peak dilation amplitude, the latency of the peak dilation amplitude, and the mean pupil dilation during the processing of the speech in each of these conditions. The peak dilation amplitude and mean pupil dilation were calculated relative to the baseline pupil diameter during listening to noise alone. For each SRT condition, participants rated the experienced listening effort and estimated their performance level. Results: The signal to noise ratios (SNRs) in the SRT50%, SRT71%, and SRT84% conditions increased as a function of the speech intelligibility level. The subjective effort ratings decreased, and the estimated performance increased with increasing speech intelligibility level. Repeated measures analyses of variance indicated that peak dilation amplitude and mean pupil dilation were higher in the SRT50% condition as compared with the SRT71% and SRT84% conditions. The peak dilation amplitude, mean pupil dilation, and peak latency increased with decreasing SNR of the speech in noise, but no effect of noise level by itself on the baseline pupil diameter was observed. Irrespective of SNR, the pupil response was higher for incorrectly repeated sentences than for correctly repeated sentences. The analyses also indicated condition-order effects on the peak dilation amplitude and mean pupil dilatation: the pupil response was higher in the first SRT test than in the second and third tests. Within the first and third test, the baseline pupil diameter and the mean pupil dilation decreased as a function of the sentence number within the test. Spearman correlation coefficients showed no relations among the SNRs at the SRTs, subjective ratings, and the pupil response. Conclusions: The peak dilation amplitude, peak latency, and mean pupil dilation systematically increase with decreasing speech intelligibility. These results support that listening effort, as indicated by the pupil response, increases with decreasing speech intelligibility. This study indicates that pupillometry can be used to examine how listeners reach a certain performance level. Application of this technique to study listening effort can yield valuable insight into the processing resources required across listening conditions and into the factors related to interindividual differences in speech perception in noise. © 2010 Lippincott Williams & Wilkins.",,"adult; article; attention; auditory threshold; comparative study; female; hearing; human; male; noise; physiology; pupil; pure tone audiometry; reaction time; speech audiometry; speech intelligibility; Adult; Attention; Audiometry, Pure-Tone; Auditory Threshold; Female; Hearing; Humans; Male; Noise; Pupil; Reaction Time; Speech Intelligibility; Speech Reception Threshold Test; Young Adult",Article,Final,Scopus,2-s2.0-77955784292,Peter,,
"Vy Q.V., Fels D.I.",23026041900;57218572233;,Using placement and name for speaker identification in captioning,2010,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),6179 LNCS,PART 1,,247,254,,7.0,10.1007/978-3-642-14097-6_40,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954936476&doi=10.1007%2f978-3-642-14097-6_40&partnerID=40&md5=34565cd5fb96481395daaf2a93837ecf,"The current method for speaker identification in closed captioning on television is ineffective and difficult in situations with multiple speakers, off-screen speakers, or narration. An enhanced captioning system that uses graphical elements (e.g., avatar and colour), speaker names and caption placement techniques for speaker identification was developed. A comparison between this system and conventional closed captions was carried out deaf and hard-of-hearing participants. Results indicate that viewers are distracted when the caption follows the character on-screen regardless of whether this should assist in identifying who is speaking. Using the speaker's name for speaker identification is useful for viewers who are hard of hearing but not for deaf viewers. There was no significant difference in understanding, distraction, or preference for the avatar with the coloured border component. © 2010 Springer-Verlag Berlin Heidelberg.",captioning; speaker identification; subtitles; universal design,Closed captioning; Graphical elements; Speaker identification; Universal Design; Design; Loudspeakers; Speech recognition,Conference Paper,Final,Scopus,2-s2.0-77954936476,Peter,,
Reichenberg M.,23478071200;,Deaf adults and comprehension of expository texts,2010,L1 Educational Studies in Language and Literature,10,2,,19,39,,1.0,10.17239/l1esll-2010.10.02.01,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906952342&doi=10.17239%2fl1esll-2010.10.02.01&partnerID=40&md5=deffeaf12c99e756c30de8f7f5a826a7,"The study presents a comparison between deaf participants´ (14-65 years of age) comprehension of expository texts. Each participant was exposed to 12 texts with regard to the following four different conditions: 1. Silent reading of an authentic text. 2. Viewing of a videotaped signed authentic text 3. Silent reading of an easy-to-read text 4. Viewing of a videotaped signed easy-to-read- text. The conditions were counterbalanced in order to control order and passage effects. The good deaf readers had a higher mean score than the poor deaf readers on all text versions. There was a significant difference in mean scores between good deaf and poor deaf readers on the easy-to-read text version. How then can the results be explained? All of the easy-to-read texts were much shorter than the authentic texts. However, since there has to be the identical content as in the authentic texts, there was much implicit information in the easy-to-read texts. Consequently, the reader needs prior knowledge and reading experience to fill in the missing information in the text. A conclusion is that the easy-to-read texts did not serve their purpose then since the process of simplification itself has given rise to the removal of structures that are relevant to facilitate understanding. © International Association for the Improvement of Mother Tongue Education.",Authentic texts; Deaf readers; Easy-to-read texts; Expository texts; Reading comprehension,,Article,Final,Scopus,2-s2.0-84906952342,Peter,,
Jacobs P.G.,36089058700;,Psychosocial Potential Maximization: A Framework of Proactive Psychosocial Attributes and Tactics Used by Individuals who are Deaf,2010,Volta Review,110,1,,5,29,,8.0,10.17955/tvr.110.1.618,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952926869&doi=10.17955%2ftvr.110.1.618&partnerID=40&md5=d9334fdfe84fa1d38d7e7e42ad2aebaf,"This article presents a systematic and comprehensive framework of proactive psychosocial attributes and tactics that individuals who are deaf use to maximize their professional and social potential. Empirical studies regarding deafness appear to lack such a framework which may be due to how the research is conducted. A framework of proactive psychosocial attributes and tactics was found in Reff Ginsberg, and Gerber's (1995) study of learning disabilities. Reff et al. 's framework was thereafter used to frame a review of empirical studies related to deafness. It was found that individuals who are deaf likely maximize their potential using 2 sets of proactive psychosocial attributes and tactics: (1) skills that individuals with typical hearing use and (2) specialized skills for circumventing deafness-related dfflculties. It isfurt her argued that adult participants who are deaf will provide greater insight into how psychosocial potential is maximized. This literature review will be complemented by empiricalfindings in a later article.",,,Review,Final,Scopus,2-s2.0-77952926869,Peter,,
"Cavender A.C., Bigham J.P., Ladner R.E.",16238140100;16238221500;7005099015;,ClassInFocus: Enabling improved visual attention strategies for deaf and hard of hearing students,2009,ASSETS'09 - Proceedings of the 11th International ACM SIGACCESS Conference on Computers and Accessibility,,,,67,74,,54.0,10.1145/1639642.1639656,https://www.scopus.com/inward/record.uri?eid=2-s2.0-72249089986&doi=10.1145%2f1639642.1639656&partnerID=40&md5=c0cc70c084157729ba4bb36fb9f57783,"Deaf and hard of hearing students must juggle their visual attention in current classroom settings. Managing many visual sources of information (instructor, interpreter or captions, slides or whiteboard, classmates, and personal notes) can be a challenge. ClassInFocus automatically notifies students of classroom changes, such as slide changes or new speakers, helping them employ more beneficial observing strategies. A user study of notification techniques shows that students who liked the notifications were more likely to visually utilize them to improve performance. Copyright 2009 ACM.",Classroom technology; Deaf and hard of hearing users; Multimedia conferencing technology,Classroom change; Classroom settings; Classroom technology; Deaf and hard of hearing users; Multimedia conferencing; User study; Visual Attention; Visual attention strategy; White board; Multimedia systems; School buildings; Students; Teaching; Technology; Behavioral research,Conference Paper,Final,Scopus,2-s2.0-72249089986,Peter,,
Stone C.,39963094900;,Toward a deaf translation norm,2009,Toward a Deaf Translation Norm,,,,1,200,,40.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896190007&partnerID=40&md5=e026b3892d2e6806742951755f397264,"As access for deaf people grows around the world, a new profession has begun to emerge as well, that of Deaf translators and interpreters (T/Is). In his new study Toward a Deaf Translation Norm, Christopher Stone explores this innovation, including its antecedents and how it is manifested in public places. Most importantly, Stone investigates whether or not a Deaf translation norm has evolved as increasing numbers of Deaf T/Is work in the mainstream translating for websites, public services, government literature, and television media. For his study, the sixth volume in the Studies in Interpretation series, Stone concentrated his research in the United Kingdom. Specifically, he examined the rendering of English broadcast television news into British Sign Language (BSL) by both Deaf and hearing T/Is. Segments of the data feature simultaneous Deaf and hearing in-vision T/I broadcasts. Recording these broadcasts produced a controlled product that enabled direct comparison of the Deaf and hearing T/Is. Close analysis of these examples revealed to Stone that Deaf T/Is not only employ a Deaf translation norm, they take labors to shape their BSL text into a stand-alone product rather than a translation. Ultimately, Toward a Deaf Translation Norm opens up engrossing new vistas on current deliberation about neutrality in translation and interpretation. © 2009 by The Gallaudet University Press All rights reserved.",,,Book,Final,Scopus,2-s2.0-84896190007,Peter,,
"Mori J., Fels D.I.",24725309200;57218572233;,Seeing the music: Can animated lyrics provide access to the emotional content in music for people who are deaf or hard of hearing?,2009,TIC-STH'09: 2009 IEEE Toronto International Conference - Science and Technology for Humanity,,,5444362,951,956,,7.0,10.1109/TIC-STH.2009.5444362,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952695740&doi=10.1109%2fTIC-STH.2009.5444362&partnerID=40&md5=df4ec9132098a32a9959afa7fad060fd,"Over the last 30 years, the deaf and hard of hearing communities have relied on closed captioning for access to television content. However, closed captioning provides limited access to non-speech audio such as music, sound effects and speech prosody. Music in closed captions is often represented by only its title and/or a music note symbol, providing little to no information to the viewer about the intended effect or emotion of the music. The study explores ways to use animated text to represent the emotional elements of music. Viewer reaction was examined and their understanding of two songs presented with animated text lyrics. Overall, there was a positive reaction to the animated lyrics and they did not interfere with readability or understanding of the content. Participants also expressed a desire for complementary visual imagery to improve the entertainment value of the content. ©2009 IEEE.",Animation; Kinetic text; Music visualization,Closed captioning; Music notes; Music visualization; Non-speech audio; Sound effects; Speech prosody; Television content; Visual imagery; Animation; Technological forecasting; Temperature indicating cameras; Visualization; Audio acoustics,Conference Paper,Final,Scopus,2-s2.0-77952695740,Peter,,
"Chapdelaine C., Foucher S., Gagnon L.",57189843313;6701728686;7005340115;,Impact of gaze analysis on the design of a caption production software,2009,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),5616 LNCS,PART 3,,314,323,,,10.1007/978-3-642-02713-0_33,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350686153&doi=10.1007%2f978-3-642-02713-0_33&partnerID=40&md5=ca54fbd36c3ac0a33e19f23eb51763ec,"Producing caption for the deaf and hearing impaired is a labor intensive task. We implemented a software tool, named SmartCaption, for assisting the caption production process using automatic visual detection techniques aimed at reducing the production workload. This paper presents the results of an eye-tracking analysis made on facial regions of interest to understand the nature of the task, not only to measure of the quantity of data but also to assess its importance to the end-user; the viewer. We also report on two interaction design approaches that were implemented and tested to cope with the inevitable outcomes of automatic detection such as false recognitions and false alarms. These approaches were compared with a Keystoke-Level Model (KLM) showing that the adopted approach allowed a gain of 43% in efficiency. © 2009 Springer Berlin Heidelberg.",Caption production; Eye-tracking analysis; Facial recognition; Keystoke-Level Model (KLM),Automatic Detection; Caption production; Deaf and hearing impaired; End users; Eye-tracking; Eye-tracking analysis; Facial recognition; Facial regions; False alarms; Gaze analysis; Interaction design; Keystoke-Level Model (KLM); Labor intensive; Level model; Production process; Production software; Production workloads; Software tool; Visual detection; Audition; Computer software; Knowledge management; Human computer interaction,Conference Paper,Final,Scopus,2-s2.0-70350686153,Peter,,
"Vy Q.V., Fels D.I.",23026041900;57218572233;,Using avatars for improving speaker identification in captioning,2009,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),5727 LNCS,PART 2,,916,919,,4.0,10.1007/978-3-642-03658-3_110,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349569726&doi=10.1007%2f978-3-642-03658-3_110&partnerID=40&md5=aa793eeb6469107509ee5ba30fec8ffc,Captioning is the main method for accessing television and film content by people who are deaf or hard-of-hearing. One major difficulty consistently identified by the community is that of knowing who is speaking particularly for an off screen narrator. A captioning system was created using a participatory design method to improve speaker identification. The final prototype contained avatars and a coloured border for identifying specific speakers. Evaluation results were very positive; however participants also wanted to customize various components such as caption and avatar location. © 2009 Springer Berlin Heidelberg.,Avatars; Captioning; Inclusive design; Speaker identification,Avatars; Captioning; Community IS; Evaluation results; Inclusive design; Participatory design; Speaker identification; Avatars; Captioning; Community IS; Evaluation results; Hard of hearings; Inclusive design; Participatory design; Speaker identification; Knowledge management; Loudspeakers; Audition; Loudspeakers; Human computer interaction; Human computer interaction,Conference Paper,Final,Scopus,2-s2.0-70349569726,Peter,,
"Kramer S.E., Zekveld A.A., Houtgast T.",7401609154;8641362500;7003507966;,Measuring cognitive factors in speech comprehension: The value of using the Text Reception Threshold test as a visual equivalent of the SRT test,2009,Scandinavian Journal of Psychology,50,5,,507,515,,40.0,10.1111/j.1467-9450.2009.00747.x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349310509&doi=10.1111%2fj.1467-9450.2009.00747.x&partnerID=40&md5=c3a4c68a739c412bd6cea915621eef1b,"The ability to comprehend speech in noise is influenced by bottom-up auditory and top-down cognitive capacities. Separate examination of these capacities is relevant for various purposes. Speech-Reception-Threshold (SRT) tests measure an individual's ability to comprehend speech. This paper addresses the value of the Text-Reception-Threshold (TRT) test (a visual parallel of the SRT test) to assess the cognitive capacities allocated during speech comprehension. We conducted a secondary data analysis, including 87 normally-hearing adults (aged 18 to 78 years). Correlation coefficients between age, TRT, working memory (Spatial Span) and SRT were examined. The TRT and SRT correlated significantly (r = 0.30), supporting the value of TRT in explaining inter-individual differences in SRTs. The relations between age and TRT and between SSP and TRT were non-significant. The results indicate that the current TRT test does not fully cover the cognitive aspects relevant in speech comprehension. Adaptation of the test is required before clinical implementation can be considered. © 2009 The Scandinavian Psychological Associations.",Cognitive capacity; Correlations; Noise; Speech comprehension; Speech in noise; SRT; TRT,"adolescent; adult; age; aged; article; attention; auditory stimulation; auditory threshold; cognition; comprehension; female; hearing; human; male; middle aged; neuropsychological test; noise; perception; photostimulation; physiology; short term memory; speech; speech audiometry; speech perception; vision; Acoustic Stimulation; Adolescent; Adult; Age Factors; Aged; Attention; Auditory Threshold; Cognition; Comprehension; Female; Hearing; Humans; Male; Memory, Short-Term; Middle Aged; Neuropsychological Tests; Noise; Perceptual Masking; Photic Stimulation; Speech; Speech Discrimination Tests; Speech Perception; Visual Perception",Article,Final,Scopus,2-s2.0-70349310509,Peter,,
"Anderson-Inman L., Terrazas-Arellanes F.E., Slabin U.",35617564600;55922468700;56157340300;,Supported eText in Captioned Videos: A Comparison of Expanded versus Standard Captions on Student Comprehension of Educational Content,2009,Journal of Special Education Technology,24,3,,21,34,,7.0,10.1177/016264340902400303,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874992447&doi=10.1177%2f016264340902400303&partnerID=40&md5=cd168b08e1a133a1d4fa5104914f8d17,"Expanded captions are designed to enhance the educational value by linking unfamiliar words to one of three types of information: vocabulary definitions, labeled illustrations, or concept maps. This study investigated the effects of expanded captions versus standard captions on the comprehension of educational video materials on DVD by secondary students who are deaf or hard of hearing. Participants were assigned randomly to two groups, with each group experiencing both conditions in counterbalanced order. Scores from pretests and posttests of vocabulary and informational content revealed no statistically significant differences between the two conditions. The findings are discussed in light of student preferences for expanded captions and screen-capture data that revealed low access levels for the expanded material. © 2009 Technology and Media Division of the Council for Exceptional Children.",,,Article,Final,Scopus,2-s2.0-84874992447,Peter,,
"Gordon-Salant S., Callahan J.S.",7003302144;35094981700;,The benefits of hearing aids and closed captioning for television viewing by older adults with hearing loss,2009,Ear and Hearing,30,4,,458,465,,16.0,10.1097/AUD.0b013e3181a26ef4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69049120249&doi=10.1097%2fAUD.0b013e3181a26ef4&partnerID=40&md5=478054ce29f25979f0e1d082b0d5311f,"OBJECTIVE: Although watching television is a common leisure activity of older adults, the ability to understand televised speech may be compromised by age-related hearing loss. Two potential assistive devices for improving television viewing are hearing aids (HAs) and closed captioning (CC), but their use and benefit by older adults with hearing loss are unknown. The primary purpose of this initial investigation was to determine if older hearing-impaired adults show improvements in understanding televised speech with the use of these two assistive devices (HAs and CC) compared with conditions without these devices. A secondary purpose was to examine the frequency of HA and CC use among a sample of older HA wearers. DESIGN: The investigation entailed a randomized, repeated-measures design of 15 older adults (59 to 82 yr) with bilateral sensorineural hearing losses who wore HAs. Participants viewed three types of televised programs (news, drama, and game show) that were each edited into lists of speech segments and provided an identification response. Each participant was tested in four conditions: baseline (no HA or CC), HA only, CC only, and HA + CC. Also, pilot testing with young normal-hearing listeners was conducted to establish list equivalence and stimulus intelligibility with a control group. All testing was conducted in a quiet room to simulate a living room, using a 20 in flat screen television. Questionnaires were also administered to participants to determine the frequency of HA and CC use while watching television. RESULTS: A significant effect of viewing condition was observed for all programs. Participants exhibited significantly better speech recognition scores in conditions with CC than those without CC (p < 0.01). Use of personal HAs did not significantly improve recognition of televised speech compared with the unaided condition. The condition effect was similar across the three different programs. Most of the participants (73%) regularly wore their HAs while watching television; very few of them (13%) had ever used CC. CONCLUSIONS: On average, use of CC while watching television dramatically improved speech understanding by a sample of older hearing-impaired adults compared with conditions without CC, including when HAs were worn. © 2009 Lippincott Williams & Wilkins, Inc.",,"adult; aged; article; clinical article; controlled study; female; hearing aid; hearing loss; human; male; perception deafness; pilot study; questionnaire; randomization; speech discrimination; speech intelligibility; television viewing; Acoustic Stimulation; Aged; Aged, 80 and over; Hearing Aids; Humans; Middle Aged; Photic Stimulation; Presbycusis; Speech Discrimination Tests; Speech Perception; Television",Article,Final,Scopus,2-s2.0-69049120249,Peter,,
"Udo J.P., Fels D.",14038549800;57218572233;,The development of a new theatrical tradition: Sighted students audio describe school play for a blind and low-vision audience,2009,International Journal of Education and the Arts,10,,20,1,27,,7.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992785198&partnerID=40&md5=ceb2cecfee865fc1f6a2ef1a75deb82e,"In this paper, we discuss our experience of facilitating the development, creation and execution of audio description for an elementary school production of Fiddler on the Roof by three grade eight students. The students were supervised by the production’s director, their drama teacher, and assisted by the authors. An actor with experience describing a live theatre event provided some feedback for the students. Qualitative insight is gained through a thematic analysis of the describer’s student learning journal and an interview with their drama teacher. The strengths and weaknesses of the project as perceived by the students and their drama teacher are discussed. Participant suggestions and solutions are also highlighted. © 2009, Pennsylvania State University Libraries. All rights reserved.",,,Article,Final,Scopus,2-s2.0-84992785198,Peter,,
"Wilson J.A.B., Wells M.G.",55495799600;57009245200;,Telehealth and the deaf: A comparison study,2009,Journal of Deaf Studies and Deaf Education,14,3,,386,402,,19.0,10.1093/deafed/enp008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-67449092768&doi=10.1093%2fdeafed%2fenp008&partnerID=40&md5=a3fbf59be96f5679b160dbc889a1d660,"Within the deaf population, an extreme mental health professional shortage exists that may be alleviated with videoconferencing technology - also known as telehealth. Moreover, much needed mental health education within the deaf population remains largely inaccessible. Researchers have warned that the deaf population may remain underserved if significant changes do not take place with traditional service delivery methods. This article evaluated the efficacy of telehealth in teaching psychoeducational objectives, with special emphasis given to its application to the deaf population. Results indicate that telehealth can be regarded as an efficacious and cost-effective option in delivering health care to the deaf population. Participants also indicated satisfaction with the telehealth technology. The use of printed transcripts for educational purposes is encouraged given the significant findings in this article. The findings also have implications for the literature on single-session interventions. © The Author 2009. Published by Oxford University Press. All rights reserved.",,"adolescent; adult; article; comparative study; cost benefit analysis; depression; economics; evaluation; female; health promotion; hearing disorder; hearing impairment; human; male; methodology; middle aged; patient education; patient satisfaction; psychological and psychiatric procedures, techniques and concepts; psychological aspect; questionnaire; telemedicine; videoconferencing; Adolescent; Adult; Cost-Benefit Analysis; Deafness; Depression; Female; Health Promotion; Hearing Disorders; Humans; Male; Middle Aged; Patient Education as Topic; Patient Satisfaction; Psychological Techniques; Questionnaires; Telemedicine; Videoconferencing; Young Adult",Article,Final,Scopus,2-s2.0-67449092768,Peter,,
"Stinson M.S., Elliot L.B., Kelly R.R., Liu Y.",7005000200;7003340239;7401960564;55871023200;,Deaf and hard-of-hearing students' memory of lectures with speech-to-text and interpreting/note taking services,2009,Journal of Special Education,43,1,,52,64,,42.0,10.1177/0022466907313453,https://www.scopus.com/inward/record.uri?eid=2-s2.0-64749109571&doi=10.1177%2f0022466907313453&partnerID=40&md5=5fbc9d5dd4901dcc02f3d6f28b681eda,"In one investigation with 48 deaf and hard-of-hearing (hh) high school students and a second investigation with 48 deaf/hh college students, all viewed one lecture with an interpreter and one with the C-Print speech-to-text support service. High school students retained more lecture information when they viewed speech-to-text support, compared to interpreter support, and when they studied note taker notes or a hard copy of the text after viewing the lecture, compared to no opportunity to study. For college students, however, there was no difference between retention with these two kinds of support or with study of notes, compared to no study. For the college investigation, there was a three-way interaction due to markedly better performance on a multiple-choice than on a sentence-completion test when students viewed an interpreter and did not study notes. This result may have reflected difficulty in comprehending unfamiliar terms. Reading proficiency was also related to retention. © 2009 Hammill Institute on Disabilities.",Assistive technology; General education curriculum; Hearing impairments and deafness; Low-incidence disabilities,,Article,Final,Scopus,2-s2.0-64749109571,Peter,,
"Zazove P., Meador H.E., Reed B.D., Sen A., Gorenflo D.W.",6701847250;6603451378;7202456097;55443624400;7006854890;,Cancer prevention knowledge of people with profound hearing loss,2009,Journal of General Internal Medicine,24,3,,320,326,,40.0,10.1007/s11606-008-0895-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-60449095965&doi=10.1007%2fs11606-008-0895-3&partnerID=40&md5=3cea27ac001a5e427e5ea75c97212403,"Background: Deaf persons, a documented minority population, have low reading levels and difficulty communicating with physicians. The effect of these on their knowledge of cancer prevention recommendations is unknown. METHODS: A cross-sectional study of 222 d/Deaf persons in Michigan, age 18 and older, chose one of four ways (voice, video of a certified American Sign Language interpreter, captions, or printed English) to complete a self-administered computer video questionnaire about demographics, hearing loss, language history, health-care utilization, and health-care information sources, as well as family and social variables. Twelve questions tested their knowledge of cancer prevention recommendations. The outcome measures were the percentage of correct answers to the questions and the association of multiple variables with these responses. RESULTS: Participants averaged 22.9% correct answers with no gender difference. Univariate analysis revealed that smoking history, types of medical problems, last physician visit, and women having previous cancer preventive tests did not affect scores. Improved scores occurred with computer use (p∈=∈0.05), higher education (p∈<∈0.01) and income (p∈=∈0.01), hearing spouses (p∈<∈0.01), speaking English in multiple situations (p∈<∈0.001), and in men with previous prostate cancer testing (p∈=∈0.04). Obtaining health information from books (p∈=∈0.05), physicians (p∈=∈0.008), nurses (p∈=∈0.03) or the internet (p∈=∈0.02), and believing that smoking is bad (p∈<∈0.001) also improved scores. Multivariate analysis revealed that English use (p∈=∈0.01) and believing that smoking was bad (p∈=∈0.05) were associated with improved scores. CONCLUSION: Persons with profound hearing loss have poor knowledge of recommended cancer prevention interventions. English use in multiple settings was strongly associated with increased knowledge. © 2009 Society of General Internal Medicine.",Cancer; Deaf; Prevention,"adult; aged; article; cancer prevention; cancer screening; controlled study; demography; female; health care; health care utilization; health status; hearing loss; human; knowledge; major clinical study; male; questionnaire; Adult; Aged; Cross-Sectional Studies; Female; Health Knowledge, Attitudes, Practice; Hearing Impaired Persons; Humans; Male; Mass Screening; Middle Aged; Neoplasms; Questionnaires",Article,Final,Scopus,2-s2.0-60449095965,Peter,,
"Feng J., Sears A.",55247917500;7005819399;,Speech input to support universal access,2009,The Universal Access Handbook,,,,30-1,30-16,,6.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999155080&partnerID=40&md5=8987ac811287a4c391df99de71b1fa5d,"Speech-based interactions allow users to communicate with computers or computer-related devices without the use of a keyboard, mouse, buttons, or any other physical interaction device. By leveraging a skill that is mastered early in life, speech-based interactions have the potential to be more natural than interactions using other technologies such as the keyboard. Based on the input and output channels being employed, speech interactions can be categorized into three groups: spoken dialogue systems, speech output systems, and speech recognition systems. Spoken dialogue systems include applications that utilize speech for both input and output, such as telephony systems and speechbased environment control systems with voice feedback. Speech output systems include applications that only utilize speech for output while leveraging other technologies, such as the keyboard and mouse, for input. Screen access so ware, which is o en used by individuals with visual impairments, is an example of speech output. Speech recognition systems include applications that utilize speech for input and other modalities for output, such as speech-based cursor control in a GUI (graphical user interface) and speech-based dictation systems (see Table 30.1). e focus of this chapter is on those interactions where speech is used to provide input to some kind of computing technology. When discussing speech-based input, potential applications can be divided into three major categories, which are most easily distinguished based on the size of the vocabulary that the system recognizes, but the reality is that there is no clear dividing line that separates these categories. Vocabulary size is a continuous variable with systems recognizing as few as two or as many as tens of thousands of words. In the subsequent discussion, both speech and nonspeech output are considered. Typical applications include: •Telephony systems, which tend to use small input vocabularies as well as speech output, environmental control applications with small input vocabularies that may support speech or nonspeech output •Speech-based interactions with GUIs can support navigation, window manipulations, and various other commandbased interactions with widely varying input vocabularies ranging from just a few words to several hundred •Dictation applications, which support users as they compose e-mails, letters, and reports as well as smaller tasks such as filling in portions of forms where free-form input is allowed From the perspective of universal access (UA), speech-based interactions should be considered one of a set of tools available to help address the goal of ensuring that information technologies are accessible by all citizens as they address a variety of tasks in diverse contexts. While UA is concerned with addressing the needs of all possible users, three populations are of particular interest: children, older adults, and individuals with disabilities. For older users, who may be experiencing age-related visual and physical impairments, the graphical interface with a keyboard and mouse for input can present a variety of challenges, while speech can offer a natural style of interaction and reduce the need for physical interactions. Educational so ware, toys, and various web sites use speech to provide information to children, which serves as a natural and potentially easy-tolearn input solution. Perhaps the most obvious population that could benefit from speech-based interactions are individuals with physical impairments that hinder their use of more traditional input devices, such as the keyboard and mouse. For these users, speech can provide effective, inexpensive interaction solutions. © 2009 by Taylor & Francis Group, LLC.",,,Book Chapter,Final,Scopus,2-s2.0-84999155080,Peter,,
Zarei A.A.,36544658200;,"The effect of bimodal, standard, and reversed subtitling on L2 vocabulary recognition and recall",2009,Pazhuhesh-e Zabanha-ye Khareji,,49,,65,85,,19.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957705058&partnerID=40&md5=67546ea01d389c5d022f8c3159c0a774,"To investigate the effect of three modes of subtitling on vocabulary recognition, 97 participants in three groups at Imam Khomeini International University in Qazvin were presented with the same film but using different ways of subtitling. To compare the participants' vocabulary recognition, a one-way ANOVA was run. Results indicated that although the difference between the bimodal and standard groups was not statistically significant, both groups were significantly better than the reversed subtitling group. Another one-way ANOVA was used to compare the learners' vocabulary recall. Analyses showed that bimodal subtitling was significantly more effective than standard subtitling, which in turn, was significantly more effective than reversed subtitling. The findings of the present study can have theoretical and practical implications for both vocabulary recognition and recall.",Bimodal; Reversed subtitling; Standard; Vocabulary learning; Vocabulary recognition and recall,,Article,Final,Scopus,2-s2.0-77957705058,Peter,,
"Tyler M.D., Jones C., Grebennikov L., Leigh G., Noble W., Burnham D.",8908033100;8870030900;15759519900;8847145200;7102481799;7005177876;,Effect of caption rate on the comprehension of educational television programmes by deaf school students,2009,Deafness and Education International,11,3,,152,162,,23.0,10.1002/dei.262,https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349324198&doi=10.1002%2fdei.262&partnerID=40&md5=33713671114c29b479f6405c15e270ce,"Television captioning has great potential to provide deaf children with access to the audio track of programmes. However, use of captions may be limited by the lower English literacy skills of the deaf population compared to the general population. Here, we investigate how the rate of caption delivery affects the comprehension of educational programmes by better- and poorer-reading deaf school children. Participants watched three short documentaries, with captions presented at 90, 120, or 180 words per minute (wpm). Across both reading levels, comprehension was uniformly higher at 90 and 120 wpm than at 180 wpm. Independent of caption rate, better readers scored higher overall than poorer readers. These results suggest that the rate of captions in children's television programmes can safely use 120 wpm as a slowest speed. Future research should seek to pinpoint the optimal rate, which appears to lie between 120 and 180 wpm. © 2009 John Wiley & Sons, Ltd.",Caption speed; Captions; Comprehension; Deaf children; Reading rate,,Article,Final,Scopus,2-s2.0-70349324198,Peter,,
"Zekveld A.A., Kramer S.E., Kessens J.M., Vlaming M.S.M.G., Houtgast T.",8641362500;7401609154;6508026466;6602649424;7003507966;,"The influence of age, hearing, and working memory on the speech comprehension benefit derived from an automatic speech recognition system",2009,Ear and Hearing,30,2,,262,272,,19.0,10.1097/AUD.0b013e3181987063,https://www.scopus.com/inward/record.uri?eid=2-s2.0-69049115108&doi=10.1097%2fAUD.0b013e3181987063&partnerID=40&md5=8b119fb0d4d23739a180032682f3e764,"Objective: The aim of the current study was to examine whether partly incorrect subtitles that are automatically generated by an Automatic Speech Recognition (ASR) system, improve speech comprehension by listeners with hearing impairment. In an earlier study (Zekveld et al. 2008), we showed that speech comprehension in noise by young listeners with normal hearing improves when presenting partly incorrect, automatically generated subtitles. The current study focused on the effects of age, hearing loss, visual working memory capacity, and linguistic skills on the benefit obtained from automatically generated subtitles during listening to speech in noise. Design: In order to investigate the effects of age and hearing loss, three groups of participants were included: 22 young persons with normal hearing (YNH, mean age = 21 years), 22 middle-aged adults with normal hearing (MA-NH, mean age = 55 years) and 30 middle-aged adults with hearing impairment (MA-HI, mean age = 57 years). The benefit from automatic subtitling was measured by Speech Reception Threshold (SRT) tests (Plomp & Mimpen, 1979). Both unimodal auditory and bimodal audiovisual SRT tests were performed. In the audiovisual tests, the subtitles were presented simultaneously with the speech, whereas in the auditory test, only speech was presented. The difference between the auditory and audiovisual SRT was defined as the audiovisual benefit. Participants additionally rated the listening effort. We examined the influences of ASR accuracy level and text delay on the audiovisual benefit and the listening effort using a repeated measures General Linear Model analysis. In a correlation analysis, we evaluated the relationships between age, auditory SRT, visual working memory capacity and the audiovisual benefit and listening effort. Results: The automatically generated subtitles improved speech comprehension in noise for all ASR accuracies and delays covered by the current study. Higher ASR accuracy levels resulted in more benefit obtained from the subtitles. Speech comprehension improved even for relatively low ASR accuracy levels; for example, participants obtained about 2 dB SNR audiovisual benefit for ASR accuracies around 74%. Delaying the presentation of the text reduced the benefit and increased the listening effort. Participants with relatively low unimodal speech comprehension obtained greater benefit from the subtitles than participants with better unimodal speech comprehension. We observed an age-related decline in the working-memory capacity of the listeners with normal hearing. A higher age and a lower working memory capacity were associated with increased effort required to use the subtitles to improve speech comprehension. Conclusions: Participants were able to use partly incorrect and delayed subtitles to increase their comprehension of speech in noise, regardless of age and hearing loss. This supports the further development and evaluation of an assistive listening system that displays automatically recognized speech to aid speech comprehension by listeners with hearing impairment. Copyright © 2009 by Lippincott Williams & Wilkins.",,"accuracy; adult; aged; article; audiovisual equipment; automatic speech recognition; clinical article; controlled study; correlation analysis; female; groups by age; hearing; hearing impairment; human; language ability; male; noise; speech; speech audiometry; speech discrimination; visual memory; working memory; adolescent; age; auditory threshold; communication aid; comparative study; Deafness; hearing aid; linguistics; middle aged; reading; short term memory; speech perception; young adult; Adolescent; Adult; Age Factors; Aged; Auditory Threshold; Communication Aids for Disabled; Deafness; Female; Hearing; Hearing Aids; Humans; Linguistics; Male; Memory, Short-Term; Middle Aged; Noise; Reading; Speech Perception; Speech Recognition Software; Young Adult",Article,Final,Scopus,2-s2.0-69049115108,Peter,,
"Zekveld A.A., Kramer S.E., Kessens J.M., Vlaming M.S.M.G., Houtgast T.",8641362500;7401609154;6508026466;6602649424;7003507966;,User Evaluation of a Communication System That Automatically Generates Captions to Improve Telephone Communication,2009,Trends in Amplification,13,1,,44,68,,14.0,10.1177/1084713808330207,https://www.scopus.com/inward/record.uri?eid=2-s2.0-59149085302&doi=10.1177%2f1084713808330207&partnerID=40&md5=41e2bd5deb2bb47e1d074423ebd9cb8d,"This study examined the subjective benefit obtained from automatically generated captions during telephone-speech comprehension in the presence of babble noise. Short stories were presented by telephone either with or without captions that were generated offline by an automatic speech recognition (ASR) system. To simulate online ASR, the word accuracy (WA) level of the captions was 60% or 70% and the text was presented delayed to the speech. After each test, the hearing impaired participants (n = 20) completed the NASA-Task Load Index and several rating scales evaluating the support from the captions. Participants indicated that using the erroneous text in speech comprehension was difficult and the reported task load did not differ between the audio + text and audio-only conditions. In a follow-up experiment (n = 10), the perceived benefit of presenting captions increased with an increase of WA levels to 80% and 90%, and elimination of the text delay. However, in general, the task load did not decrease when captions were presented. These results suggest that the extra effort required to process the text could have been compensated for by less effort required to comprehend the speech. Future research should aim at reducing the complexity of the task to increase the willingness of hearing impaired persons to use an assistive communication system automatically providing captions. The current results underline the need for obtaining both objective and subjective measures of benefit when evaluating assistive communication systems. © 2009, SAGE Publications. All rights reserved.",assistive text display; automatic speech recognition; communication device for hearing impaired; user evaluation,"accuracy; adult; aged; article; assistive technology; auditory stimulation; automatic speech recognition; clinical article; controlled study; female; follow up; hearing impairment; human; information processing; Internet; low frequency noise; male; rating scale; simulation; speech discrimination; task performance; telecommunication; telephone; Adult; Aged; Aged, 80 and over; Cognition; Communication Aids for Disabled; Comprehension; Computer Systems; Female; Hearing Loss, Mixed Conductive-Sensorineural; Hearing Loss, Sensorineural; Humans; Male; Memory; Middle Aged; Noise; Perceptual Masking; Questionnaires; Rehabilitation of Hearing Impaired; Speech Perception; Speech Reception Threshold Test; Speech Recognition Software; Telephone; Time Factors; Visual Perception",Article,Final,Scopus,2-s2.0-59149085302,Peter,,
"Inoue S., Nakano Y., Arai T., Nagai N., Ooshima K., Yamamoto Y., Schepker N.",55346694100;55346538300;55346459100;55345508700;24833228500;55346101400;55346381400;,Closed-captions for viewers with low vision caption speed and new tools,2008,Assistive Technology Research Series,22,,,205,215,,,10.3233/978-1-58603-902-8-205,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865478209&doi=10.3233%2f978-1-58603-902-8-205&partnerID=40&md5=e262cddaaab8d1f39cff55a3317f9450,"In an era in which the world population of individuals aged 65 and older constitutes nearly 500 million people, the accessible design of information is increasingly important. Closed-captioning, originally designed to make televised information accessible to deaf and hearing-impaired individuals, now benefits millions of people with a wide range of abilities. The United States, which has legislated captioned television for over thirty years, can be considered a world leader in the practice from which many countries, including Japan, may learn. Yet even in the U.S. there is no effective style of captioning for elders and people with low-vision. The captioning of short clips, such as TV commercials and short video, is especially lacking. In our research we are establishing a methodology for appropriate captions for people with low vision, based on the Universal Design process pertaining to TV commercials. In Experiment 1 we examined the ways participants' visual behaviors were affected when their visual acuity was artificially reduced while viewing closed-captions, and the effects of reduced visual acuity on contents comprehension. In Experiment 2 we examined the participants' eye movement as they watched Japanese closed-captions at various speeds with no sound. We realized that 1) participants' tended not to look at the captions when their visual acuity was reduced making it more difficult to read, 2) for participants with visual acuity lower than 0.25, captions were unreadable, 3) participants were affected by the caption speed when they had diminished visual acuity, and thus, they could not see the caption, and 4) eye movement is a valid index in researching people's viewing and comprehension of film clips. The results also indicate that closed-captions are currently unsuitable for viewers with lowered visual acuity, even for those viewers with slightly reduced visual acuity. We now know that we must develop closed-captions that consider viewers with low-vision. We also know that total fixation time on captions calculated from eye movement is an especially valuable tool for researching the visual behaviors of viewers. This technology will not only aid the captioning process, but will also make the growing areas of new and online media accessible to millions of people. © 2008 The authors and IOS Press. All rights reserved.",,,Article,Final,Scopus,2-s2.0-84865478209,Peter,,
"Kosec P., Debevc M.",23397543000;56816724400;,Velap: Video-based e-lectures for all participants,2008,"Proceedings of the 7th IASTED International Conference on Web-Based Education, WBE 2008",,,,265,269,,,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949138458&partnerID=40&md5=c71e68174f40b76eb0987ee79a7c79aa,"Because of the rapid development of information and communication technology and the growing availability of broadband connections, the demand for e-learning webcast systems is increasing. What is required is a simple and natural way to record and present lectures, allowing for both live streams and archived copies to be produced without additional post-production. This paper presents our plan for and realization of a lecturer-friendly mobile webcast system (vELAP), which puts a particular emphasis on accessibility, taking people with disabilities into particular account. We also describe hardware and software solutions which allow for the automated and simultaneous recording of video and audio while including presentation slides, subtitles and tables of topics. Additionally, the vELAP system offers deaf and hard of hearing persons the option of viewing videos of sign language interpreters, while blind and weak-sighted students can access audio subtitles, text enlargements and colour corrections.",E-learning; Persons with disabilities; Webcasting,Broadband connections; Hardware and softwares; Information and communication technologies; People with disabilities; Persons with disabilities; Post productions; Presentation slides; Rapid development; Sign languages; Simultaneous recordings; Webcasting; Broadcasting; E-learning; Education; Internet; Multimedia systems; Telecommunication; Usability engineering; World Wide Web; Handicapped persons,Conference Paper,Final,Scopus,2-s2.0-62949138458,Peter,,
"Vy Q.V., Mori J.A., Fourney D.W., Fels D.I.",23026041900;24725309200;24070279200;57218572233;,EnACT: A software tool for creating animated text captions,2008,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),5105 LNCS,,,609,616,,8.0,10.1007/978-3-540-70540-6_87,https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249165446&doi=10.1007%2f978-3-540-70540-6_87&partnerID=40&md5=84031d8840e1f083a6c8e6907c36a1cc,"Music in captioning is often represented by only its title and/or a music note. This representation provides little to no information of the intended effect or emotion of the music. In this paper, we present a software tool that was created to enable users to mark emotions in a script or lyrics and then render those marks into animated text for display as captions. A pilot study was conducted to collect initial responses to, preferences and understanding of the animated lyrics of one song by a deaf and hard of hearing audience. Participants were able to identify the animated lyrics as belonging to a song and found that the animations helped them understand the portrayed emotions. They also identified the shaking style of animation portraying fear as least preferable. © 2008 Springer-Verlag Berlin Heidelberg.",Animation; Kinetic text; Music visualization,Animation; Hard-of-hearing; International conferences; Kinetic text; Music visualization; Pilot studies; Software tools; Special needs; Computer software,Conference Paper,Final,Scopus,2-s2.0-50249165446,Peter,,
"Rashid R., Vy Q., Hunt R., Fels D.I.",23025557900;23026041900;23024708000;57218572233;,Dancing with words: Using animated text for captioning,2008,International Journal of Human-Computer Interaction,24,5,,505,519,,18.0,10.1080/10447310802142342,https://www.scopus.com/inward/record.uri?eid=2-s2.0-46349099076&doi=10.1080%2f10447310802142342&partnerID=40&md5=118c8ad51fc59d94f77f2dcebf7c4a1b,"Closed captioning has suffered from a lack of innovation since its inception in the early 1970s. However, television and film technologies and user preferences have changed dramatically. Sound from music, sound effects, and speech prosody are essentially missing from current closed captions. We used animated text to represent emotions contained in music and speech as well as sound effects. Twenty-five hard of hearing and hearing participants watched two short television clips with three different types of captionsconventional, enhanced, and extreme. Hard of hearing and hearing participants preferred enhanced, animated text captions as they provide improved access to the emotive information contained in the content. Text-based animated sound effects confused participants and animated symbols were recommended as a replacement.",,Closed captioning; Film technology; Hard of hearings; Sound effects; Speech prosody; Human computer interaction; Psychology computing; Audition,Article,Final,Scopus,2-s2.0-46349099076,Peter,,
"Rice M., Alm N.",57213614027;57205138916;,Designing new interfaces for digital interactive television usable by older adults,2008,Computers in Entertainment,6,1,6,,,,57.0,10.1145/1350843.1350849,https://www.scopus.com/inward/record.uri?eid=2-s2.0-43949097929&doi=10.1145%2f1350843.1350849&partnerID=40&md5=c947028297aff176c488c429f0098acb,"The adoption of digital television (DTV), if appropriately designed, could be particularly attractive for older people, who tend to be overlooked when new services and applications are introduced, and remain a marginalized segment of the television broadcasting population. This article explores a range of methodologies and interactive approaches designed to support older people who have difficulties in using current interface models for DTV. Following an extensive requirements-gathering exercise, four different navigational layouts with a simplified remote control were tested and evaluated with older users to assess their ease of use. The results demonstrated the expected difficulties in understanding some of the terminology and interactive concepts utilized in ""traditional"" DTV design. Aspects of the experimental layouts suggest promising new directions in the development of visualization and navigation metaphors for user-led activities on DTV for older adults. © 2008 ACM.",Digital television; Interactive theatre; Interface design; Older people; Participatory design,Data acquisition; Digital television; Optical design; Television broadcasting; User interfaces; Visualization; Interactive theatre; Interface design; Participatory designs; Video on demand,Conference Paper,Final,Scopus,2-s2.0-43949097929,Peter,,
"Chapdelaine C., Beaulieu M., Gagnon L.",57189843313;57198137843;7005340115;,"Designing caption production rules based on face, text and motion detections",2008,Proceedings of SPIE - The International Society for Optical Engineering,6806,,68061K,,,,2.0,10.1117/12.766841,https://www.scopus.com/inward/record.uri?eid=2-s2.0-41149110548&doi=10.1117%2f12.766841&partnerID=40&md5=788aa648eab18fe06b567880357f3b6b,"Producing off-line captions for the deaf and hearing impaired people is a labor-intensive task that can require up to 18 hours of production per hour of film. Captions are placed manually close to the region of interest but it must avoid masking human faces, texts or any moving objects that might be relevant to the story flow. Our goal is to use image processing techniques to reduce the off-line caption production process by automatically placing the captions on the proper consecutive frames. We implemented a computer-assisted captioning software tool which integrates detection of faces, texts and visual motion regions. The near frontal faces are detected using a cascade of weak classifier and tracked through a particle filter. Then, frames are scanned to perform text spotting and build a region map suitable for text recognition. Finally, motion mapping is based on the Lukas-Kanade optical flow algorithm and provides MPEG-7 motion descriptors. The combined detected items are then fed to a rule-based algorithm to determine the best captions localization for the related sequences of frames. This paper focuses on the defined rules to assist the human captioners and the results of a user evaluation for this approach. © 2008 SPIE-IS&T.",E-accessibility; Eye-tracking; Image processing; TV captioning; Video object detection,Algorithms; Computer software; Motion estimation; Optical flows; Systems analysis; TV captioning; Video object detection; Face recognition,Conference Paper,Final,Scopus,2-s2.0-41149110548,Peter,,
Downey G.J.,8782624400;,"Closed captioning: Subtitling, stenography, and the digital convergence of text with television",2008,"Closed Captioning: Subtitling, Stenography, and the Digital Convergence of Text with Television",,,,1,387,,32.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895691304&partnerID=40&md5=83ce5a6e07a4f7630ad0679386db38ba,"This engaging study traces the development of closed captioning-a field that emerged in the 1970s and 1980s from decades-long developments in cinematic subtitling, courtroom stenography, and education for the deaf. Gregory J. Downey discusses how digital computers, coupled with human mental and physical skills, made live television captioning possible. Downey's survey includess the hidden information workers who mediate between live audiovisual action and the production of visual track and written records. His work examines communication technology, human geography, and the place of labor in a technologically complex and spatially fragmented world. Illustrating the ways in which technological development grows out of government regulation, education innovation, professional profit-seeking, and social activism, this interdisciplinary study combines insights from several fields, among them the history of technology, human geography, mass communication, and information studies. © 2008 The Johns Hopkins University Press. All rights reserved.",,,Book,Final,Scopus,2-s2.0-84895691304,Peter,,
"Ribas M.A., Fresco P.R.",57211056195;26666759200;,A practical proposal for the training of respeakers1,2008,Journal of Specialised Translation,,10,,106,127,,23.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650113030&partnerID=40&md5=eeda3dab0abaa2db3167ac277a2c4439,"In the field of Audiovisual Translation, some disciplines still have a long way to go in terms of visibility. Speech recognition-based subtitling, also known as respeaking, is a case in point. Even though it seems to be consolidating as the preferred method of providing intralingual live subtitles for the Deaf and Hard-of-Hearing in many TV channels, it is far from being consolidated regarding research and especially teaching. Building on the research carried out so far in the field, the present article attempts to tackle the question of the training of respeakers. First of all, respeaking is presented, described and compared to subtitling and interpreting. Then, a full account is given of the skills required for a respeaker, whether they are to be obtained from subtitling, interpreting or specifically from respeaking. Finally, a practical proposal for the training of respeakers is put forward by way of practical exercises geared at providing students with the required skills. © 2008 University of Roehampton. All rights reserved.",Competences; Respeaking; Simultaneous interpreting; Skills; Subtitling; Training,,Article,Final,Scopus,2-s2.0-78650113030,Peter,,
"Cambra C., Silvestre N., Leal A.",7801358308;15137439300;7004298024;,Comprehension of television messages by deaf students at various stages of education,2008,American Annals of the Deaf,153,5,,425,434,,18.0,10.1353/aad.0.0065,https://www.scopus.com/inward/record.uri?eid=2-s2.0-65349117646&doi=10.1353%2faad.0.0065&partnerID=40&md5=638ddb8ee59e170d0fb3600516fbdb36,"Ince television captioning first became available in Spain, advances have occurred on two fronts: a progressive increase in the number of programs with captions and improved captioning technology. The present study explores the effectiveness of captioning through analysis of deaf viewers' comprehension, taking into account the contribution of the program's verbal and visual information. The study participants, comprising two groups of deaf students at different educational stages, were asked to explain what they had understood after watching a film under two conditions: voice and sound but without captions; and voice, sound, and captions. Results indicate that deaf students have difficulty accessing the information even under the voice/sound/caption condition, not only due to their level of reading ability but also to the speed of caption presentation when oral content is literally transcribed on the screen in its entirety.",,"age; article; child; cochlea prosthesis; comparative study; comprehension; evaluation; hearing impairment; human; information processing; reading; sex difference; statistical analysis; television; Age Factors; Child; Cochlear Implants; Comprehension; Data Collection; Data Interpretation, Statistical; Deafness; Humans; Reading; Sex Factors; Television",Article,Final,Scopus,2-s2.0-65349117646,Peter,,
"Rashid R., Vy Q., Hunt R.G., Fels D.I.",23025557900;23026041900;23024708000;57218572233;,Dancing with words,2007,"Creativity and Cognition 2007, CC2007 - Seeding Creativity: Tools, Media, and Environments",,,,269,270,,1.0,10.1145/1254960.1255007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-36649026470&doi=10.1145%2f1254960.1255007&partnerID=40&md5=a6ef5b963fcfd1e9a3b21e61fb20a3cf,"Music, sound effects and speech prosody are missing from conventional closed captions. Despite advancements, closed captioning has yet to surpass the threshold of 1970s technology. In this study, animated text was used to convey missing sound information to hearing impaired users. Twenty-five hard of hearing and hearing participants watched two television clips containing three different styles of captions: conventional, enhanced and extreme. Participants preferred enhanced, animated text captions as their access to emotive information was improved.",Captioning; Information systems; Kinetic text; Miscellaneous,Acoustic waves; Animation; Computer music; Information retrieval systems; Animated text captions; Captioning; Kinetic texts; Word processing,Article,Final,Scopus,2-s2.0-36649026470,Peter,,
"Debevc M., Povalej P., Verlič M., Stjepanovic Z.",56816724400;55906083200;8949978100;7003286032;,Exploring usability and accessibility of an e-learning system for improving computer literacy,2007,"New Trends in ICT and Accessibility - Proceedings of the 1st International Conference in Information and Communication Technology and Accessibility, ICTA 2007",,,,119,125,,10.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880213337&partnerID=40&md5=84a260fd6d64f0d1c2560980880ecf6e,"Use of ICT in education brings new opportunities for people with special needs. Web based learning and adapted e-learning materials provide an alternative way of learning comparing to traditional classroom learning. In this study deaf and hard of hearing persons were using an accessible and adapted e-learning environment for improving computer literacy which included also adapted streaming video with interpreter and subtitles. The process of adapting the materials required a new approach to properly displaying the video with interpreter and minimizing the time the users had to wait for the materials to load. The results of the pedagogical and usability study, based on the evaluation of the participants before taking the e-learning course and after completing it, have shown that deaf and hard of hearing persons can successfully, efficiently, and effectively use the advantages of e-learning. The work described originates from the largest project earmarked for the people with special needs in Slovenia.",,Classroom learning; Computer literacy; E-learning course; E-learning environment; E-learning materials; E-learning systems; Usability studies; Web based learning; Audition; Communication systems; Computer science; Computers; E-learning; Information technology; Teaching; Computer aided instruction,Conference Paper,Final,Scopus,2-s2.0-84880213337,Peter,,
"Fitzpatrick E., Coyle D.E., Durieux-Smith A., Graham I.D., Angus D.E., Gaboury I.",35577968800;7102318841;56851614700;35372072800;7102124560;6505829593;,Parents' preferences for services for children with hearing loss: A conjoint analysis study,2007,Ear and Hearing,28,6,,842,849,,25.0,10.1097/AUD.0b013e318157676d,https://www.scopus.com/inward/record.uri?eid=2-s2.0-35848931442&doi=10.1097%2fAUD.0b013e318157676d&partnerID=40&md5=bd5f625e1d958eb9e5bbd0f65c9bb819,"OBJECTIVE: Early identification of permanent childhood hearing loss through universal newborn hearing screening is rapidly becoming a standard of care. However, it is well recognized that hearing screening must be embedded within a comprehensive system of rehabilitation and parent support services. This study was undertaken with parents of young children with permanent hearing loss to examine their preferences for characteristics associated with intervention services. A secondary goal was to explore whether preferences may differ according to patient subgroups. DESIGN: Conjoint analysis, a preference-based economic technique, was used to investigate parents' strength of preferences. A cross-sectional survey that consisted of hypothetical clinic scenarios was developed based on information from qualitative interviews with parents. The questionnaire was administered to parents receiving intervention services in the province of Ontario, Canada, shortly after the implementation of a universal hearing screening program. The sample was recruited from three different clinical programs. RESULTS: A total of 48 of 75 respondents completed the questionnaire, a response rate of 64%. The participants varied by screening status of the child (25 screened, 23 not screened), type of device (23 hearing aids, 25 cochlear implants), and region. All five characteristics of care that were selected for inclusion in the survey were found to be statistically significant attributes of services: coordinated services, access to parent support, access to information, frequency of services, and location of services. Parents showed a preference for clinic-based rather than home-based services. Preferences toward once a week therapy services rather than services two to three times weekly were also found. In particular, parents valued service models that consisted of well-coordinated care with access to support from other parents. Differences in respondents according to hearing screening status (screened or unscreened), type of hearing device (hearing aid or cochlear implant), or region (Ottawa or Toronto) did not seem to affect parents' preferences for attributes of care. CONCLUSIONS: Conjoint analysis is a useful technique for quantifying parents' preferences for care. The values expressed by parents provide insights into the aspects of a service model that should receive consideration in the development of programs for young children with hearing loss and their families. © 2007 Lippincott Williams & Wilkins, Inc.",,"access to information; article; auditory rehabilitation; auditory screening; Canada; child; childhood disease; cochlea prosthesis; early diagnosis; family; hearing aid; hearing loss; human; newborn screening; parent; patient care; qualitative analysis; questionnaire; Attitude to Health; Child; Child Health Services; Child, Preschool; Disabled Children; Family; Hearing Loss; Hearing Tests; Humans; Models, Theoretical; Parents; Professional-Family Relations; Questionnaires; Regression Analysis",Article,Final,Scopus,2-s2.0-35848931442,Peter,,
"Chapdelaine C., Gouaillier V., Beaulieu M., Gagnon L.",57189843313;6508375030;57198137843;7005340115;,Improving video captioning for deaf and hearing-impaired people based on eye movement and attention overload,2007,Proceedings of SPIE - The International Society for Optical Engineering,6492,,64921K,,,,8.0,10.1117/12.703344,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548240050&doi=10.1117%2f12.703344&partnerID=40&md5=992f357f04b7ecb6ab62273297497753,"Deaf and hearing-impaired people capture information in video through visual content and captions. Those activities require different visual attention strategies and up to now, little is known on how caption readers balance these two visual attention demands. Understanding these strategies could suggest more efficient ways of producing captions. Eye tracking and attention overload detections are used to study these strategies. Eye tracking is monitored using a pupilcenter-corneal-reflection apparatus. Afterward, gaze fixation is analyzed for each region of interest such as caption area, high motion areas and faces location. This data is also used to identify the scanpaths. The collected data is used to establish specifications for caption adaptation approach based on the location of visual action and presence of character faces This approach is implemented in a computer-assisted captioning software which uses a face detector and a motion detection algorithm based on the Lukas-Kanade optical flow algorithm. The different scanpaths obtained among the subjects provide us with alternatives for conflicting caption positioning. This implementation is now undergomg a user evaluation with hearing impaired participants to validate the efficiency of our approach. © 2007 SPIE-IS&T.",Captioning systems; Eye tracking; Human-computer interaction; Motion detection,Computer software; Eye movements; Human computer interaction; Human rehabilitation equipment; Motion estimation; Visual communication; Captioning software; Eye tracking; Motion detection; Video captioning; Image analysis,Conference Paper,Final,Scopus,2-s2.0-34548240050,Peter,,
"Lee D.G., Fels D.I., Udo J.P.",7406662216;57218572233;14038549800;,Emotive captioning,2007,Computers in Entertainment,5,2,1281344,,,,25.0,10.1145/1279540.1279551,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548267146&doi=10.1145%2f1279540.1279551&partnerID=40&md5=f941aafec1cfb28d58be6a469680b870,"Television and film have become important equalization mechanisms for the dissemination and distribution of cultural materials. Closed captioning has allowed people who are deaf and hard of hearing to be included as audience members. However, some of the audio information such as music, sound effects, and speech prosody are not generally provided for in captioning. To include some of this information in closed captions, we generated graphical representations of the emotive information that is normally represented with nondialog sound. Eleven deaf and hard of hearing viewers watched two different video clips containing static and dynamic enhanced captions and compared them with conventional closed captions of the same clips. These viewers then provided verbal and written feedback regarding positive and negative aspects of the various captions. We found that hard of hearing viewers were significantly more positive about this style of captioning than deaf viewers and that some viewers believed that these augmentations were useful and enhanced their viewing experience. © 2007 ACM.",Assistive technologies; Closed captioning; Human-computer interaction; People with disabilities; Television,Acoustic waves; Computer music; Feedback; Handicapped persons; Human computer interaction; Virtual reality; Assistive technologies; Closed captioning; Cultural materials; Equalization; Television,Article,Final,Scopus,2-s2.0-34548267146,Peter,,
"Ward P., Wang Y., Paul P., Loeterman M.",57198935541;15764048200;14028983400;8337284000;,Near-verbatim captioning versus edited captioning for students who are deaf or hard of hearing: A preliminary investigation of effects on comprehension,2007,American Annals of the Deaf,152,1,,20,28,,18.0,10.1353/aad.2007.0015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-34447267979&doi=10.1353%2faad.2007.0015&partnerID=40&md5=34b8b3a38c62cc462a528f97a6d2bf0e,"The study assessed the effects of near-verbatim captioning versus edited captioning on a comprehension task performed by 15 children, ages 7-11 years, who were deaf or hard of hearing. The children's animated television series Arthur was chosen as the content for the study. The researchers began the data collection procedure by asking participants to watch videotapes of the program. Researchers signed or spoke (or signed and spoke) 12 comprehension questions from a script to each participant. The sessions were videotaped, and a checklist was used to ensure consistency of the question-asking procedure across participants and sessions. Responses were coded as correct or incorrect, and the dependent variable was reported as the number of correct answers. Neither near-verbatim captioning nor edited captioning was found to be better at facilitating comprehension; however, several issues emerged that provide specific directions for future research on edited captions.",,,Article,Final,Scopus,2-s2.0-34447267979,Peter,,
"Fels D.I., Lee D.G., Branje C., Hornburg M.",57218572233;7406662216;26427565700;55498264700;,Emotive captioning and access to television,2005,"Association for Information Systems - 11th Americas Conference on Information Systems, AMCIS 2005: A Conference on a Human Scale",4,,,2026,2033,,10.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869770030&partnerID=40&md5=b5b94636ffd6d684340bab95b11c3d02,"Closed captioning has been enabling access to television for people who are deaf and hard of hearing since the early 1970s. Since that time, technology and people's demands have been steadily improving and increasing. Closed captioning has not kept up with these changes. We present the results of a study that used graphics, colour, icons and animation as well as text, emotive captions, to capture more of the sound information contained in television content, deaf and hard of hearing participants compared emotive and conventional captions for two short video segments. The results showed that there was a significant difference between deaf and hard of hearing viewers in their reaction to the emotive captions. Hard of hearing viewers seemed to enjoy them and find them interesting, deaf viewers had a strong dislike for them although they did see some potential for intermittent use of emotive captions or for use with children's programs.",Closed captioning; Deaf and hard of hearing; Emotions; Television production processes,Closed captioning; Emotions; Production process; Television content; Video segments; Animation; Information systems; Audition,Conference Paper,Final,Scopus,2-s2.0-84869770030,Peter,,
"Folkins A., Sadler G.R., Ko C., Branz P., Marsh S., Bovee M.",23566577100;7005152183;7202596814;8711747100;7202502548;36817016900;,Improving the Deaf community's access to prostate and testicular cancer information: A survey study,2005,BMC Public Health,5,,63,,,,37.0,10.1186/1471-2458-5-63,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23244463361&doi=10.1186%2f1471-2458-5-63&partnerID=40&md5=6b345e9cfb8e7d630899e6f1afe741ef,"Background: Members of the Deaf community face communication barriers to accessing health information. To resolve these inequalities, educational programs must be designed in the appropriate format and language to meet their needs. Methods: Deaf men (102) were surveyed before, immediately following, and two months after viewing a 52-minute prostate and testicular cancer video in American Sign Language (ASL) with open text captioning and voice overlay. To provide the Deaf community with information equivalent to that available to the hearing community, the video addressed two cancer topics in depth. While the inclusion of two cancer topics lengthened the video, it was anticipated to reduce redundancy and encourage men of diverse ages to learn in a supportive, culturally aligned environment while also covering more topics within the partnership's limited budget. Survey data were analyzed to evaluate the video's impact on viewers' preand post-intervention understanding of prostate and testicular cancers, as well as respondents' satisfaction with the video, exposure to and use of early detection services, and sources of cancer information. Results: From baseline to immediately post-intervention, participants' overall knowledge increased significantly, and this gain was maintained at the two-month follow-up. Men of diverse ages were successfully recruited, and this worked effectively as a support group. However, combining two complex cancer topics, in depth, in one video appeared to make it more difficult for participants to retain as many relevant details specific to each cancer. Participants related that there was so much information that they would need to watch the video more than once to understand each topic fully. When surveyed about their best sources of health information, participants ranked doctors first and showed a preference for active rather than passive methods of learning. Conclusion: After viewing this ASL video, participants showed significant increases in cancer understanding, and the effects remained significant at the two-month follow-up. However, to achieve maximum learning in a single training session, only one topic should be covered in future educational videos. © 2005 Folkins et al; licensee BioMed Central Ltd.",,"adult; age distribution; aged; article; controlled study; cultural anthropology; education program; environmental factor; exposure; follow up; health service; health survey; hearing impairment; human; language; learning; major clinical study; male; medical information; patient satisfaction; physician; prostate carcinoma; testis carcinoma; videorecording; voice; attitude to health; communication disorder; deaf education; health care delivery; information service; middle aged; program evaluation; Prostatic Neoplasms; sign language; standards; supply and distribution; Testicular Neoplasms; United States; very elderly; videorecording; Adult; Aged; Aged, 80 and over; California; Communication Barriers; Education of Hearing Disabled; Health Knowledge, Attitudes, Practice; Health Services Accessibility; Humans; Information Services; Male; Middle Aged; Program Evaluation; Prostatic Neoplasms; Sign Language; Testicular Neoplasms; Videotape Recording",Article,Final,Scopus,2-s2.0-23244463361,Peter,,
"Petrie H.L., Weber G., Fisher W.",12759866100;56395998400;55423597300;,"Personalization, interaction, and navigation in rich multimedia documents for print-disabled users",2005,IBM Systems Journal,44,3,,629,635,,24.0,10.1147/sj.443.0629,https://www.scopus.com/inward/record.uri?eid=2-s2.0-23744482679&doi=10.1147%2fsj.443.0629&partnerID=40&md5=9b3227c6ed813d1904b0acd948d671fe,"Multimedia documents, such as textbooks, reference materials, and leisure materials, inherently use techniques that also can help make them accessible for people with disabilities who find it difficult or impossible to use printed materials. This includes individuals who are blind, partially sighted, deaf, hard of hearing, or dyslexic. The varying requirements of print-disabled users have led us to the notion of enriched media documents that contain redundant alternative representations of the same information. Unlike existing one-document-for-all approaches, we propose a personalization process that customizes these rich media documents to the needs of an individual reader. This paper describes, from an iterative user-centered design perspective, the development of a multimedia reading system for a variety of print-disabled user groups. We address issues of establishing user personalization profiles, as well as adapting and customizing content, interaction, and navigation. Customization of interaction and navigation leads to differences in the user interface, as well as different structural views of indexes. Customization of content includes insertion of a summary, synchronization of sign language video with highlighting of text, self-voicing capability, alternative support for screen readers, or reorganization of layout to accommodate large fonts. Finally, we consider whether this approach of addressing the specific needs of heterogeneous user groups provides a basis for a universal design approach for multimedia user interfaces. © Copyright 2005 by International Business Machines Corporation.",,Handicapped persons; Human computer interaction; Multimedia systems; User interfaces; Print-disabled users; User-centered design; Usability engineering,Article,Final,Scopus,2-s2.0-23744482679,Peter,,
"Debevc M., Peljhan Ž.",56816724400;6505711037;,The role of video technology in on-line lectures for the deaf,2004,Disability and Rehabilitation,26,17,,1048,1059,,29.0,10.1080/09638280410001702441,https://www.scopus.com/inward/record.uri?eid=2-s2.0-4544367024&doi=10.1080%2f09638280410001702441&partnerID=40&md5=916b2ccac3ca2cad890a0b74856573c6,"Purpose: The purpose of this paper is to determine the effectiveness of web-based video lectures on demand for the deaf in comparison to the traditional method of teaching using a sign language interpreter. The web-based lectures presented are specifically designed for the deaf in education and in rehabilitation. Method: Sixty-three deaf students and adults were divided into four groups. All of the groups were made up of users who shared similar knowledge in the field of computers, but with different abilities in using computers, from beginners to advanced users. All of the groups were of mixed gender. The first two groups (consisting of 23 test users) graded the usability of the user interface for web-based lecture on demand with the help of the standardized SUMI questionnaire. After that, two groups (20 students from high school and 20 adults) joined in a 45-min informational program on the history of the deaf. Both groups were then divided into two smaller subgroups of 10 participants. The first subgroup in the first part of the learning program followed a traditional teaching style with the help of a teacher and an interpreter for Slovenian sign language. Meanwhile, the second group observed a 12-min web-based video lecture on demand and still had available an additional 18 min for a more detailed observation of the video. At the end of the lecture, the teacher used the questionnaire to review the participants' understanding of the content of the lecture in both of the groups. During the entire testing period, the interpreter used Slovenian sign language. Results: By using the SUMI questionnaire, we determined the usability of the user interface for comprehension and gathering of knowledge. We discovered that the system was usable according to the standards. The global Median results (Global Median = 51) were in the range of 50. In the second part of testing, we determined the level of significance between the traditional and web-based lectures. The results were statistically evaluated using the t-tests and the ANOVA test. From the t-tests we established the hypothesis that the number of correct answers for both groups (group 1: web-based, group 2: traditional) differed. The t-test used for the age groups rejected the hypothesis that the number of correct answers for both groups differed, where group 1 was comprised of adults and group 2 was comprised of students. Additionally, the ANOVA test showed that the number of correct answers for adults using traditional lectures differed significantly from the number of correct answers for both adult and student web-based users. The ANOVA test showed no differences between any of the remaining groups. Conclusions: We can conclude that for deaf people it is extremely important to introduce the use of information and communications technology on all levels of education and rehabilitation. This increases their ability to learn and improves their understanding of learning materials, especially if the applications are designed specifically for their needs. Through daily exposure to a larger number of such materials, we can positively influence the literacy (reading and writing skills) of the deaf. With increased literacy, the deaf would be able to read literature, and subtitles, enabling them to receive information through written sources. Therefore, we can expect them to have a higher self-esteem, more easily integrate into society and have more opportunities for employment. © 2004 Taylor & Francis Ltd.",,"adult; analysis of variance; article; comprehension; controlled study; educational technology; employment; female; hearing impairment; high school; human; human computer interaction; hypothesis; information system; interpersonal communication; learning; major clinical study; male; medical information; online system; priority journal; questionnaire; reading; self esteem; sign language; socialization; standardization; student; teaching; videorecording; writing; Adolescent; Adult; Analysis of Variance; Communication Aids for Disabled; Computer-Assisted Instruction; Deafness; Education, Special; Educational Technology; Female; Humans; Internet; Male; Middle Aged; Questionnaires; Slovenia; Video Recording",Article,Final,Scopus,2-s2.0-4544367024,Peter,,
"Gulliver S.R., Ghinea G.",6603891003;35616295700;,Stars in their eyes: What eye-tracking reveals about multimedia perceptual quality,2004,"IEEE Transactions on Systems, Man, and Cybernetics Part A:Systems and Humans.",34,4,,472,482,,44.0,10.1109/TSMCA.2004.826309,https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142674306&doi=10.1109%2fTSMCA.2004.826309&partnerID=40&md5=28ddcbc73cade1e38a82d1ed1c0d9cbc,"Perceptual multimedia quality is of paramount importance to the continued take-up and proliferation of multimedia applications; users will not use and pay for applications if they are perceived to be of low quality. While traditionally distributed multimedia quality has been characterized by quality-of-service (QoS) parameters, these neglect the user perspective of the issue of quality. In order to redress this shortcoming, we characterize the user multimedia perspective using the quality-of-perception (QoP) metric, which encompasses not only a user's satisfaction with the quality of a multimedia presentation, but also his/her ability to analyze, synthesize, and assimilate informational content of multimedia. In recognition of the fact that monitoring eye movements offers insights into visual perception, as well as the associated attention mechanisms and cognitive processes, this paper reports on the results of a study investigating the impact of differing multimedia presentation frame rates on user QoP and eye path data. Our results show that provision of higher frame rates, usually assumed to provide better multimedia presentation quality, do not significantly impact upon the median coordinate value of eye path data. Moreover, higher frame rates do not significantly increase the level of participant information assimilation, although they do significantly improve overall user enjoyment and quality perception of the multimedia content being shown. © 2004 IEEE.",,Eye movements; Human computer interaction; Human engineering; Image quality; Quality of service; Usability engineering; Vision; Eye tracking; Frame rate; Information assimilation; Multimedia video; Perceptual multimedia quality; Quality-of-perception; Multimedia systems,Article,Final,Scopus,2-s2.0-3142674306,Peter,,
"Gulliver S.R., Ghinea G.",6603891003;35616295700;,How level and type of deafness affect user perception of multimedia video clips,2003,Universal Access in the Information Society,2,4,A007,374,386,,24.0,10.1007/s10209-003-0067-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-27944477647&doi=10.1007%2fs10209-003-0067-5&partnerID=40&md5=3647500cea10ee31df6f51e10e22bd25,"Our research investigates the impact that hearing has on the perception of multimedia, with and without captions, by discussing how hearing loss, captions and deafness type affect user quality of perception (QoP). QoP encompasses both the user's level of satisfaction and their ability to assimilate informational content of multimedia.Experimental results show that hearing has a significant effect on participants' ability to assimilate information, independent of video type or use of captions. It is shown that captioned video does not necessarily provide deaf users with a greater level of information but changes user QoP, providing a greater level of video contextualisation. © Springer-Verlag 2003.",Deafness; Multimedia; Perception; Quality; Video,Image quality; Information science; Information systems; Sensory perception; Software engineering; Contextualisation; Deafness; Level of satisfaction; Multimedia; Multimedia video; Quality of perception; User perceptions; Video; Audition,Article,Final,Scopus,2-s2.0-27944477647,Peter,,
"Gulliver S.R., Ghinea G.",6603891003;35616295700;,Impact of captions on deaf and hearing perception of multimedia video clips,2002,"Proceedings - 2002 IEEE International Conference on Multimedia and Expo, ICME 2002",1,,1035891,753,756,,6.0,10.1109/ICME.2002.1035891,https://www.scopus.com/inward/record.uri?eid=2-s2.0-11244275911&doi=10.1109%2fICME.2002.1035891&partnerID=40&md5=2d5862ba16450ba17de06c01d7d5f739,"We investigate the impact of captions on deaf and hearing perception of multimedia video clips. We measure perception using a parameter called quality of perception (QoP), which encompasses not only a user's satisfaction with multimedia clips, but also his/her ability to perceive, synthesise and analyse the informational content of such presentations. By studying perceptual diversity, it is our aim to identify trends that will help future implementation of adaptive multimedia technologies. Results show that although hearing level has a significant affect on information assimilation, the effect of captions is not significant on the objective level of information assimilated. Deaf participants predict that captions significantly improve their level of information assimilation, although no significant objective improvement was measured. The level of enjoyment is unaffected by a participant's level of hearing or use of captions. © 2002 IEEE.",,Multimedia video,Conference Paper,Final,Scopus,2-s2.0-11244275911,Peter,,
Wheeler-Scruggs K.,6506487767;,Assessing the employment and independence of people who are deaf and low functioning,2002,American Annals of the Deaf,147,4,,11,17,,14.0,10.1353/aad.2012.0260,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036824917&doi=10.1353%2faad.2012.0260&partnerID=40&md5=c2d7ecddb21456a0b2be84b8ef55de77,"There is much variation in functional level among people who are deaf and low functioning. The present study focused on the work and independent living status of people who are deaf and have varying levels of low functioning. Each study participant attended a comprehensive rehabilitation facility. Information was collected through personal interviews. Not all interviewees were employed at the time of the interview, but the majority lived on their own. Those interviewees who worked were, on the whole, happy with their jobs. Workplace accommodations were virtually nonexistent, while home accommodations included television closed captioning, TTYs, and alarm clocks with visual or vibrating signals. Suggestions and guidelines are given on areas in which people who are deaf and low functioning may need services beyond job placement.",,"adult; article; daily life activity; employment; female; hearing impairment; human; job satisfaction; male; mental deficiency; middle aged; psychological aspect; vocational rehabilitation; workplace; Activities of Daily Living; Adult; Deafness; Employment; Female; Humans; Job Satisfaction; Male; Mental Retardation; Middle Aged; Rehabilitation, Vocational; Workplace",Article,Final,Scopus,2-s2.0-0036824917,Peter,,
"Burnham D., Robert-Ribes J., Ellison R.",7005177876;6507176248;57781535000;,Why captions have to be on time,1998,"Auditory-Visual Speech Processing 1998, AVSP 1998",,,,1,2,,5.0,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133446781&partnerID=40&md5=4504a009da4a7f7b244b14908948abc6,"Closed captioning dramatically improves deaf people’s enjoyment of television shows, and appears to augment the auditory signal for people with some degree of hearing impairment. However, reports from people with mild to severe hearing loss suggest that when there is a delay between the audio track and the caption, perceivers are confused unless they turn down the volume. These effects have not yet been investigated experimentally. This study provides a preliminary investigation of the importance of synchronisation of captions with auditory-visual material for hearing-impaired people’s enjoyment and comprehension of captioned television programs. Two participants were presented with audio-caption delays of 0, 1, 2, and 4 secs in an auditory-visual condition and an auditory-only condition. Both enjoyment and intelligibility diminished over lag times. In general enjoyment and intelligibility were higher in the auditory-visual than the auditory-only condition, however, for the more severely hearing impaired of the two participants, both enjoyment and intelligibility diminished at a faster rate over delay times for the auditory-visual than the auditory-only condition. Thus at long delays the presence of the visual signal appeared to be distracting. These results are discussed in terms of perceptual mechanisms and practical applications for captioning. © Auditory-Visual Speech Processing 1998, AVSP 1998.",,Speech processing; Audio track; Auditory signals; Auditory-visual; Closed-captioning; Condition; Deaf peoples; Hearing impaired; Hearing impairments; Hearing loss; Television shows; Audition,Conference Paper,Final,Scopus,2-s2.0-85133446781,Peter,,
Jensema C.,7004520188;,Viewer Reaction to Different Television Captioning Speeds,1998,American Annals of the Deaf,143,4,,318,324,,59.0,10.1353/aad.2012.0073,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032193263&doi=10.1353%2faad.2012.0073&partnerID=40&md5=d8e04a458644048a2eda58fd07afa5ac,"Video segments captioned at different speeds were shown to a group of 578 people that included deaf, hard of hearing, and hearing viewers. Participants used a five-point scale to assess each segment's caption speed. The ""OK"" speed, defined as the rate at which ""caption speed is comfortable to me,"" was found to be about 145 words per minute (WPM), very close to the 141 WPM mean rate actually found in television programs (Jensema, McCann, & Ramsey, 1996). Participants adapted well to increasing caption speeds. Most apparently had little trouble with the captions until the rate was at least 170 WPM. Hearing people wanted slightly slower captions. However, this apparently related to how often they watched captioned television. Frequent viewers were comfortable with faster captions. Age and sex were not related to caption speed preference; nor was education, with the exception that people who had attended graduate school showed evidence that they might prefer slightly faster captions.",,"adolescent; adult; aged; article; child; female; hearing impairment; human; male; middle aged; television; time; vision; Adolescent; Adult; Aged; Aged, 80 and over; Child; Deafness; Female; Humans; Male; Middle Aged; Television; Time Factors; Visual Perception",Article,Final,Scopus,2-s2.0-0032193263,Peter,,
"Lipton D.S., Goldstein M.F., Fahnbulleh F.W., Gertz E.N.",7007008764;7403014971;6506603077;7005391833;,The interactive video-questionnaire: A new technology for interviewing deaf persons,1996,American Annals of the Deaf,141,5,,370,378,,17.0,10.1353/aad.2012.0228,https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030330605&doi=10.1353%2faad.2012.0228&partnerID=40&md5=ac4570a6492f334d41ababb46376ae6f,"This paper traces the development of a new technology, the Interactive Video-Questionnaire, for interviewing Deaf persons by using manually signed questionnaires. After encountering numerous obstacles to conducting surveys with Deaf persons about substance abuse using the same methods typically used with hearing persons, the researchers, with a Small Business Innovative Research grant from the National Institute on Drug Abuse, piloted a survey method that uses videodisc and bar code readers to present survey questions signed on screen in American Sign Language and Signed English. Following consultations with Deaf participants, deficiencies of this method were identified and corrected. An interactive multimedia program was created in Phase II of this research effort that offered questions visually in American Sign Language, Signed English, or Speechreading. All questions were subtitled in written English, with Touchscreen entry and automatic data capture and storage. The potential exists for many important uses of the Interactive Video-Questionnaire.",,addiction; article; female; hearing impairment; human; interpersonal communication; lip reading; male; questionnaire; sign language; technology; videorecording; Communication; Deafness; Female; Humans; Lipreading; Male; Questionnaires; Sign Language; Substance-Related Disorders; Technology; Videotape Recording,Article,Final,Scopus,2-s2.0-0030330605,Peter,,
